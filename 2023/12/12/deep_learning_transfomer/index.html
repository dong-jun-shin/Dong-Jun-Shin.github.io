<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Nerual Networks 기법 - Transformer의 작동 원리</title>
  <meta
    name="description"
    content="배경AI/ML에 대해 학습하던 중 Transformer 기술을 접하게 되었습니다. 최근 AI 분야에서 중점적으로 활용되며, 연구되고 있는 이 기술은 LLM과 같은 딥러닝 모델이 어떻게 데이터를 학습하는지 이해하기 위해 필수로 학습해야겠다는 생각이 들었습니다. 처음 접하는 용어가 많..."
  />
  <!-- Twitter Cards -->
<meta name="twitter:title" content="Nerual Networks 기법 - Transformer의 작동 원리">
<meta name="twitter:description" content="배경

AI/ML에 대해 학습하던 중 Transformer 기술을 접하게 되었습니다. 최근 AI 분야에서 중점적으로 활용되며, 연구되고 있는 이 기술은 LLM과 같은 딥러닝 모델이 어떻게 데이터를 학습하는지 이해하기 위해 필수로 학습해야겠다는 생각이 들었습니다. 처음 접하는 용어가 많았고, 이해가 쉽지 않은 이 기술을 학습하기 위해 초심자의 시각으로 내용을 정리해보았습니다.

설명

1. Transformer란

Transformer는 자연어 문장에 대해 전체를 한 번에 보고, 관계를 파악하며, 중요도를 매기고, 병렬 처리를 할 수 있는 자연어 처리 기술입니다. 전체 문장을 한 번에 보고 모든 단어 간 관계를 동시에 파악하는 것은 마치, 퍼즐을 맞추듯이 각 단어가 다른 단어들과 어떻게 연결되는지를 전체적으로 이해하는 것과 유사한 것 같습니다.

예를 들어 “나는 사과를 먹었다”라는 문장이 있다고 했을 때, Transformer는 이 문장의 모든 단어를 동시에 바라보며, ‘나’와 ‘먹었다’, ‘사과’와 ‘먹었다’ 등의 관계를 한 번에 파악합니다.

Transformer의 주요 특징을 정리하면 다음과 같습니다.


  전체를 한 번에 본다: 문장 전체를 동시에 처리합니다.
  관계를 파악한다: 모든 단어들 사이의 관계를 계산합니다.
  중요도를 매긴다: 각 단어가 다른 단어에 얼마나 중요한지 판단합니다.
  병렬 처리가 가능하다: 여러 작업을 동시에 처리할 수 있어 빠릅니다.


Transformer가 문장을 이해하는 과정

Transformer가 자연어를 이해한다는 것과 어떤 특징이 있는지는 알았지만, 아직 Transformer가 어떻게 문장을 이해하는지는 모르겠습니다. 어떤 원리로 컴퓨터가 문장의 맥락을 이해할 수 있는지 더 자세히 알아보도록 하겠습니다. 여기서는 이해를 위해 “나는 사과를 먹었다”라는 간단한 문장으로 과정을 살펴보겠습니다.

Transformer가 이해하는 과정을 간단하게 도식으로 그려보았습니다.


[자연어를 이해하는 과정]

전체 과정을 보셨다면, 이제 각 과정에 대해 하나씩 단계별로 확인해보겠습니다.

1. 문장 분해

Transformer는 먼저 문장을 개별 단어로 나눕니다.

예: "나는", "사과를", "먹었다"


2. 단어를 숫자로 변환

각 단어는 고유한 숫자(인덱스)로 변환됩니다.

예: "나는" → 1, "사과를" → 2, "먹었다" → 3


3. 숫자를 벡터로 변환

각 숫자는 다시 의미 있는 숫자들의 집합(벡터)으로 바뀝니다. 이를 ‘임베딩’이라고 합니다.

예: 1 → [0.1, 0.2, 0.3], 2 → [0.4, 0.5, 0.6], 3 → [0.7, 0.8, 0.9]


4. 관계 파악

Transformer는 모든 단어 쌍 사이의 관계를 계산합니다. 이를 ‘어텐션’이라고 합니다.

예: "나는"과 "먹었다"의 관계, "사과를"과 "먹었다"의 관계 등을 동시에 계산


5. 중요도 부여

각 관계에 중요도를 부여합니다. 관련성이 높은 단어 쌍은 높은 점수를 받습니다.

예: "사과를"과 "먹었다"의 관계가 "나는"과 "사과를"의 관계보다 높은 점수를 받을 수 있습니다.


6. 정보 종합

이렇게 계산된 관계와 중요도를 바탕으로 각 단어의 최종 의미를 결정합니다.

예: "사과를 먹었다"는 행동과 관련이 높고, "나는 먹었다"는 주체와 행동이 연결됨.


7. 문장 이해

각 단어의 의미를 종합하여 전체 문장의 의미를 파악합니다.

예: "나는 사과를 먹었다"는 문장을 이해하여, 주어가 '나'이고, '사과를 먹는'은 행동을 의미.


이 과정을 통해 Transformer는 “나는 사과를 먹었다”라는 문장에서 누가(나), 무엇을(사과), 어떻게 했는지(먹었다)를 이해할 수 있게 됩니다. 이것은 인간의 언어 이해 과정을 모방했다고 하는데, 인간의 사고 과정을 코드 형태로 구현한다는 것이 참 신기한 것 같습니다.

2. Transformer의 작동 원리

이제 Transformer가 어떤 방식으로 자연어를 이해하는지 알았습니다. 이것을 컴퓨터가 이해할 수 있도록 구현을 해야 하는데, 어떤 식으로 구현하는지 작동 원리도 하나씩 살펴보겠습니다.

2.1 토큰화와 임베딩

Transformer가 텍스트를 이해하는 첫 단계는 토큰화와 임베딩입니다. 이는 마치 우리가 책을 읽을 때 단어를 하나씩 인식하고 그 의미를 파악하는 것과 비슷해요.

1. 토큰화(Tokenization)

토큰화는 문장을 작은 단위(토큰)로 나누는 과정입니다. 이 과정은 1. 문장 분해와 2. 단어를 숫자로 변환에 해당 합니다. 보통 공백을 기준으로 단어 단위를 분할하지만, 때로는 더 작은 단위(subword)로 나누기도 합니다.

예를 들어, “나는 사과를 먹었다”라는 문장을 Transformer에 입력하면 아래처럼 처리됩니다.

- 입력 문장: "나는 사과를 먹었다"
- 토큰화: ['나', '는', '사과', '를', '먹', '었', '다']
- 단어 인덱스: '나'-1, '는'-2, '사과'-3, '를'-4, '먹'-5, '었'-6, '다'-7


2. 임베딩(Embedding)

임베딩은 각 토큰을 컴퓨터가 이해할 수 있는 숫자의 나열(벡터)로 바꾸는 과정입니다. 이 과정은 3. 숫자를 벡터로 변환에 해당 합니다. 이 벡터는 단어의 의미, 문법적 특성 등을 표현해요.

# 임베딩 예시 (실제 값은 더 높은 차원)
- '나': [0.1, 0.2, 0.3]
- '는': [0.4, 0.1, 0.6]
- '사과': [0.2, 0.8, 0.3]


이렇게 각 단어는 숫자의 나열(벡터)로 표현됩니다. 이 숫자들이 바로 컴퓨터가 이해하는 단어의 의미랍니다.

2.2 자기 어텐션(Self-Attention) 메커니즘

자기 어텐션(Self-Attention)은 Transformer의 핵심 부분으로 입력 시퀀스 내의 다른 위치들과의 관계를 고려하여 각 위치의 표현을 계산하는 메커니즘입니다. 이 과정은 4. 관계 파악에 해당합니다. 이 과정에서 각 단어가 다른 단어들과 얼마나 관련이 있는지를 계산합니다.

1. 문맥 연관도(Score) 계산

각 단어 쌍 사이의 관계를 점수로 나타냅니다. 이를 수식으로 표현하면 다음과 같습니다.

score = fi,j / (ti * tj)



  fi,j: 두 단어의 공동 등장 횟수
  ti, tj: 각 단어의 개별 등장 횟수


예를 들어, “나는 사과를 먹었다”라는 문장은 아래와 같습니다.

- '나-는' 관계 점수 = 0.8 (높은 연관성)
- '사과-먹' 관계 점수 = 0.3 (중간 정도의 연관성)
- '나-사과' 관계 점수 = 0.1 (낮은 연관성)


2. 가중치 할당과 갱신

가중치(Weight)는 신경망에서 입력값의 중요도를 조절하는 파라미터입니다. 이 과정은 5. 중요도 부여에 해당합니다. 이 가중치의 업데이트를 위해 손실 함수의 기울기를 의미하는 오차 기울기(Error Gradient)를 활용하게 됩니다.

가중치의 업데이트는 계산된 문맥 연관 정도(Score)에 따라 할당된 α(알파)와 β(베타) 값을 사용해서 이루어지게 됩니다.

- 높은 score (0.5 이상): α = 0.4-0.5
- 중간 score (0.3-0.5): α = 0.2-0.4
- 낮은 score (0.1-0.3): α = 0.1-0.3
- 매우 낮은 score (0.1 미만): β = 0.01-0.3


이 값들은 아래와 같이 가중치를 갱신하는데 사용됩니다.

w ← w + α*δ (δ는 오차 기울기)


3. 자기 어텐션 계산

Query(Q), Key(K), Value(V) 벡터를 사용하여 최종적인 자기 어텐션을 계산합니다. 이 과정은 6. 정보 종합에 해당합니다. (Query, Key, Value는 어텐션 메커니즘에서 사용되는 세 가지 벡터로, Query는 현재 단어, Key와 Value는 모든 단어에 대해 생성됩니다.)

# 자기 어텐션 계산 과정 (의사 코드)
attention_scores = dot_product(Q, K) / sqrt(d_k)
attention_weights = softmax(attention_scores)
output = dot_product(attention_weights, V)


이 과정을 통해 각 단어는 문장 내의 다른 단어들과의 관계를 고려한 새로운 표현을 갖게 됩니다.

2.3 인코더와 디코더

Transformer는 인코더와 디코더라는 두 가지 주요 구성 요소로 이루어져 있습니다. 이 두 구성 요소는 우리가 무의식적으로 하는 언어 이해 과정을 기반으로 모델링된 것으로, 위의 과정을 수행하게 됩니다. 그리고 두 구성 요소가 함께 작동하면서 Transformer는 복잡한 자연어 처리 작업을 수행할 수 있게 되는 것입니다.

1. 인코더(Encoder)

인코더는 여러 레이어로 구성된 네트워크입니다. 주요 역할은 입력 문장의 문맥 정보를 학습하고 인코딩하는 것이에요. 각 레이어에서는 자기 어텐션을 수행하여 문맥 정보를 갱신합니다. (앞서 설명한 3.1 토큰화와 임베딩과 3.2 자기 어텐션(Self-Attention) 메커니즘이 해당)

2. 디코더(Decoder)

디코더는 인코더의 출력을 이용하여 최종 작업(예: 번역, 요약)을 수행하는 네트워크입니다. 디코더도 여러 레이어로 구성되어 있으며, 각 레이어에서는 다음 과정을 통해 인코더의 출력으로 결과(타겟 시퀀스)를 생성합니다.

디코더 과정
1. 디코더는 이전에 생성된 출력을 입력으로 받습니다.
2. 마스크드 멀티-헤드 어텐션을 적용하여 미래 정보를 차단합니다.
3. 인코더-디코더 어텐션을 통해 인코더의 출력과 현재 디코더 상태를 연결합니다.
4. 최종 출력 층에서 다음 토큰을 예측합니다.


3. Transformer의 특징

이제 Transformer가 어떻게 작동하는지까지 대략적으로 살펴봤으니, 특징을 좀 더 세밀하게 알아보겠습니다.


  
    병렬 처리: Transformer는 마치 여러분이 책의 한 페이지를 한 번에 볼 수 있는 것처럼 모든 단어를 동시에 처리할 수 있습니다. 이것은 Transformer가 매우 빠르게 학습할 수 있다는 것을 의미합니다.
  
  
    장거리 의존성 포착: 긴 문장에서도 멀리 떨어진 단어들의 관계를 잘 이해할 수 있습니다. 예를 들어, “나는 어제 산 빨간 사과를 맛있게 먹었다”라는 문장에서 “산”과 “먹었다”의 관계도 쉽게 파악할 수 있습니다.
  
  
    유연성: Transformer는 번역, 요약, 감성 분석 등 다양한 언어 처리 작업에 사용될 수 있습니다. 언어 처리 작업에 있어, 단어 관계를 활용하는 것이라면 광범위하게 활용이 가능합니다.
  
  
    확장성: 데이터에 대해 효과적으로 학습할 수 있으며, 모델의 크기를 쉽게 조절할 수 있어 필요에 따라 더 강력한 모델을 만들 수 있습니다.
  
  
    해석 가능성: Transformer가 어떻게 결정을 내렸는지 알 수 있습니다. LLM 활용에 있어 근거를 알 수 있다는 것은 사용자에게 중요한 일입니다.
  


Transformer는 이처럼 다양한 특징으로 자연어 처리에서 강력한 도구로 자리잡았습니다. 또한 자연어 이해 과정을 통해 다양한 언어 처리 작업을 효과적으로 해결할 수 있던 것입니다.

4. 실제 적용 사례

이런 Transformer는 이미 우리 일상 곳곳에서 활용되고 있습니다. 몇 가지 예를 통해 살펴보도록 하겠습니다.

1. 기계 번역


  구글 번역기: 외국어 문장을 번역할 때 사용하는 이 서비스는 이미 Transformer 기술이 적용되어 번역 정확도를 크게 향상시켰습니다.
  DeepL: 고품질 번역으로 유명한 이 서비스도 Transformer를 사용합니다.


2. 텍스트 요약


  BBC News Labs: 긴 뉴스 기사를 짧게 요약해주는 서비스를 제공합니다.
  Salesforce Einstein: 비즈니스 문서를 자동으로 요약합니다.


3. 감성 분석


  Amazon: 제품 리뷰의 감정을 분석해 고객의 만족도를 파악합니다.
  Twitter: 트윗의 감정을 분석해 여론을 조사합니다.


4. 챗봇


  OpenAI의 GPT 시리즈: 사람과 대화하는 개인화된 맞춤형 AI 대화 서비스를 제공합니다.
  Google의 LaMDA: 사전 학습된 데이터로 특정 분야에 자연스러운 대화를 나누는 AI 대화 서비스를 제공합니다.


5. 코드 생성 및 분석


  GitHub Copilot: 프로그래머를 돕는 AI 도구로, 코드 자동 완성 기능을 제공합니다.
  Google DeepMind의 AlphaCode: 프로그래밍 문제를 해결하는 AI 도구로, 고품질 코드의 작성 기능을 제공합니다.


이처럼 Transformer 기술은 우리 일상 곳곳에서 활용되고 있습니다.

5. 결론

지금까지 Transformer란 무엇인지, 어떻게 작동되며, 어떤 특징이 있고, 어떻게 활용되고 있는지에 대해 공부한 내용을 정리해보았습니다.

Transformer는 문장의 전체적인 맥락을 효과적으로 이해하고 처리할 수 있는 강력한 자연어 처리 기술입니다. 이 기술은 현재도 다음과 서비스 사례와 같이 계속 발전하고 있습니다.


  Longformer, Reformer: 더 긴 텍스트를 효율적으로 처리할 수 있는 모델
  DALL-E, Flamingo: 텍스트뿐만 아니라 이미지도 함께 처리할 수 있는 모델


이런 발전은 Transformer가 앞으로 더 다양한 분야에서 활용될 수 있음을 보여주고, 자연어 처리를 넘어 컴퓨터 비전, 음성 인식, 로보틱스 등 다양한 분야에서 적용을 시도하고 있습니다.

AI는 이미 우리 일상에 파고들어 여러 AI 서비스로 편의를 가져다주고 있으며, 앞으로 더욱 깊게 파고들 AI 서비스는 우리가 부딪혔던 많은 장애물을 제거하고 한계를 넘어서게 해줄 것이라고 생각합니다. 이런 발전 속에서 LLM의 기반 기술인 Transformer를 이해하는 것은 중요한 부분이며, 이 글이 저와 같은 초심자에게 조금이나마 도움이 되었으면 하는 마음으로 글을 마칩니다.

혹시 글에 잘못된 내용이 있다면 댓글로 의견을 남겨주시기 바랍니다. 대환영입니다.

Reference

  Jay Alammar. “The Illustrated Transformer.” 2019.


">



<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dong-jun-shin.github.io/images/CS_AI_ML/2023/12/deep_learning_transfomer/thumbnail.png">


<!-- Open Graph -->
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Nerual Networks 기법 - Transformer의 작동 원리">

<meta property="og:image" content="https://dong-jun-shin.github.io/images/CS_AI_ML/2023/12/deep_learning_transfomer/thumbnail.png">


<meta property="og:description" content="배경

AI/ML에 대해 학습하던 중 Transformer 기술을 접하게 되었습니다. 최근 AI 분야에서 중점적으로 활용되며, 연구되고 있는 이 기술은 LLM과 같은 딥러닝 모델이 어떻게 데이터를 학습하는지 이해하기 위해 필수로 학습해야겠다는 생각이 들었습니다. 처음 접하는 용어가 많았고, 이해가 쉽지 않은 이 기술을 학습하기 위해 초심자의 시각으로 내용을 정리해보았습니다.

설명

1. Transformer란

Transformer는 자연어 문장에 대해 전체를 한 번에 보고, 관계를 파악하며, 중요도를 매기고, 병렬 처리를 할 수 있는 자연어 처리 기술입니다. 전체 문장을 한 번에 보고 모든 단어 간 관계를 동시에 파악하는 것은 마치, 퍼즐을 맞추듯이 각 단어가 다른 단어들과 어떻게 연결되는지를 전체적으로 이해하는 것과 유사한 것 같습니다.

예를 들어 “나는 사과를 먹었다”라는 문장이 있다고 했을 때, Transformer는 이 문장의 모든 단어를 동시에 바라보며, ‘나’와 ‘먹었다’, ‘사과’와 ‘먹었다’ 등의 관계를 한 번에 파악합니다.

Transformer의 주요 특징을 정리하면 다음과 같습니다.


  전체를 한 번에 본다: 문장 전체를 동시에 처리합니다.
  관계를 파악한다: 모든 단어들 사이의 관계를 계산합니다.
  중요도를 매긴다: 각 단어가 다른 단어에 얼마나 중요한지 판단합니다.
  병렬 처리가 가능하다: 여러 작업을 동시에 처리할 수 있어 빠릅니다.


Transformer가 문장을 이해하는 과정

Transformer가 자연어를 이해한다는 것과 어떤 특징이 있는지는 알았지만, 아직 Transformer가 어떻게 문장을 이해하는지는 모르겠습니다. 어떤 원리로 컴퓨터가 문장의 맥락을 이해할 수 있는지 더 자세히 알아보도록 하겠습니다. 여기서는 이해를 위해 “나는 사과를 먹었다”라는 간단한 문장으로 과정을 살펴보겠습니다.

Transformer가 이해하는 과정을 간단하게 도식으로 그려보았습니다.


[자연어를 이해하는 과정]

전체 과정을 보셨다면, 이제 각 과정에 대해 하나씩 단계별로 확인해보겠습니다.

1. 문장 분해

Transformer는 먼저 문장을 개별 단어로 나눕니다.

예: "나는", "사과를", "먹었다"


2. 단어를 숫자로 변환

각 단어는 고유한 숫자(인덱스)로 변환됩니다.

예: "나는" → 1, "사과를" → 2, "먹었다" → 3


3. 숫자를 벡터로 변환

각 숫자는 다시 의미 있는 숫자들의 집합(벡터)으로 바뀝니다. 이를 ‘임베딩’이라고 합니다.

예: 1 → [0.1, 0.2, 0.3], 2 → [0.4, 0.5, 0.6], 3 → [0.7, 0.8, 0.9]


4. 관계 파악

Transformer는 모든 단어 쌍 사이의 관계를 계산합니다. 이를 ‘어텐션’이라고 합니다.

예: "나는"과 "먹었다"의 관계, "사과를"과 "먹었다"의 관계 등을 동시에 계산


5. 중요도 부여

각 관계에 중요도를 부여합니다. 관련성이 높은 단어 쌍은 높은 점수를 받습니다.

예: "사과를"과 "먹었다"의 관계가 "나는"과 "사과를"의 관계보다 높은 점수를 받을 수 있습니다.


6. 정보 종합

이렇게 계산된 관계와 중요도를 바탕으로 각 단어의 최종 의미를 결정합니다.

예: "사과를 먹었다"는 행동과 관련이 높고, "나는 먹었다"는 주체와 행동이 연결됨.


7. 문장 이해

각 단어의 의미를 종합하여 전체 문장의 의미를 파악합니다.

예: "나는 사과를 먹었다"는 문장을 이해하여, 주어가 '나'이고, '사과를 먹는'은 행동을 의미.


이 과정을 통해 Transformer는 “나는 사과를 먹었다”라는 문장에서 누가(나), 무엇을(사과), 어떻게 했는지(먹었다)를 이해할 수 있게 됩니다. 이것은 인간의 언어 이해 과정을 모방했다고 하는데, 인간의 사고 과정을 코드 형태로 구현한다는 것이 참 신기한 것 같습니다.

2. Transformer의 작동 원리

이제 Transformer가 어떤 방식으로 자연어를 이해하는지 알았습니다. 이것을 컴퓨터가 이해할 수 있도록 구현을 해야 하는데, 어떤 식으로 구현하는지 작동 원리도 하나씩 살펴보겠습니다.

2.1 토큰화와 임베딩

Transformer가 텍스트를 이해하는 첫 단계는 토큰화와 임베딩입니다. 이는 마치 우리가 책을 읽을 때 단어를 하나씩 인식하고 그 의미를 파악하는 것과 비슷해요.

1. 토큰화(Tokenization)

토큰화는 문장을 작은 단위(토큰)로 나누는 과정입니다. 이 과정은 1. 문장 분해와 2. 단어를 숫자로 변환에 해당 합니다. 보통 공백을 기준으로 단어 단위를 분할하지만, 때로는 더 작은 단위(subword)로 나누기도 합니다.

예를 들어, “나는 사과를 먹었다”라는 문장을 Transformer에 입력하면 아래처럼 처리됩니다.

- 입력 문장: "나는 사과를 먹었다"
- 토큰화: ['나', '는', '사과', '를', '먹', '었', '다']
- 단어 인덱스: '나'-1, '는'-2, '사과'-3, '를'-4, '먹'-5, '었'-6, '다'-7


2. 임베딩(Embedding)

임베딩은 각 토큰을 컴퓨터가 이해할 수 있는 숫자의 나열(벡터)로 바꾸는 과정입니다. 이 과정은 3. 숫자를 벡터로 변환에 해당 합니다. 이 벡터는 단어의 의미, 문법적 특성 등을 표현해요.

# 임베딩 예시 (실제 값은 더 높은 차원)
- '나': [0.1, 0.2, 0.3]
- '는': [0.4, 0.1, 0.6]
- '사과': [0.2, 0.8, 0.3]


이렇게 각 단어는 숫자의 나열(벡터)로 표현됩니다. 이 숫자들이 바로 컴퓨터가 이해하는 단어의 의미랍니다.

2.2 자기 어텐션(Self-Attention) 메커니즘

자기 어텐션(Self-Attention)은 Transformer의 핵심 부분으로 입력 시퀀스 내의 다른 위치들과의 관계를 고려하여 각 위치의 표현을 계산하는 메커니즘입니다. 이 과정은 4. 관계 파악에 해당합니다. 이 과정에서 각 단어가 다른 단어들과 얼마나 관련이 있는지를 계산합니다.

1. 문맥 연관도(Score) 계산

각 단어 쌍 사이의 관계를 점수로 나타냅니다. 이를 수식으로 표현하면 다음과 같습니다.

score = fi,j / (ti * tj)



  fi,j: 두 단어의 공동 등장 횟수
  ti, tj: 각 단어의 개별 등장 횟수


예를 들어, “나는 사과를 먹었다”라는 문장은 아래와 같습니다.

- '나-는' 관계 점수 = 0.8 (높은 연관성)
- '사과-먹' 관계 점수 = 0.3 (중간 정도의 연관성)
- '나-사과' 관계 점수 = 0.1 (낮은 연관성)


2. 가중치 할당과 갱신

가중치(Weight)는 신경망에서 입력값의 중요도를 조절하는 파라미터입니다. 이 과정은 5. 중요도 부여에 해당합니다. 이 가중치의 업데이트를 위해 손실 함수의 기울기를 의미하는 오차 기울기(Error Gradient)를 활용하게 됩니다.

가중치의 업데이트는 계산된 문맥 연관 정도(Score)에 따라 할당된 α(알파)와 β(베타) 값을 사용해서 이루어지게 됩니다.

- 높은 score (0.5 이상): α = 0.4-0.5
- 중간 score (0.3-0.5): α = 0.2-0.4
- 낮은 score (0.1-0.3): α = 0.1-0.3
- 매우 낮은 score (0.1 미만): β = 0.01-0.3


이 값들은 아래와 같이 가중치를 갱신하는데 사용됩니다.

w ← w + α*δ (δ는 오차 기울기)


3. 자기 어텐션 계산

Query(Q), Key(K), Value(V) 벡터를 사용하여 최종적인 자기 어텐션을 계산합니다. 이 과정은 6. 정보 종합에 해당합니다. (Query, Key, Value는 어텐션 메커니즘에서 사용되는 세 가지 벡터로, Query는 현재 단어, Key와 Value는 모든 단어에 대해 생성됩니다.)

# 자기 어텐션 계산 과정 (의사 코드)
attention_scores = dot_product(Q, K) / sqrt(d_k)
attention_weights = softmax(attention_scores)
output = dot_product(attention_weights, V)


이 과정을 통해 각 단어는 문장 내의 다른 단어들과의 관계를 고려한 새로운 표현을 갖게 됩니다.

2.3 인코더와 디코더

Transformer는 인코더와 디코더라는 두 가지 주요 구성 요소로 이루어져 있습니다. 이 두 구성 요소는 우리가 무의식적으로 하는 언어 이해 과정을 기반으로 모델링된 것으로, 위의 과정을 수행하게 됩니다. 그리고 두 구성 요소가 함께 작동하면서 Transformer는 복잡한 자연어 처리 작업을 수행할 수 있게 되는 것입니다.

1. 인코더(Encoder)

인코더는 여러 레이어로 구성된 네트워크입니다. 주요 역할은 입력 문장의 문맥 정보를 학습하고 인코딩하는 것이에요. 각 레이어에서는 자기 어텐션을 수행하여 문맥 정보를 갱신합니다. (앞서 설명한 3.1 토큰화와 임베딩과 3.2 자기 어텐션(Self-Attention) 메커니즘이 해당)

2. 디코더(Decoder)

디코더는 인코더의 출력을 이용하여 최종 작업(예: 번역, 요약)을 수행하는 네트워크입니다. 디코더도 여러 레이어로 구성되어 있으며, 각 레이어에서는 다음 과정을 통해 인코더의 출력으로 결과(타겟 시퀀스)를 생성합니다.

디코더 과정
1. 디코더는 이전에 생성된 출력을 입력으로 받습니다.
2. 마스크드 멀티-헤드 어텐션을 적용하여 미래 정보를 차단합니다.
3. 인코더-디코더 어텐션을 통해 인코더의 출력과 현재 디코더 상태를 연결합니다.
4. 최종 출력 층에서 다음 토큰을 예측합니다.


3. Transformer의 특징

이제 Transformer가 어떻게 작동하는지까지 대략적으로 살펴봤으니, 특징을 좀 더 세밀하게 알아보겠습니다.


  
    병렬 처리: Transformer는 마치 여러분이 책의 한 페이지를 한 번에 볼 수 있는 것처럼 모든 단어를 동시에 처리할 수 있습니다. 이것은 Transformer가 매우 빠르게 학습할 수 있다는 것을 의미합니다.
  
  
    장거리 의존성 포착: 긴 문장에서도 멀리 떨어진 단어들의 관계를 잘 이해할 수 있습니다. 예를 들어, “나는 어제 산 빨간 사과를 맛있게 먹었다”라는 문장에서 “산”과 “먹었다”의 관계도 쉽게 파악할 수 있습니다.
  
  
    유연성: Transformer는 번역, 요약, 감성 분석 등 다양한 언어 처리 작업에 사용될 수 있습니다. 언어 처리 작업에 있어, 단어 관계를 활용하는 것이라면 광범위하게 활용이 가능합니다.
  
  
    확장성: 데이터에 대해 효과적으로 학습할 수 있으며, 모델의 크기를 쉽게 조절할 수 있어 필요에 따라 더 강력한 모델을 만들 수 있습니다.
  
  
    해석 가능성: Transformer가 어떻게 결정을 내렸는지 알 수 있습니다. LLM 활용에 있어 근거를 알 수 있다는 것은 사용자에게 중요한 일입니다.
  


Transformer는 이처럼 다양한 특징으로 자연어 처리에서 강력한 도구로 자리잡았습니다. 또한 자연어 이해 과정을 통해 다양한 언어 처리 작업을 효과적으로 해결할 수 있던 것입니다.

4. 실제 적용 사례

이런 Transformer는 이미 우리 일상 곳곳에서 활용되고 있습니다. 몇 가지 예를 통해 살펴보도록 하겠습니다.

1. 기계 번역


  구글 번역기: 외국어 문장을 번역할 때 사용하는 이 서비스는 이미 Transformer 기술이 적용되어 번역 정확도를 크게 향상시켰습니다.
  DeepL: 고품질 번역으로 유명한 이 서비스도 Transformer를 사용합니다.


2. 텍스트 요약


  BBC News Labs: 긴 뉴스 기사를 짧게 요약해주는 서비스를 제공합니다.
  Salesforce Einstein: 비즈니스 문서를 자동으로 요약합니다.


3. 감성 분석


  Amazon: 제품 리뷰의 감정을 분석해 고객의 만족도를 파악합니다.
  Twitter: 트윗의 감정을 분석해 여론을 조사합니다.


4. 챗봇


  OpenAI의 GPT 시리즈: 사람과 대화하는 개인화된 맞춤형 AI 대화 서비스를 제공합니다.
  Google의 LaMDA: 사전 학습된 데이터로 특정 분야에 자연스러운 대화를 나누는 AI 대화 서비스를 제공합니다.


5. 코드 생성 및 분석


  GitHub Copilot: 프로그래머를 돕는 AI 도구로, 코드 자동 완성 기능을 제공합니다.
  Google DeepMind의 AlphaCode: 프로그래밍 문제를 해결하는 AI 도구로, 고품질 코드의 작성 기능을 제공합니다.


이처럼 Transformer 기술은 우리 일상 곳곳에서 활용되고 있습니다.

5. 결론

지금까지 Transformer란 무엇인지, 어떻게 작동되며, 어떤 특징이 있고, 어떻게 활용되고 있는지에 대해 공부한 내용을 정리해보았습니다.

Transformer는 문장의 전체적인 맥락을 효과적으로 이해하고 처리할 수 있는 강력한 자연어 처리 기술입니다. 이 기술은 현재도 다음과 서비스 사례와 같이 계속 발전하고 있습니다.


  Longformer, Reformer: 더 긴 텍스트를 효율적으로 처리할 수 있는 모델
  DALL-E, Flamingo: 텍스트뿐만 아니라 이미지도 함께 처리할 수 있는 모델


이런 발전은 Transformer가 앞으로 더 다양한 분야에서 활용될 수 있음을 보여주고, 자연어 처리를 넘어 컴퓨터 비전, 음성 인식, 로보틱스 등 다양한 분야에서 적용을 시도하고 있습니다.

AI는 이미 우리 일상에 파고들어 여러 AI 서비스로 편의를 가져다주고 있으며, 앞으로 더욱 깊게 파고들 AI 서비스는 우리가 부딪혔던 많은 장애물을 제거하고 한계를 넘어서게 해줄 것이라고 생각합니다. 이런 발전 속에서 LLM의 기반 기술인 Transformer를 이해하는 것은 중요한 부분이며, 이 글이 저와 같은 초심자에게 조금이나마 도움이 되었으면 하는 마음으로 글을 마칩니다.

혹시 글에 잘못된 내용이 있다면 댓글로 의견을 남겨주시기 바랍니다. 대환영입니다.

Reference

  Jay Alammar. “The Illustrated Transformer.” 2019.


">
<meta property="og:url" content="https://dong-jun-shin.github.io/2023/12/12/deep_learning_transfomer/">
<meta property="og:site_name" content="Jun's Dev_Blog">
  <link rel="canonical" href="https://dong-jun-shin.github.io/2023/12/12/deep_learning_transfomer/" />
  <link
    rel="alternate"
    type="application/rss+xml"
    title="Jun's Dev_Blog"
    href="https://dong-jun-shin.github.io/feed.xml"
  />
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css?family=Volkhov:400,700" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@200;300;400;500;700;900&display=swap" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Courier+Prime:wght@700&display=swap" rel="stylesheet" />
  <!-- Common -->
  <style>
    
    /*! normalize.css v7.0.0 | MIT License | github.com/necolas/normalize.css */html,body{scroll-behavior:smooth}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{font-weight:normal;letter-spacing:0;margin:0}article,aside,footer,header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}figcaption,figure,main{display:block}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:rgba(0,0,0,0);-webkit-text-decoration-skip:objects}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}dfn{font-style:italic}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-0.25em}sup{top:-0.5em}audio,video{display:inline-block}audio:not([controls]){display:none;height:0}img{border-style:none}svg:not(:root){overflow:hidden}button,input,optgroup,select,textarea{font-family:"Noto Sans KR",sans-serif;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}button,html [type=button],[type=reset],[type=submit]{-webkit-appearance:button}button::-moz-focus-inner,[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring,[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{display:inline-block;vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details,menu{display:block}summary{display:list-item}canvas{display:inline-block}template{display:none}[hidden]{display:none}body,h1,h2,h3,h4,h5,h6,p,blockquote,pre,dl,dd,ol,ul,fieldset,legend,figure,hr{margin:0;padding:0}li>ul,li>ol{margin-bottom:0}table{border:.14em solid #000;border-collapse:collapse;border-spacing:0;word-break:initial;width:100%}table tr:nth-child(even){background-color:#f2fafd}thead{background-color:#a0d0ee}table th{text-align:center;padding:6px 13px;border:.1em solid #757575}table td{padding:6px 13px;border:.1em solid #757575}table tr{padding:6px 13px;border:.1em solid #757575}@-webkit-keyframes spin{100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes spin{100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.icon{position:relative;display:inline-block;width:25px;height:25px;overflow:hidden;fill:currentColor}.icon__cnt{width:100%;height:100%;background:inherit;fill:inherit;pointer-events:none;transform:translateX(0);-ms-transform:translate(0.5px, -0.3px)}.icon--m{width:50px;height:50px}.icon--l{width:100px;height:100px}.icon--xl{width:150px;height:150px}.icon--xxl{width:200px;height:200px}.icon__spinner{position:absolute;top:0;left:0;width:100%;height:100%}.icon--ei-spinner .icon__spinner,.icon--ei-spinner-2 .icon__spinner{-webkit-animation:spin 1s steps(12) infinite;animation:spin 1s steps(12) infinite}.icon--ei-spinner-3 .icon__spinner{-webkit-animation:spin 1.5s linear infinite;animation:spin 1.5s linear infinite}.icon--ei-sc-facebook{fill:#3b5998}.icon--ei-sc-github{fill:#333}.icon--ei-sc-google-plus{fill:#dd4b39}.icon--ei-sc-instagram{fill:#3f729b}.icon--ei-sc-linkedin{fill:#0976b4}.icon--ei-sc-odnoklassniki{fill:#ed812b}.icon--ei-sc-skype{fill:#00aff0}.icon--ei-sc-soundcloud{fill:#f80}.icon--ei-sc-tumblr{fill:#35465c}.icon--ei-sc-twitter{fill:#55acee}.icon--ei-sc-vimeo{fill:#1ab7ea}.icon--ei-sc-vk{fill:#45668e}.icon--ei-sc-youtube{fill:#e52d27}.icon--ei-sc-pinterest{fill:#bd081c}.icon--ei-sc-telegram{fill:#08c}*,*::after,*::before{box-sizing:border-box}h1,h2,h3,h4,h5,h6,ul,ol,dl,blockquote,p,address,hr,table,fieldset,figure,pre{margin-bottom:20px}ul,ol,dd{margin-left:20px}.highlight{background:#f7f7f7}.highlighter-rouge .highlight{background:#0d1117;color:#e6edf3;border-radius:10px}.highlight .c{color:#7ca668;font-style:italic}.highlight .ch{color:#7ca668;font-style:italic}.highlight .cm{color:#7ca668;font-style:italic}.highlight .cp{color:#7ca668;font-style:italic;font-weight:normal}.highlight .cpf{color:#7ca668;font-style:italic}.highlight .c1{color:#7ca668;font-style:italic}.highlight .cs{color:#7ca668;font-style:italic}.highlight .err{color:#f85149}.highlight .esc{color:#e6edf3}.highlight .g{color:#e6edf3}.highlight .k{color:#c586c0}.highlight .l{color:#a5d6ff}.highlight .n{color:#e6edf3}.highlight .o{color:#d4d4d4}.highlight .x{color:#e6edf3}.highlight .p{color:#d4d4d4}.highlight .gd{color:#ffa198;background-color:#490202}.highlight .ge{color:#e6edf3;font-style:italic}.highlight .ges{color:#e6edf3;font-weight:bold;font-style:italic}.highlight .gr{color:#ffa198}.highlight .gh{color:#79c0ff;font-weight:bold}.highlight .gi{color:#56d364;background-color:#0f5323}.highlight .go{color:#8b949e}.highlight .gp{color:#8b949e}.highlight .gs{color:#e6edf3;font-weight:bold}.highlight .gu{color:#79c0ff}.highlight .gt{color:#ff7b72}.highlight .g-Underline{color:#e6edf3;text-decoration:underline}.highlight .kc{color:#79c0ff}.highlight .kd{color:#569cd6}.highlight .kn{color:#ff7b72}.highlight .kp{color:#79c0ff}.highlight .kr{color:#4ec9b0}.highlight .kt{color:#4ec9b0}.highlight .ld{color:#ce9178}.highlight .sd{color:#a5d6ff}.highlight .na{color:#e6edf3}.highlight .nb{color:#4ec9b0}.highlight .nc{color:#4ec9b0;font-weight:bold}.highlight .no{color:#79c0ff;font-weight:bold}.highlight .nd{color:#d2a8ff;font-weight:bold}.highlight .ni{color:#ffa657}.highlight .ne{color:#f0883e;font-weight:bold}.highlight .nf{color:#dcdcaa;font-weight:bold}.highlight .nl{color:#4ec9b0;font-weight:bold}.highlight .nn{color:#ff7b72}.highlight .nx{color:#9cdcfe}.highlight .py{color:#79c0ff}.highlight .nt{color:#4ec9b0}.highlight .nv{color:#79c0ff}.highlight .ow{color:#ff7b72;font-weight:bold}.highlight .pm{color:#e6edf3}.highlight .w{color:#6e7681}.highlight .mb{color:#a5d6ff}.highlight .mf{color:#b5cea8}.highlight .mh{color:#a5d6ff}.highlight .mi{color:#b5cea8}.highlight .mo{color:#a5d6ff}.highlight .sa{color:#79c0ff}.highlight .sb{color:#a5d6ff}.highlight .sc{color:#a5d6ff}.highlight .dl{color:#ce9178}.highlight .sd{color:#a5d6ff}.highlight .s{color:#ce9178}.highlight .s1{color:#ce9178}.highlight .s2{color:#ce9178}.highlight .se{color:#79c0ff}.highlight .sh{color:#79c0ff}.highlight .si{color:#a5d6ff}.highlight .sx{color:#a5d6ff}.highlight .sr{color:#79c0ff}.highlight .ss{color:#a5d6ff}.highlight .bp{color:#e6edf3}.highlight .fm{color:#d2a8ff;font-weight:bold}.highlight .vc{color:#79c0ff}.highlight .vg{color:#79c0ff}.highlight .vi{color:#79c0ff}.highlight .vm{color:#79c0ff}.highlight .il{color:#a5d6ff}body{font-family:"Open Sans",Helvetica Neue,Helvetica,"Noto Sans KR",Arial,sans-serif;font-size:16px;line-height:28px;color:#404040;background-color:#fbfbfb;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}*::selection{color:#fff;background-color:#3af}.toc-container{margin-bottom:20px;display:flex;justify-content:center}.toc{background-color:#fff;border:1px solid #edeeee;border-radius:8px;padding:20px;box-shadow:0 2px 4px rgba(0,0,0,.1);width:100%;font-family:"Noto Sans KR",sans-serif;text-align:left}.toc-title{font-family:"Noto Sans KR",sans-serif;font-size:32px;font-weight:700;color:#05b;margin-bottom:20px;text-align:left;background-color:rgba(204,238,255,.5);padding:15px;border-radius:5px;border:1px solid #ccc}.toc ul{list-style-type:disc;padding-left:15px}.toc a{font-family:"Noto Sans KR",sans-serif;font-size:16px;color:hsl(205,100%,45%);text-decoration:none;display:block;transition:color .3s ease,background-color .3s ease}.toc a:hover{background-color:#eee;color:rgb(0,89.25,153);text-decoration:underline}.toc a:active{color:#05b}.toc .list-group-item.active{background-color:rgba(34,34,34,.8);color:#fff;font-weight:bold;border-radius:5px}.toc .list-group-item.disabled{color:#6c757d;background-color:rgba(0,0,0,0)}.toc .list-group-item+.list-group-item{margin-top:8px}h1,h2,h3,h4,h5,h6{font-family:"Noto Sans KR",serif;font-weight:700;line-height:initial}h1{font-weight:700;font-size:36px;line-height:110%;margin-top:1.6em;margin-bottom:.8em}h2{font-weight:700;font-size:32px;margin-top:1.6em;margin-bottom:.8em}h3{font-weight:700;font-size:28px;margin-top:1.8em;margin-bottom:.9em}h4{font-weight:700;font-size:24px;margin-top:2em;margin-bottom:1em}h5{font-weight:700;font-size:22px;margin-top:2em;margin-bottom:1em;color:#333}h6{font-weight:700;font-size:20px;margin-top:2em;margin-bottom:1em;color:#444}img{max-width:100%;height:auto;vertical-align:middle}img+strong:before{display:inline-block;content:"▲";padding-right:5px;font-size:14px}img+strong{display:block;font-size:14px}p:has(>img):has(>strong){max-width:90%;margin:50px auto;text-align:center}a{text-decoration:none;color:hsl(205,100%,45%);transition:.35s}a:hover{color:rgb(0,89.25,153)}blockquote{padding-left:20px;border-left:4px solid #3af;font-family:"Noto Sans KR",serif;font-style:normal;font-size:14px;background-color:rgb(237.58,248.3,252.32)}blockquote p{padding:10px}hr{height:4px;margin:20px 0;border:0;background-color:#6b6b6b}pre{overflow:auto;padding:14px;font-size:14px;white-space:pre-wrap;word-wrap:break-word;word-break:break-all;font-family:Courier,"Noto Sans KR",monospace}pre.highlight{padding:15px 20px}code{border-radius:10px;overflow:auto;white-space:pre-wrap;word-wrap:break-word;word-break:break-all;font-family:Menlo,Monaco,"Courier New",monospace;font-weight:bold;font-size:14px;vertical-align:middle}p code,li code{color:#eb5757;background-color:#edeeee;margin:0 2px;padding:5px 6px}pre code{font-size:14px;color:#ddd;background-color:rgba(0,0,0,0)}.language-plaintext code{color:#ddd}.o-wrapper{max-width:1440px;position:relative}.o-opacity{animation-duration:.7s;animation-delay:.2s;animation-fill-mode:both;animation-name:opacity}@keyframes opacity{from{opacity:0}to{opacity:1}}.c-btn{display:inline-block;white-space:nowrap;vertical-align:middle;font-family:"Noto Sans KR",serif;font-size:14px;text-align:center;padding:5px 15px;cursor:pointer;transition:.35s}.c-btn--primary{color:#fff;background-color:#3af;background:linear-gradient(135deg, #33aaff 0%, #62d5ff 100%)}.c-btn--secondary{color:#fff;background-color:#cfcfdd;background:linear-gradient(135deg, #a2a2bd 0%, #cfcfdd 100%)}.c-btn--bar{color:#fff;background-color:#444;background:#525252;font-size:14px;width:76%;height:40px}.c-btn--round{border-radius:30px}.c-btn--shadow{box-shadow:8px 10px 20px 0 rgba(46,61,73,.15)}.c-btn--shadow:hover{box-shadow:2px 4px 8px 0px rgba(46,61,73,.2)}.c-btn--middle{display:block;width:300px;max-width:100%}.c-btn--big{display:block;width:100%}.c-btn:hover{color:#fff;transition:.35s}.c-btn:active{transform:translateY(2px)}.c-sidebar{display:flex;flex-direction:column;justify-content:space-between;position:fixed;top:0;left:0;bottom:0;width:360px;padding:40px 20px 20px;text-align:center;box-shadow:1px 1px 0 rgba(31,35,46,.15);background-color:#fff}.c-sidebar-author{display:flex;flex-direction:column}.c-sidebar-author .c-author__cover{width:100px;height:100px;margin:0 auto 10px;border-radius:50%;overflow:hidden;background-color:#cfcfdd}.c-sidebar-author .c-author__cover img{width:100%;height:100%;border-radius:50%;transition:.35s}.c-sidebar-author .c-author__cover img:hover{transform:scale3d(0.9, 0.9, 1)}.c-sidebar-author .c-contact-menu .c-btn{min-width:110px}.c-sidebar-author .c-contact-menu .c-btn .icon{vertical-align:text-bottom;fill:#fff}.c-sidebar-author .c-author__info{font-family:"Noto Sans KR",serif}.c-sidebar-author .c-author__name{font-size:18px;font-weight:700;line-height:21px}.c-sidebar-author .c-author__job{font-size:12px;color:#a0a0a0;margin:5px 0 0}.c-sidebar-author .c-contact-menu{justify-items:center;margin:30px 0px 10px 0px}.c-sidebar-author .c-contact-menu .c-btn{width:130px}.c-sidebar-author .c-contact-menu-bar{justify-items:center;margin:0px 0px 25px 0px}.c-sidebar-author .c-contact-menu-bar .c-btn{font-size:14px;width:260px}.c-sidebar-author .c-author__about{max-width:400px;margin:0 auto 15px;font-size:13px}.c-sidebar-footer .c-social__title{position:relative;font-family:"Noto Sans KR",serif;font-size:16px;font-weight:700;color:#444}.c-sidebar-footer .c-social__title::before{content:"";display:block;height:2px;width:calc(50% - 40px);transform:translateY(-50%);position:absolute;top:50%;left:0;background-color:#444}.c-sidebar-footer .c-social__title::after{content:"";display:block;height:2px;width:calc(50% - 40px);transform:translateY(-50%);position:absolute;top:50%;right:0;background-color:#444}.c-sidebar-footer .c-social__list{list-style-type:none;padding:0;margin:15px 0}.c-sidebar-footer .c-social__list .c-social__item{display:inline-block;width:27px;height:27px}.c-sidebar-footer .c-social__list .icon{width:27px;height:27px;fill:#444;vertical-align:middle;transition:.35s}.c-sidebar-footer .c-social__list .icon:hover{fill:#3af;transform:scale(1.2);transition:.35s}.c-sidebar-footer .c-copyright p{font-size:13px;margin:0}@media only screen and (max-width: 900px){.c-sidebar{position:relative;width:100%;padding:20px}.c-sidebar .c-contact-menu{margin:20px 0}.c-sidebar-footer .c-social .c-social__title{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}.c-sidebar-footer .c-social__list{margin:0}.c-sidebar-footer .c-copyright{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}}@media only screen and (max-width: 480px){.c-sidebar-author{display:flex;flex-direction:column}.c-sidebar-author .c-author__cover{width:80px;height:80px}.c-sidebar-author .c-author__cover img{width:100%;height:100%}.c-sidebar-author .c-contact-menu{justify-items:center;margin:15px 0px 0px 0px;min-width:245px}.c-sidebar-author .c-contact-menu .c-btn{min-width:80px;font-size:12px;width:120px;height:36px;margin-bottom:10px}.c-sidebar-author .c-contact-menu-bar{justify-items:center;margin:0px 0px 25px 0px}.c-sidebar-author .c-contact-menu-bar .c-btn{min-width:80px;font-size:12px;width:244px;height:36px;padding:4px 15px}.c-sidebar-footer .c-social .c-social__title{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}.c-sidebar-footer .c-social__list{margin:0}.c-sidebar-footer .c-social__list .icon{width:25px;height:25px}.c-sidebar-footer .c-copyright{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}}.c-content{position:relative;display:flex;flex-direction:row;flex-wrap:wrap;align-items:stretch;padding:0 20px 0;margin-left:360px}@media only screen and (max-width: 900px){.c-content{position:static;padding:0 15px 0;margin-left:0}}.c-posts{width:100%;display:flex;flex-direction:row;flex-wrap:wrap}.c-post{width:100%;max-width:100%;margin-bottom:20px;display:flex;flex-direction:row;align-items:stretch;min-height:180px;border-radius:10px;overflow:hidden;transition:.35s;background-color:#fff;box-shadow:0 1px 1px 0 rgba(31,35,46,.15)}.c-post:hover{transform:translate(0px, -2px);box-shadow:0 15px 45px -10px rgba(10,16,34,.2)}.c-post .c-post-thumbnail{display:block;width:30%;max-width:100%;min-height:180px;border-radius:10px 0 0 10px;background-color:rgba(220,235,245,.2);background-size:cover;background-position:50% 50%}.c-post .c-post-content{padding:15px;width:70%}.c-post .c-post-content .c-post-title{font-size:30px;font-weight:400;margin:0 0 15px}.c-post .c-post-content .c-post-title a{text-decoration:none;color:#263959}.c-post .c-post-content .c-post-tags{padding:3px 5px;border-radius:3px;background-color:rgba(135,131,120,.2);color:#eb5757;font-size:85%;font-family:"Courier Prime","Noto Sans KR"}.c-post .c-post-content .c-post-date,.c-post .c-post-content .c-post-words{font-size:12px}.c-load-more{padding:20px;margin:20px auto 40px;font-size:13px;color:#fff;border:none;background-color:#3af;outline:none}.c-load-more:hover{background-color:hsl(205,100%,45%)}@media only screen and (max-width: 1200px){.c-post{width:48%;max-width:100%;margin:0 1% 20px;flex-direction:column}.c-post .c-post-thumbnail{width:100%;border-radius:10px 10px 0 0}.c-post .c-post-content{width:100%}.c-post .c-post-content .c-post-title{font-size:21px;margin:15px 0}}@media only screen and (max-width: 480px){.c-post{width:100%;max-width:100%;margin:0 0 20px}.c-post .c-post-content{width:100%}.c-post .c-post-content .c-post-title{font-size:21px;margin:15px 0}}.c-article{width:100%;margin:20px 0}.c-wrap-content{padding:7%;background-color:#fff}.c-article__image{position:relative;background-color:rgba(220,235,245,.2);background-position:center;background-size:cover;background-repeat:no-repeat}.c-article__image:after{content:"";display:block;padding-top:56%}.c-article__header{margin-bottom:60px;padding-bottom:10px;text-align:center;border-bottom:1px solid #6b6b6b}.c-article__header .c-article__title{margin-bottom:10px}.c-article__date span{font-size:13px;text-transform:uppercase;color:#a0a0a0}.c-article__footer{margin:60px 0 0;padding-top:20px;padding-bottom:10px;text-align:center;border-top:1px solid #6b6b6b}.c-article__footer .c-article__share{transition:.35s}.c-article__footer .c-article__share a .icon{vertical-align:middle;transition:.35s}.c-article__footer .c-article__share a .icon:hover{opacity:.7;transition:.35s}.c-article__footer .c-article__tag{margin-bottom:5px}.c-article__footer .c-article__tag a{display:inline-block;vertical-align:middle;padding:5px 10px;font-family:"Noto Sans KR",serif;font-size:10px;line-height:10px;text-transform:uppercase;background-color:rgba(115,138,160,.6);color:#fff}.c-article__footer .c-article__tag a:hover{background-color:rgba(80.2446808511,99.6723404255,118.2553191489,.6)}.c-article__footer .c-article__tag a:last-child{margin-right:0}.c-recent-post{padding:30px 0}.c-recent-post .c-recent__title{font-size:14px;text-align:center;text-transform:uppercase;margin-bottom:30px}.c-recent-post .c-recent__box{display:flex;flex-direction:row;flex-wrap:wrap}.c-recent-post .c-recent__item{max-width:23%;flex-basis:23%;margin:0 1% 20px;border-radius:10px;overflow:hidden;text-align:center;background-color:#fff;box-shadow:0 1px 1px 0 rgba(31,35,46,.15);transition:.35s}.c-recent-post .c-recent__item h4{margin-bottom:5px;font-size:12px;text-transform:uppercase}.c-recent-post .c-recent__item h4 a{color:#444}.c-recent-post .c-recent__item:hover{box-shadow:2px 4px 8px 0px rgba(46,61,73,.2)}.c-recent-post .c-recent__footer{padding:15px}.c-recent-post .c-recent__image{display:block;width:100%;min-height:180px;background-color:rgba(220,235,245,.2);background-size:cover;background-position:center;background-repeat:no-repeat}.c-recent-post .c-recent__date{color:#a0a0a0;font-size:12px}@media only screen and (max-width: 1200px){.c-recent-post .c-recent__item{max-width:48%;flex-basis:48%}}@media only screen and (max-width: 900px){.c-article{margin:15px 0}}@media only screen and (max-width: 480px){.c-wrap-content{padding:15px}.c-article__header{margin-bottom:5px}.c-article__header .c-article__title{font-size:24px;margin-bottom:5px}.c-recent-post .c-recent__item{max-width:100%;flex-basis:100%;margin:0 0 20px}}.c-blog-tags{width:100%;padding:20px;margin:20px 0 40px;background-color:#fff}.c-blog-tags h1{text-align:center;margin-bottom:0}.c-blog-tags h2{font-size:18px;text-transform:uppercase;margin:30px 0;color:#757575}.c-tag__list{list-style:none;padding:0 0 40px;margin:40px 0 0;border-bottom:1px solid #6b6b6b}.c-tag__list li{display:inline-block;margin-right:15px}.c-tag__list li a{color:#404040;text-transform:uppercase;font-size:12px}.c-tag__list li a:hover{color:hsl(0,0%,-4.9019607843%)}.c-tag__item{margin-bottom:15px}.c-tag__image{width:50px;height:50px;border-radius:50%;margin-right:5px}@media only screen and (max-width: 480px){.c-blog-tags{padding:15px}.c-blog-tags h1{font-size:27px}.c-blog-tags h2{font-size:16px;margin:15px 0}.c-tag__list{padding:0 0 30px;margin:30px 0 0}.c-tag__item{margin-bottom:5px}.c-tag__image{display:none}}.c-header{position:relative;width:100%;margin:20px 0}.c-header__box{position:relative;display:flex;flex-direction:row;justify-content:space-between;align-items:center}.c-header__box .icon--ei-search{position:absolute;top:7px;left:15px;fill:#ccc}.c-search{width:80%}.c-search .c-search__box{display:flex;align-items:center}.c-search .c-search__text{position:relative;width:100%;padding:10px 10px 10px 40px;border:1px solid #f2fafd;border-radius:30px;outline:none;color:#a0a0a0}.c-search .c-search__text::placeholder{color:#ccc}.c-search .c-search__text:hover{box-shadow:0 1px 0px rgba(132,135,138,.1);transition:.35s}.c-search .c-search-results-list{position:absolute;width:100%;margin:10px 0 0;list-style-type:none;background-color:#fff;z-index:1}.c-search .c-search-results-list li{display:flex;flex-wrap:wrap;align-items:center;margin:0;padding:20px 25px 0;background-color:#fff;line-height:1.4;border-left:solid 1px #edeeee;border-right:solid 1px #edeeee}.c-search .c-search-results-list li:first-child{border-top-left-radius:5px;border-top-right-radius:5px;border-top:solid 1px #edeeee}.c-search .c-search-results-list li:last-child{border-bottom-left-radius:5px;border-bottom-right-radius:5px;padding-bottom:25px;border-bottom:solid 1px #edeeee}.c-search .c-search-results-list li a{font-size:16px}.c-nav{flex-grow:1;padding-left:20px}.c-nav .c-nav__list{display:flex;justify-content:flex-end}.c-nav .c-nav__list .c-nav__item{display:flex;align-items:center;float:left;padding:4px 10px;font-size:10px;text-transform:uppercase;white-space:nowrap;border:1px solid #f2fafd;box-shadow:0 1px 0px rgba(132,135,138,.4);will-change:transform;transform:translateY(0px);cursor:pointer}.c-nav .c-nav__list .c-nav__item:hover{color:#222;background-color:#fff}.c-nav .c-nav__list .c-nav__item.is-active{box-shadow:0 0 0 rgba(132,135,138,.5);transform:translateY(1px);color:#cfcfdd}.c-nav .c-nav__list .c-nav__item.is-active:hover{background-color:#fbfbfb}.c-nav .c-nav__list .c-nav__item:first-child{border-radius:10px 0 0 10px}.c-nav .c-nav__list .c-nav__item:last-child{border-radius:0 10px 10px 0}.c-nav .c-nav__list .c-nav__item .icon{width:18px;height:18px;margin-right:3px}@media only screen and (max-width: 900px){.c-header{margin:15px 0}}@media only screen and (max-width: 480px){.c-header .c-header__box{flex-direction:column}.c-header .c-search{width:100%}.c-header .c-search .c-search__text{padding:8px 8px 8px 40px}.c-header .c-nav{margin-top:15px}.c-header .c-nav .c-nav__list{justify-content:center}.c-header .c-nav .c-nav__item{padding:4px 8px}}.c-categories{width:100%}.c-categories__list{width:100%;display:flex;flex-direction:row;flex-wrap:wrap}.c-categories__item{max-width:25%;flex-basis:25%;padding:0 10px 20px}.c-categories__link{height:100%;display:flex;flex-direction:column;align-items:center;padding:20px 10px;border-radius:5px;box-shadow:5px 5px 25px rgba(46,61,73,.15);background-color:#fff;transition:.35s}.c-categories__link:hover{box-shadow:2px 4px 8px 0px rgba(46,61,73,.2)}.c-categories__link:hover .c-categories__img .c-categories__more{opacity:1;transition:.35s}.c-categories__link .c-categories__container{width:100%;word-wrap:break-word}.c-categories__link .c-categories__container.c-empty-figure{display:flex;flex-direction:column;justify-content:center;flex-grow:1}.c-categories__img{position:relative;max-width:100%}.c-categories__img figure{position:relative;width:200px;max-width:100%;margin-bottom:20px;overflow:hidden;background-size:cover;background-repeat:no-repeat;background-position:center;border-radius:50%;box-shadow:inset 0 1px 3px rgba(141,165,185,.3)}.c-categories__img figure:after{content:"";display:block;padding-top:100%}.c-categories__img figure:before{content:"";position:absolute;top:0;bottom:0;left:0;right:0;background-color:rgba(0,0,0,.15)}.c-categories__img .c-categories__more{display:inline-block;position:absolute;top:50%;left:50%;transform:translate(-50%, -50%);font-weight:700;color:#fff;text-transform:uppercase;text-shadow:0 1px 0 rgba(104,172,191,.3);opacity:0;transition:.35s}.c-categories__container{text-align:center}.c-categories__container .c-categories__header{font-size:13px;margin-bottom:10px;font-weight:normal;text-transform:uppercase;color:#404040}.c-categories__container .c-categories__count{font-family:"Noto Sans KR",serif;font-size:12px;color:#404040;margin-bottom:0}.c-categories__container .c-categories__count span{display:inline-block;width:20px;height:20px;line-height:20px;vertical-align:baseline;margin-right:5px;border-radius:50%;color:#fff;background-color:#ee6c6c}@media only screen and (max-width: 1200px){.c-categories .c-categories__item{max-width:33.333%;flex-basis:33.333%}}@media only screen and (max-width: 1050px){.c-categories .c-categories__item{max-width:50%;flex-basis:50%}}@media only screen and (max-width: 480px){.c-categories .c-categories__item{max-width:100%;flex-basis:100%;padding:0 0 20px}}.c-form-box{position:absolute;top:0;width:calc(100% - 40px);min-height:100vh;padding:0 20px;background-color:#fff;z-index:1}.c-form-bnt__close{position:absolute;top:0px;left:0;width:30px;height:30px;cursor:pointer;transition:.35s}.c-form-bnt__close:hover{transform:scale(0.8);opacity:.8}.c-form{position:relative;width:750px;max-width:100%;margin:40px auto}.c-form .c-form__title{margin:0 0 40px;min-width:0;border:0;padding:0;font-family:"Noto Sans KR",serif;text-transform:uppercase;text-align:center}.c-form a{color:#404040}.c-form__group{margin-bottom:20px}.c-form__group label{display:block;text-transform:uppercase;font-size:10px}.c-form__group input,.c-form__group textarea{width:100%;padding:10px 15px;color:#404040;border:1px solid #f2fafd;outline:none;transition:.35s}.c-form__group input:focus,.c-form__group textarea:focus{box-shadow:0 4px 25px rgba(132,135,138,.1)}.c-form__group button{padding:20px;text-transform:uppercase;outline:none;border:none}.c-thank-you p{position:relative;padding:20px 40px;width:750px;max-width:100%;text-transform:uppercase;font-size:12px;line-height:18px;font-weight:700;margin:40px auto 0;text-align:center;color:#fff;background:linear-gradient(135deg, #55b5ad 0%, #5ec9c5 100%)}.c-thank-you p .c-form-bnt__close{width:25px;height:25px;background:rgba(0,0,0,0)}.c-thank-you p a{color:#fff}@media only screen and (max-width: 900px){.c-form-box{width:100%;left:0;right:0}}@media only screen and (max-width: 480px){.c-form-bnt__close{width:25px;height:25px}}.c-newsletter{padding:30px 0 60px;margin:0 auto;border-bottom:1px solid #6b6b6b}.c-newsletter__header{text-align:center}.c-newsletter__header .c-newsletter__title{font-size:14px;text-transform:uppercase;text-align:center}.c-newsletter__header .c-newsletter__subtitle{margin-bottom:15px}.c-newsletter-form{width:100%;max-width:750px;margin:0 auto}.c-newsletter-form .c-newsletter-form__group{display:flex}.c-newsletter__email{width:70%;height:40px;padding:10px 15px;border:1px solid #ddd;border-right-color:rgba(0,0,0,0);outline:none;transition:.35s}.c-newsletter__email:focus{box-shadow:0 4px 25px rgba(132,135,138,.1)}.c-newsletter__button{width:30%;height:40px;color:#fff;background-color:#3af;transition:.35s;border:none;outline:none;cursor:pointer}.c-newsletter__button:hover{background-color:hsl(205,100%,45%)}@media only screen and (max-width: 480px){.c-newsletter__button{font-size:13px}}.c-comments{padding:30px 0;border-top:1px solid #6b6b6b}.c-top{position:fixed;width:40px;height:40px;bottom:20px;color:#05b;background-color:#cef;border-radius:50%;cursor:pointer;transition:.35s;right:-100px;z-index:10;opacity:.5}.c-top--active{right:15px}.c-top:hover{color:#757575;opacity:1}.u-text-left{text-align:left}.u-text-right{text-align:right}.u-text-center{text-align:center}.u-text-justify{text-align:justify}.u-block{display:block}.u-inline-block{display:inline-block}.u-inline{display:inline}.u-full-width{display:block;width:100%}.u-vertical-center{display:flex;align-items:center;justify-content:center}.u-responsive-image{max-width:100%;height:auto;vertical-align:middle}.u-show{display:block !important}.u-hide{display:none !important}.u-invisible{visibility:hidden}.u-float-left{float:left}.u-float-right{float:right}.u-no-padding-top{padding-top:0}.u-no-padding-bottom{padding-bottom:0}.u-no-padding-left{padding-left:0}.u-no-padding-right{padding-right:0}.u-no-padding{padding:0}.u-no-margin-top{margin-top:0}.u-no-margin-bottom{margin-bottom:0}.u-no-margin-left{margin-left:0}.u-no-margin-right{margin-right:0}.u-no-margin{margin:0}.u-lists-reset{list-style-type:none;margin:0;padding:0}.u-clearfix::before,.u-clearfix::after{content:"";display:table;clear:both}.u-screen-reader-text{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}
  </style>
  <!-- KaTeX 관련 파일 -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" />
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "$", right: "$", display: false },
          { left: "\\(", right: "\\)", display: false },
          { left: "\\[", right: "\\]", display: true },
        ],
      });
    });
  </script>
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VNMTFT1R2R"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "G-VNMTFT1R2R");
  </script>
</head>


<body>
  
    <script>
  (function (i, s, o, g, r, a, m) {
  i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
    (i[r].q = i[r].q || []).push(arguments)
}, i[r].l = 1 * new Date(); a = s.createElement(o),
    m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'G-VNMTFT1R2R', 'auto');
  ga('send', 'pageview');

</script> <!-- /google analytics -->
  
  <div class="o-wrapper">
    <aside class="c-sidebar">
  <div class="c-sidebar-author">
    <div class="c-author__cover">
      <a href="/">
        <img src="/images/Profile/profile.png" alt="Dong-Jun Shin">
      </a>
    </div>
    <div class="c-author__info">
      <div class="c-author__name">Dong-Jun Shin</div>
      <span class="c-author__job">Web Developer</span>
    </div>
    <div class="c-contact-menu">
      <div style="width: 100%">
        <a href="/about/profile" class="c-contact-btn c-btn c-btn--secondary c-btn--round c-btn--shadow">
          <div style="
            display: flex;
            flex-direction: row;
            align-items: center;">
            <span data-icon='ei-tag' data-size='s'></span>
            <span>About me</span>
          </div>
        </a>
        
        <a target="_blank" href="https://github.com/Dong-Jun-Shin" class="c-btn c-btn--primary c-btn--round c-btn--shadow">
          <div style="
            display: flex;
            flex-direction: row;
            align-items: center;">
            <span data-icon='ei-sc-github' data-size='s'></span>
            <span>Visit Github</span>
          </div>  
        </a>
      </div>
      
    </div>
    <div class="c-contact-menu-bar">
      <div style="width: 100%">
        <a href="/" class="c-contact-btn c-btn c-btn--bar c-btn--round c-btn--shadow">All Posts</a>
      </div>
    </div>
    <p class="c-author__about">개발자로 성장하며 배운 것을 정리한 블로그</p>
  </div>

  <div class="c-sidebar-footer">
    <div class="c-social">
      <div class="c-social__title">Social</div>
      <ul class="c-social__list u-lists-reset">
        
        <li class="c-social__item"><a href="https://www.linkedin.com/in/kr-jun-shin" target="_blank"><div data-icon='ei-sc-linkedin' data-size='s'></div></a></li>
        
        
        
        
        
        
        
        
        
        
          <li class="c-social__item"><a href="https://youtube.com/channel/UCIHM7drY2vvvFhrhXtpbbzw" target="_blank"><div data-icon='ei-sc-youtube' data-size='s'></div></a></li>
        
        
        
      </ul>
    </div>
    <div class="c-copyright">
      <p>2025 &copy; Dong-Jun Shin</p>
      <!-- <a target="_blank" href="https://analytics.google.com/"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fdong-jun-shin.github.io&count_bg=%23FFD540&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=true"/></a> -->
      <a target="_blank" href="https://analytics.google.com/"><img src="https://img.shields.io/badge/Info-Analytics-infomation?style=flat-square&color=yellow"/></a>
      <a target="_blank" href="https://search.google.com/search-console"><img src="https://img.shields.io/badge/Info-Search-informational?style=flat-square"/></a>
    </div>
  </div>
</aside> <!-- /.c-sidebar -->

<main class="c-content">
  <article class="c-article">
  <div class="c-article__content">
    <header class="c-header u-hide u-no-margin-top">
      <div class="c-header__box">
        <div class="c-search u-full-width">
          <div class="c-search__box">
            <label for="js-search-input" class="u-screen-reader-text">Search for Blog</label>
            <input type="text" id="js-search-input" class="c-search__text" autocomplete="off" placeholder="Type to search...">
            <div data-icon='ei-search' data-size='s'></div>
          </div>
          <ul id="js-results-container" class="c-search-results-list"></ul>
        </div>
      </div>
    </header>
    
    <div class="c-article__image o-opacity" style="background-image: url( /images/CS_AI_ML/2023/12/deep_learning_transfomer/thumbnail.png )"></div>
    
    <div class="c-wrap-content">
      <header class="c-article__header">
        <h1 class="c-article__title">Nerual Networks 기법 - Transformer의 작동 원리</h1>
        <div class="c-article__date">
          <span>2023, Dec 12</span>
        </div>
      </header>
      
      <div class="toc-container">
        <div class="toc">
          <div class="toc-title">Index</div>
          <ul><li><a href="#배경">배경</a></li><li><a href="#설명">설명</a><ul><li><a href="#1-transformer란">1. Transformer란</a><ul><li><a href="#transformer가-문장을-이해하는-과정">Transformer가 문장을 이해하는 과정</a></li></ul></li><li><a href="#2-transformer의-작동-원리">2. Transformer의 작동 원리</a><ul><li><a href="#21-토큰화와-임베딩">2.1 토큰화와 임베딩</a></li><li><a href="#22-자기-어텐션self-attention-메커니즘">2.2 자기 어텐션(Self-Attention) 메커니즘</a></li><li><a href="#23-인코더와-디코더">2.3 인코더와 디코더</a></li></ul></li><li><a href="#3-transformer의-특징">3. Transformer의 특징</a></li><li><a href="#4-실제-적용-사례">4. 실제 적용 사례</a><ul><li><a href="#1-기계-번역">1. 기계 번역</a></li><li><a href="#2-텍스트-요약">2. 텍스트 요약</a></li><li><a href="#3-감성-분석">3. 감성 분석</a></li><li><a href="#4-챗봇">4. 챗봇</a></li><li><a href="#5-코드-생성-및-분석">5. 코드 생성 및 분석</a></li></ul></li><li><a href="#5-결론">5. 결론</a></li></ul></li><li><a href="#reference">Reference</a></li></ul>
        </div>
      </div>
      
      <h1 id="배경">배경</h1>

<p>AI/ML에 대해 학습하던 중 Transformer 기술을 접하게 되었습니다. 최근 AI 분야에서 중점적으로 활용되며, 연구되고 있는 이 기술은 LLM과 같은 딥러닝 모델이 어떻게 데이터를 학습하는지 이해하기 위해 필수로 학습해야겠다는 생각이 들었습니다. 처음 접하는 용어가 많았고, 이해가 쉽지 않은 이 기술을 학습하기 위해 초심자의 시각으로 내용을 정리해보았습니다.</p>

<h1 id="설명">설명</h1>

<h2 id="1-transformer란">1. Transformer란</h2>

<p>Transformer는 자연어 문장에 대해 전체를 한 번에 보고, 관계를 파악하며, 중요도를 매기고, 병렬 처리를 할 수 있는 자연어 처리 기술입니다. 전체 문장을 한 번에 보고 모든 단어 간 관계를 동시에 파악하는 것은 마치, 퍼즐을 맞추듯이 각 단어가 다른 단어들과 어떻게 연결되는지를 전체적으로 이해하는 것과 유사한 것 같습니다.</p>

<p>예를 들어 “나는 사과를 먹었다”라는 문장이 있다고 했을 때, Transformer는 이 문장의 모든 단어를 동시에 바라보며, ‘나’와 ‘먹었다’, ‘사과’와 ‘먹었다’ 등의 관계를 한 번에 파악합니다.</p>

<p>Transformer의 주요 특징을 정리하면 다음과 같습니다.</p>

<ol>
  <li><strong>전체를 한 번에 본다</strong>: 문장 전체를 동시에 처리합니다.</li>
  <li><strong>관계를 파악한다</strong>: 모든 단어들 사이의 관계를 계산합니다.</li>
  <li><strong>중요도를 매긴다</strong>: 각 단어가 다른 단어에 얼마나 중요한지 판단합니다.</li>
  <li><strong>병렬 처리가 가능하다</strong>: 여러 작업을 동시에 처리할 수 있어 빠릅니다.</li>
</ol>

<h3 id="transformer가-문장을-이해하는-과정">Transformer가 문장을 이해하는 과정</h3>

<p>Transformer가 자연어를 이해한다는 것과 어떤 특징이 있는지는 알았지만, 아직 Transformer가 어떻게 문장을 이해하는지는 모르겠습니다. 어떤 원리로 컴퓨터가 문장의 맥락을 이해할 수 있는지 더 자세히 알아보도록 하겠습니다. 여기서는 이해를 위해 “나는 사과를 먹었다”라는 간단한 문장으로 과정을 살펴보겠습니다.</p>

<p>Transformer가 이해하는 과정을 간단하게 도식으로 그려보았습니다.</p>

<p><img src="/images/CS_AI_ML/2023/12/deep_learning_transfomer/1.png" alt="" />
<strong>[자연어를 이해하는 과정]</strong></p>

<p>전체 과정을 보셨다면, 이제 각 과정에 대해 하나씩 단계별로 확인해보겠습니다.</p>

<h4 id="1-문장-분해">1. <strong>문장 분해</strong></h4>

<p>Transformer는 먼저 문장을 개별 단어로 나눕니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>예: "나는", "사과를", "먹었다"
</code></pre></div></div>

<h4 id="2-단어를-숫자로-변환">2. <strong>단어를 숫자로 변환</strong></h4>

<p>각 단어는 고유한 숫자(인덱스)로 변환됩니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>예: "나는" → 1, "사과를" → 2, "먹었다" → 3
</code></pre></div></div>

<h4 id="3-숫자를-벡터로-변환">3. <strong>숫자를 벡터로 변환</strong></h4>

<p>각 숫자는 다시 의미 있는 숫자들의 집합(벡터)으로 바뀝니다. 이를 ‘임베딩’이라고 합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>예: 1 → [0.1, 0.2, 0.3], 2 → [0.4, 0.5, 0.6], 3 → [0.7, 0.8, 0.9]
</code></pre></div></div>

<h4 id="4-관계-파악">4. <strong>관계 파악</strong></h4>

<p>Transformer는 모든 단어 쌍 사이의 관계를 계산합니다. 이를 ‘어텐션’이라고 합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>예: "나는"과 "먹었다"의 관계, "사과를"과 "먹었다"의 관계 등을 동시에 계산
</code></pre></div></div>

<h4 id="5-중요도-부여">5. <strong>중요도 부여</strong></h4>

<p>각 관계에 중요도를 부여합니다. 관련성이 높은 단어 쌍은 높은 점수를 받습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>예: "사과를"과 "먹었다"의 관계가 "나는"과 "사과를"의 관계보다 높은 점수를 받을 수 있습니다.
</code></pre></div></div>

<h4 id="6-정보-종합">6. <strong>정보 종합</strong></h4>

<p>이렇게 계산된 관계와 중요도를 바탕으로 각 단어의 최종 의미를 결정합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>예: "사과를 먹었다"는 행동과 관련이 높고, "나는 먹었다"는 주체와 행동이 연결됨.
</code></pre></div></div>

<h4 id="7-문장-이해">7. <strong>문장 이해</strong></h4>

<p>각 단어의 의미를 종합하여 전체 문장의 의미를 파악합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>예: "나는 사과를 먹었다"는 문장을 이해하여, 주어가 '나'이고, '사과를 먹는'은 행동을 의미.
</code></pre></div></div>

<p>이 과정을 통해 Transformer는 “나는 사과를 먹었다”라는 문장에서 누가(나), 무엇을(사과), 어떻게 했는지(먹었다)를 이해할 수 있게 됩니다. 이것은 인간의 언어 이해 과정을 모방했다고 하는데, 인간의 사고 과정을 코드 형태로 구현한다는 것이 참 신기한 것 같습니다.</p>

<h2 id="2-transformer의-작동-원리">2. Transformer의 작동 원리</h2>

<p>이제 Transformer가 어떤 방식으로 자연어를 이해하는지 알았습니다. 이것을 컴퓨터가 이해할 수 있도록 구현을 해야 하는데, 어떤 식으로 구현하는지 작동 원리도 하나씩 살펴보겠습니다.</p>

<h3 id="21-토큰화와-임베딩">2.1 토큰화와 임베딩</h3>

<p>Transformer가 텍스트를 이해하는 첫 단계는 토큰화와 임베딩입니다. 이는 마치 우리가 책을 읽을 때 단어를 하나씩 인식하고 그 의미를 파악하는 것과 비슷해요.</p>

<h4 id="1-토큰화tokenization">1. 토큰화(Tokenization)</h4>

<p>토큰화는 문장을 작은 단위(토큰)로 나누는 과정입니다. 이 과정은 <code class="language-plaintext highlighter-rouge">1. 문장 분해</code>와 <code class="language-plaintext highlighter-rouge">2. 단어를 숫자로 변환</code>에 해당 합니다. 보통 공백을 기준으로 단어 단위를 분할하지만, 때로는 더 작은 단위(subword)로 나누기도 합니다.</p>

<p>예를 들어, “나는 사과를 먹었다”라는 문장을 Transformer에 입력하면 아래처럼 처리됩니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 입력 문장: "나는 사과를 먹었다"
- 토큰화: ['나', '는', '사과', '를', '먹', '었', '다']
- 단어 인덱스: '나'-1, '는'-2, '사과'-3, '를'-4, '먹'-5, '었'-6, '다'-7
</code></pre></div></div>

<h4 id="2-임베딩embedding">2. 임베딩(Embedding)</h4>

<p>임베딩은 각 토큰을 컴퓨터가 이해할 수 있는 숫자의 나열(벡터)로 바꾸는 과정입니다. 이 과정은 <code class="language-plaintext highlighter-rouge">3. 숫자를 벡터로 변환</code>에 해당 합니다. 이 벡터는 단어의 의미, 문법적 특성 등을 표현해요.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 임베딩 예시 (실제 값은 더 높은 차원)
- '나': [0.1, 0.2, 0.3]
- '는': [0.4, 0.1, 0.6]
- '사과': [0.2, 0.8, 0.3]
</code></pre></div></div>

<p>이렇게 각 단어는 숫자의 나열(벡터)로 표현됩니다. 이 숫자들이 바로 컴퓨터가 이해하는 단어의 의미랍니다.</p>

<h3 id="22-자기-어텐션self-attention-메커니즘">2.2 자기 어텐션(Self-Attention) 메커니즘</h3>

<p>자기 어텐션(Self-Attention)은 Transformer의 핵심 부분으로 입력 시퀀스 내의 다른 위치들과의 관계를 고려하여 각 위치의 표현을 계산하는 메커니즘입니다. 이 과정은 <code class="language-plaintext highlighter-rouge">4. 관계 파악</code>에 해당합니다. 이 과정에서 각 단어가 다른 단어들과 얼마나 관련이 있는지를 계산합니다.</p>

<h4 id="1-문맥-연관도score-계산">1. 문맥 연관도(Score) 계산</h4>

<p>각 단어 쌍 사이의 관계를 점수로 나타냅니다. 이를 수식으로 표현하면 다음과 같습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>score = fi,j / (ti * tj)
</code></pre></div></div>

<ul>
  <li>fi,j: 두 단어의 공동 등장 횟수</li>
  <li>ti, tj: 각 단어의 개별 등장 횟수</li>
</ul>

<p>예를 들어, “나는 사과를 먹었다”라는 문장은 아래와 같습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- '나-는' 관계 점수 = 0.8 (높은 연관성)
- '사과-먹' 관계 점수 = 0.3 (중간 정도의 연관성)
- '나-사과' 관계 점수 = 0.1 (낮은 연관성)
</code></pre></div></div>

<h4 id="2-가중치-할당과-갱신">2. 가중치 할당과 갱신</h4>

<p>가중치(Weight)는 신경망에서 입력값의 중요도를 조절하는 파라미터입니다. 이 과정은 <code class="language-plaintext highlighter-rouge">5. 중요도 부여</code>에 해당합니다. 이 가중치의 업데이트를 위해 손실 함수의 기울기를 의미하는 오차 기울기(Error Gradient)를 활용하게 됩니다.</p>

<p>가중치의 업데이트는 계산된 문맥 연관 정도(Score)에 따라 할당된 α(알파)와 β(베타) 값을 사용해서 이루어지게 됩니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 높은 score (0.5 이상): α = 0.4-0.5
- 중간 score (0.3-0.5): α = 0.2-0.4
- 낮은 score (0.1-0.3): α = 0.1-0.3
- 매우 낮은 score (0.1 미만): β = 0.01-0.3
</code></pre></div></div>

<p>이 값들은 아래와 같이 가중치를 갱신하는데 사용됩니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w ← w + α*δ (δ는 오차 기울기)
</code></pre></div></div>

<h4 id="3-자기-어텐션-계산">3. 자기 어텐션 계산</h4>

<p>Query(Q), Key(K), Value(V) 벡터를 사용하여 최종적인 자기 어텐션을 계산합니다. 이 과정은 <code class="language-plaintext highlighter-rouge">6. 정보 종합</code>에 해당합니다. (Query, Key, Value는 어텐션 메커니즘에서 사용되는 세 가지 벡터로, Query는 현재 단어, Key와 Value는 모든 단어에 대해 생성됩니다.)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 자기 어텐션 계산 과정 (의사 코드)
attention_scores = dot_product(Q, K) / sqrt(d_k)
attention_weights = softmax(attention_scores)
output = dot_product(attention_weights, V)
</code></pre></div></div>

<p>이 과정을 통해 각 단어는 문장 내의 다른 단어들과의 관계를 고려한 새로운 표현을 갖게 됩니다.</p>

<h3 id="23-인코더와-디코더">2.3 인코더와 디코더</h3>

<p>Transformer는 인코더와 디코더라는 두 가지 주요 구성 요소로 이루어져 있습니다. 이 두 구성 요소는 우리가 무의식적으로 하는 언어 이해 과정을 기반으로 모델링된 것으로, 위의 과정을 수행하게 됩니다. 그리고 두 구성 요소가 함께 작동하면서 Transformer는 복잡한 자연어 처리 작업을 수행할 수 있게 되는 것입니다.</p>

<h4 id="1-인코더encoder">1. 인코더(Encoder)</h4>

<p>인코더는 여러 레이어로 구성된 네트워크입니다. 주요 역할은 입력 문장의 문맥 정보를 학습하고 인코딩하는 것이에요. 각 레이어에서는 자기 어텐션을 수행하여 문맥 정보를 갱신합니다. (앞서 설명한 <code class="language-plaintext highlighter-rouge">3.1 토큰화와 임베딩</code>과 <code class="language-plaintext highlighter-rouge">3.2 자기 어텐션(Self-Attention) 메커니즘</code>이 해당)</p>

<h4 id="2-디코더decoder">2. 디코더(Decoder)</h4>

<p>디코더는 인코더의 출력을 이용하여 최종 작업(예: 번역, 요약)을 수행하는 네트워크입니다. 디코더도 여러 레이어로 구성되어 있으며, 각 레이어에서는 다음 과정을 통해 인코더의 출력으로 결과(타겟 시퀀스)를 생성합니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>디코더 과정
1. 디코더는 이전에 생성된 출력을 입력으로 받습니다.
2. 마스크드 멀티-헤드 어텐션을 적용하여 미래 정보를 차단합니다.
3. 인코더-디코더 어텐션을 통해 인코더의 출력과 현재 디코더 상태를 연결합니다.
4. 최종 출력 층에서 다음 토큰을 예측합니다.
</code></pre></div></div>

<h2 id="3-transformer의-특징">3. Transformer의 특징</h2>

<p>이제 Transformer가 어떻게 작동하는지까지 대략적으로 살펴봤으니, 특징을 좀 더 세밀하게 알아보겠습니다.</p>

<ol>
  <li>
    <p><strong>병렬 처리</strong>: Transformer는 마치 여러분이 책의 한 페이지를 한 번에 볼 수 있는 것처럼 모든 단어를 동시에 처리할 수 있습니다. 이것은 Transformer가 매우 빠르게 학습할 수 있다는 것을 의미합니다.</p>
  </li>
  <li>
    <p><strong>장거리 의존성 포착</strong>: 긴 문장에서도 멀리 떨어진 단어들의 관계를 잘 이해할 수 있습니다. 예를 들어, “나는 어제 산 빨간 사과를 맛있게 먹었다”라는 문장에서 “산”과 “먹었다”의 관계도 쉽게 파악할 수 있습니다.</p>
  </li>
  <li>
    <p><strong>유연성</strong>: Transformer는 번역, 요약, 감성 분석 등 다양한 언어 처리 작업에 사용될 수 있습니다. 언어 처리 작업에 있어, 단어 관계를 활용하는 것이라면 광범위하게 활용이 가능합니다.</p>
  </li>
  <li>
    <p><strong>확장성</strong>: 데이터에 대해 효과적으로 학습할 수 있으며, 모델의 크기를 쉽게 조절할 수 있어 필요에 따라 더 강력한 모델을 만들 수 있습니다.</p>
  </li>
  <li>
    <p><strong>해석 가능성</strong>: Transformer가 어떻게 결정을 내렸는지 알 수 있습니다. LLM 활용에 있어 근거를 알 수 있다는 것은 사용자에게 중요한 일입니다.</p>
  </li>
</ol>

<p>Transformer는 이처럼 다양한 특징으로 자연어 처리에서 강력한 도구로 자리잡았습니다. 또한 자연어 이해 과정을 통해 다양한 언어 처리 작업을 효과적으로 해결할 수 있던 것입니다.</p>

<h2 id="4-실제-적용-사례">4. 실제 적용 사례</h2>

<p>이런 Transformer는 이미 우리 일상 곳곳에서 활용되고 있습니다. 몇 가지 예를 통해 살펴보도록 하겠습니다.</p>

<h3 id="1-기계-번역">1. 기계 번역</h3>

<ul>
  <li>구글 번역기: 외국어 문장을 번역할 때 사용하는 이 서비스는 이미 Transformer 기술이 적용되어 번역 정확도를 크게 향상시켰습니다.</li>
  <li>DeepL: 고품질 번역으로 유명한 이 서비스도 Transformer를 사용합니다.</li>
</ul>

<h3 id="2-텍스트-요약">2. 텍스트 요약</h3>

<ul>
  <li>BBC News Labs: 긴 뉴스 기사를 짧게 요약해주는 서비스를 제공합니다.</li>
  <li>Salesforce Einstein: 비즈니스 문서를 자동으로 요약합니다.</li>
</ul>

<h3 id="3-감성-분석">3. 감성 분석</h3>

<ul>
  <li>Amazon: 제품 리뷰의 감정을 분석해 고객의 만족도를 파악합니다.</li>
  <li>Twitter: 트윗의 감정을 분석해 여론을 조사합니다.</li>
</ul>

<h3 id="4-챗봇">4. 챗봇</h3>

<ul>
  <li>OpenAI의 GPT 시리즈: 사람과 대화하는 개인화된 맞춤형 AI 대화 서비스를 제공합니다.</li>
  <li>Google의 LaMDA: 사전 학습된 데이터로 특정 분야에 자연스러운 대화를 나누는 AI 대화 서비스를 제공합니다.</li>
</ul>

<h3 id="5-코드-생성-및-분석">5. 코드 생성 및 분석</h3>

<ul>
  <li>GitHub Copilot: 프로그래머를 돕는 AI 도구로, 코드 자동 완성 기능을 제공합니다.</li>
  <li>Google DeepMind의 AlphaCode: 프로그래밍 문제를 해결하는 AI 도구로, 고품질 코드의 작성 기능을 제공합니다.</li>
</ul>

<p>이처럼 Transformer 기술은 우리 일상 곳곳에서 활용되고 있습니다.</p>

<h2 id="5-결론">5. 결론</h2>

<p>지금까지 <code class="language-plaintext highlighter-rouge">Transformer란 무엇인지</code>, <code class="language-plaintext highlighter-rouge">어떻게 작동되며</code>, <code class="language-plaintext highlighter-rouge">어떤 특징이 있고</code>, <code class="language-plaintext highlighter-rouge">어떻게 활용되고 있는지</code>에 대해 공부한 내용을 정리해보았습니다.</p>

<p>Transformer는 문장의 전체적인 맥락을 효과적으로 이해하고 처리할 수 있는 강력한 자연어 처리 기술입니다. 이 기술은 현재도 다음과 서비스 사례와 같이 계속 발전하고 있습니다.</p>

<ul>
  <li>Longformer, Reformer: 더 긴 텍스트를 효율적으로 처리할 수 있는 모델</li>
  <li>DALL-E, Flamingo: 텍스트뿐만 아니라 이미지도 함께 처리할 수 있는 모델</li>
</ul>

<p>이런 발전은 Transformer가 앞으로 더 다양한 분야에서 활용될 수 있음을 보여주고, 자연어 처리를 넘어 컴퓨터 비전, 음성 인식, 로보틱스 등 다양한 분야에서 적용을 시도하고 있습니다.</p>

<p>AI는 이미 우리 일상에 파고들어 여러 AI 서비스로 편의를 가져다주고 있으며, 앞으로 더욱 깊게 파고들 AI 서비스는 우리가 부딪혔던 많은 장애물을 제거하고 한계를 넘어서게 해줄 것이라고 생각합니다. 이런 발전 속에서 LLM의 기반 기술인 Transformer를 이해하는 것은 중요한 부분이며, 이 글이 저와 같은 초심자에게 조금이나마 도움이 되었으면 하는 마음으로 글을 마칩니다.</p>

<p>혹시 글에 잘못된 내용이 있다면 댓글로 의견을 남겨주시기 바랍니다. 대환영입니다.</p>

<h1 id="reference">Reference</h1>
<ul>
  <li><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar. “The Illustrated Transformer.” 2019.</a></li>
</ul>


      <div class="c-article__footer u-clearfix">
        <div class="c-article__tag">
          
          <a href="/tags#DeepLearning">&#35; DeepLearning</a>
          
          <a href="/tags#NN">&#35; NN</a>
          
          <a href="/tags#신경망">&#35; 신경망</a>
          
          <a href="/tags#Transformer">&#35; Transformer</a>
          
        </div>
        <div class="c-article__share">
          <a href="https://twitter.com/intent/tweet?text=Nerual%20Networks%20%EA%B8%B0%EB%B2%95%20-%20Transformer%EC%9D%98%20%EC%9E%91%EB%8F%99%20%EC%9B%90%EB%A6%AC&url=https://dong-jun-shin.github.io/2023/12/12/deep_learning_transfomer/" title="Share
          on Twitter" rel="nofollow" target="_blank"><div data-icon='ei-sc-twitter' data-size='s'></div></a>
          <a href="https://facebook.com/sharer.php?u=https://dong-jun-shin.github.io/2023/12/12/deep_learning_transfomer/" title="Share on Facebook" rel="nofollow" target="_blank"><div data-icon='ei-sc-facebook' data-size='s'></div></a>
          <a href="https://plus.google.com/share?url=https://dong-jun-shin.github.io/2023/12/12/deep_learning_transfomer/" title="Share on Google+" rel="nofollow" target="_blank"><div data-icon='ei-sc-google-plus' data-size='s'></div></a>
        </div>
      </div>
      <div class="c-newsletter">
  <div class="c-newsletter__header">
    <h4 class="c-newsletter__title">Newsletter</h4>
    <div class="c-newsletter__subtitle">Subscribe to this blog and receive notifications of new posts by email.</div>
  </div>
  <form class="c-newsletter-form validate" action="#" method="POST" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" target="_blank" novalidate>
    <div class="c-newsletter-form__group">
      <label class="u-screen-reader-text" for="mce-EMAIL">Email address</label>
      <input class="c-newsletter__email required email" id="mce-EMAIL" type="email" name="EMAIL" placeholder="Your email address" autocomplete="on">
      <input class="c-newsletter__button" id="mc-embedded-subscribe" type="submit" name="subscribe" value="Subscribe">
    </div>
  </form>
</div> <!-- /.c-newsletter -->
      <div class="c-recent-post">
        <h4 class="c-recent__title">You might also enjoy</h4>
        <div class="c-recent__box">
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2025/01/10/permutation_and_combination_implements/" style="background-image: url( /images/CS_Algorithm/2025/01/permutation_and_combination_implements/thumbnail.png)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2025/01/10/permutation_and_combination_implements/">순열과 조합 - 2, 경우의 수 뿐만 아니라 경우를 직접 구해보자</a></h4>
              <div class="c-recent__date">
                <time datetime="2025-01-10T17:45:03+09:00">January 10, 2025</time>
              </div>
            </div>
          </div>
        
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2025/01/10/permutation_and_combination_theory/" style="background-image: url( /images/CS_Algorithm/2025/01/permutation_and_combination_theory/thumbnail.png)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2025/01/10/permutation_and_combination_theory/">순열과 조합 - 1, 어떤 원리로 경우의 수를 계산할 수 있는걸까?</a></h4>
              <div class="c-recent__date">
                <time datetime="2025-01-10T17:45:02+09:00">January 10, 2025</time>
              </div>
            </div>
          </div>
        
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2024/08/21/2_years_of_experience_from_2022/" style="background-image: url( /images/Life/2024/08/2_years_of_experience_from_2022/thumbnail_retrospective-journey.jpg)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2024/08/21/2_years_of_experience_from_2022/">퇴사 회고(라 쓰고, 2022년 ~ 2024년 정리)</a></h4>
              <div class="c-recent__date">
                <time datetime="2024-08-21T23:47:38+09:00">August 21, 2024</time>
              </div>
            </div>
          </div>
        
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2024/05/21/Aws_Summit_Seoul_2024/" style="background-image: url( /images/IT_Tech/2024/05/Aws_Summit_Seoul_2024/thumbnail.png)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2024/05/21/Aws_Summit_Seoul_2024/">AWS Summit Seoul 2024 방문기</a></h4>
              <div class="c-recent__date">
                <time datetime="2024-05-21T21:55:48+09:00">May 21, 2024</time>
              </div>
            </div>
          </div>
        
        
        </div>
      </div> <!-- /.c-recent-post -->
      
        <div class="c-comments">
  <div id="disqus_thread" class="article-comments"></div>
  <script>
    (function () {
      var d = document, s = d.createElement('script');
      s.src = '//dong-jun-shin-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
</div> <!-- /.c-comments -->
      
    </div> <!-- /.c-wrap-content -->
  </div> <!-- /.c-article__content -->
</article> <!-- /.c-article-page -->

</main> <!-- /.c-content -->
  </div> <!-- /.o-wrapper -->
  <div class="c-top" data-icon='ei-chevron-up' data-size='s' title="Scroll To Top"></div> <!-- /.c-top -->
  <script src="/js/jquery-3.3.1.min.js"></script>
<script src="/js/evil-icons.min.js"></script>
<script src="/js/jquery.fitvids.js"></script>
<script src="/js/simple-jekyll-search.min.js"></script>
<script src="/js/main.js"></script>
<!-- /javascripts -->
</body>
</html>