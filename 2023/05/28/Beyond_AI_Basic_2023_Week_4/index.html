<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>코칭스터디 Beyond AI Basic 2023 - 4주차 학습</title>
  <meta
    name="description"
    content="CNNCNN - Convolution은 무엇인가?Convolution  이미지를 특정 형태로 변환하는 것  용어 정의          I(Image): 이미지      K(Kernel): 적용할 컨볼루션 필터      O(Output): 필터가 적용된 이미지      Dimensi..."
  />
  <!-- Twitter Cards -->
<meta name="twitter:title" content="코칭스터디 Beyond AI Basic 2023 - 4주차 학습">
<meta name="twitter:description" content="CNN

CNN - Convolution은 무엇인가?

Convolution


  이미지를 특정 형태로 변환하는 것
  용어 정의
    
      I(Image): 이미지
      K(Kernel): 적용할 컨볼루션 필터
      O(Output): 필터가 적용된 이미지
      Dimension = Channel = Depth: 층
    
  
  원리
    
      필터를 Image의 시작부터 크기대로 적용해서 Output을 얻음
        
          3x3 filter가 있고, 7x7 image가 있는 경우
            
              필터의 크기인 Image(1, 1) ~ Image(3, 3)의 범위에 적용하여 Output(1, 1)을 얻음
            
          
        
      
      필터를 적용 시, Depth는 항상 타겟과 동일한 것을 사용
      필터는 Output의 Depth만큼 필요함
    
  




RGB Image Convolution


  수학적으로 표현할 때, tensor(32x32)로 표현됨
  R, G, B 표현을 위해 Depth는 3으로 구성
  e.g. I(32x32x3), K_R(5x5x3), K_G(5x5x3), K_B(5x5x3) -&gt; O(28x28x3)




Stack of Convolutions


  여러개의 Convolutions을 적용하는 것




Convolutional Neural Networks


  구성
    
      convolution layer
        
          도장을 찍듯이 한 부분을 선택하여 훑어서 값을 얻는 층
        
      
      pooling layer
        
          convolution layer로 얻은 값들을 다 합쳐서 최종적인 값을 출력함
        
      
      fully connected layer (dense layer)
        
          분류, 회귀를 통해 원하는 값을 얻는 층
        
      
    
  
  각 구성 요소의 목적
    
      convolution layer, pooling layer는 이미지에서 유용한 정보를 추출하는데 사용
      fully connected layer는 의사 결정을 하기 위한 값을 추출하는데 사용
    
  
  CNN 성능 향상을 위한 목표
    
      convolution layer를 최대한 많이 쌓고, 파라미터 숫자와 관련있는 fully connected layer를 최소로 하는데 집중
    
  




Convolution Arithmetic


  Output size를 구하는 수식
    
      W: width, F: Filter, S: Stride
      (W-F)/S+1
    
  
  Patch size / Stride
    
      다음 필터를 적용하기까지의 이동 간격
      e.g. 1이면 한 칸씩 이동
    
  
  Padding
    
      추가적으로 부여하는 가상의 테두리 크기
      e.g. 1이면 한 칸씩 더 붙임
    
  
  params
    
      convolution layer의 param 개수
        
          K(NxMxD) O_depth(출력 Depth) N(나누어진 네트워크 개수)
        
      
      fully connected layer의 param 개수
        
          I(NxMxD) C(channel 개수) O_depth(출력 Depth) * N(나누어진 네트워크 개수)
        
      
    
  




Exercise


  GPU 메모리 크기가 부족할 경우, 네트워크가 여러개로 나누어져 각각의 GPU로 병렬 수행될 수 있음
    
      N(나누어진 네트워크 수)를 곱해야 전체 파라미터를 계산할 수 있음
    
  
  Kernel이 모든 위치에 1:1 매핑되기 때문에 fully connected layer가 더 많은 param 개수를 가짐
    
      Convolution operator는 모두 동일하게 적용되기 때문에 일종의 Shared parameter임
    
  




1X1 Convolution


  이미지에서 1픽셀씩 매치시키고 Channel 방향으로 줄이는 방법
  Dimension reduction
    
      Convolution(1x1)을 이용해서 이전 Dimension(NxM)을 유지하면서 Channel(Depth)만 줄임
      이를 통해, depth를 증가시키면서 parameter 개수를 줄일 수 있음
    
  
  e.g. bottleneck architecture




Modern CNN - 1x1 convolution의 중요성


  layer는 깊게 쌓으면서 param을 줄인 방법




ILSVRC


  ImageNet Large-Scale Visual Recognition Challenge
    
      Classification(분류) / Detection(물체 인식) / Localization(하나의 물체 인식)/ Segmentation(pixel 단위로 특징을 분류)
      Classification
        
          AlexNet / VGG / ResNet
          e.g. image가 강아지 사진인지 맞추는 문제
        
      
      Localization / Detection
        
          R-CNN / Fast R-CNN / Faster R-CNN
          e.g. image 안에 강아지가 어디 있는지 찾는 문제
        
      
      Segmentation
        
          FCN / DeepLab
          e.g. image를 보고 하나가 아닌 전체를 이해하는 문제
        
      
    

    
ILSVRC 예시
  
  1000개 정도의 분류
  1백만개 이상의 이미지들
  훈련 데이터로 약 46만장 사용




AlexNet


  특징
    
      Rectified Linear Unit(ReLU) activation function을 사용
        
          0보다 작으면 0, 크면 그대로 사용
          activation function의 특징인 Non-linear 성질을 가지고 있음
          e.g. max(0, x)
        
      
      2개의 GPU 사용
      Local response normalization(LRN)
        
          ReLU로 0이상의 값이 주변 픽셀에 영향주는 것을 방지하기 위해 정규화하는 방법
          측면 억제 원리를 사용(lateral inhibition)
          현재는 잘 사용되지 않음
        
      
      Overlapping pooling
        
          3x3 영역을 2픽셀 단위로 pooling해서 조금씩 겹치는 부분을 만들어 처리하는 방법
        
      
      Data Augmentation
        
          overfitting을 방지하기 위해 학습 데이터를 증가시키는 방법
        
      
      Dropout
        
          랜덤하게 일부 데이터를 제외하는 방법
        
      
    
  




VGGNet


  특징
    
      3x3 convolution filters(with stride 1)만 사용
        
          3x3 2번하는 게, 5x5로 filter 했을 때보다 params의 receptive field를 유지하면서 네트워크를 더 깊게 쌓을 수 있음
        
      
      1x1 convolution for fully connected layers
      Dropout (p=0.5)
      VGG16, VGG19
    
  




GoogLeNet


  특징
    
      1x1 convolution filters를 사용해서 Params를 줄임
        
          inception blocks을 활용한 network-in-network(NIN) 구조
          inception blocks
            
              Input에 3x3, 5x5 conv를 적용하기 전에 1x1 conv를 적용하는 구성
              1x1를 중간에 사용해서 3x3, 5x5에 적용되는 Depth를 줄여 전체적인 params를 줄임
            
          
        
      
    
  




ResNet


  특징
    
      데이터를 학습할 때, 기존 학습된 데이터랑 다른 차이만을 학습한 방법
      네트워크가 커짐에 따라, 오히려 학습 에러가 커져서 학습을 더 시킬 수 없는 상황을 개선하기 위해 identity map이라는 개념을 추가
      identity map (Residual connection, skip connection)
        
          network의 출력 또는 1x1 conv에 input 결과의 차이만 더함
          종류
            
              Simple Shortcut
              Projected Shortcut
            
          
        
      
      Bottleneck architecture
        
          3x3 2번 conv하는 것에서 params를 줄이기 위해, 1x1을 먼저 적용하여 3x3의 input을 줄이고 input을 늘리기 위해 Depth를 늘린 1x1을 다시 conv함
        
      
    
  




DenseNet


  ResNet에서 차이를 더하면 값이 섞이니, 섞지 말고 이어주는 방법
  이어줄 경우, channel이 커짐에 따라 param이 기하급수적으로 늘어날 수 있어서 Transition Block으로 param을 줄여줌
  구조
    
      Dense Block
        
          각 학습의 차이를 이어주는 층
        
      
      Transition Block
        
          1x1 conv를 해서 param을 줄여주는 층
        
      
    
  




Summary


  VGG: 3x3 conv를 사용.
  GoogLeNet: 1x1 conv를 사용. params를 줄일 수 있음
  ResNet: skip-connection 방법 사용. 네트워크를 깊이 쌓을 수 있음
  DenseNet: concatenation으로 차이를 쌓는 방법 사용.




Computer Vision Applications

Semantic Segmentation


  pixel마다 분류하여 labeling하는 것




Fully Convolutional Network


  Convolutionalization(컨볼루션화)
    
      Fully Connected Layer(dense layer)를 없애기 위해 convolution을 이용한 방법
      Fully Connected Layer(dense layer) 대신 conv filter를 적용해서 conv layer로 변환
        
          convolution이 가지는 shared parameter의 성질로 인해, input 이미지의 크기에 상관없이 conv filter를 적용 가능
        
      
      변환된 conv layer는 2차원 정보(이미지의 객체와 위치)를 가지고 있음 (결과가 heat map과 유사)
    
  




Deconvolution


  conv 연산을 하면 output은 절반으로 줄게 되는데 conv로 줄어든 output을 다시 늘려주는 것




Detection

R-CNN


  CNN에 입력할 때 고정된 크기로 Crop 또는 Warp한 뒤 네트워크에 입력하는 방법
  Crop 또는 Warp하는 과정에서 객체가 가지고 있는 형태적인 정보가 왜곡될 수 있음
  순서
    
      이미지를 받음
      이미지 안에서 2000개의 region을 랜덤하게 선택
      AlexNet으로 각 region의 특징을 계산
      linear SVMs로 분류
    
  




SPPNet


  Spatial Pyramid Pooling을 사용해서, R-CNN의 과도한 연산량 문제와 정보 왜곡 문제를 해결한 방법
    
      Spatial Pyramid Pooling은 잘라낸 영역에 대해 미리 정한 영역(1분할, 4분할, …)으로 나누고 각 결과를 고정된 크기의 벡터로 이어서 반환하는 방법
    
  
  이미지 안에서 CNN을 한 번만 함




Fast R-CNN


  binary SVM + bounding box regression을 하나로 합쳐서 multi-pipeline구조의 번거로움을 제거하고 최적화를 위한 방법
    
      기존 물체 분류를 위해 binary SVM을 학습하고, 객체 위치를 보정하기 위해 bounding box regression을 별도로 학습해야 하는 단점이 있었음
    
  
  구조 개선으로 네트워크 추론 시간이 짧아짐
  개선점
    
      binary SVM 대신 FC layer와 softmax를 활용
      CNN으로 얻은 Output(feature vector)를 bounding box regression의 입력으로 사용
    
  




Faster R-CNN


  하나로 합쳐진 bounding box regression와 Region Proposal(객체 후보)도 하나로 합쳐서 Selective Search 알고리즘의 시간을 없앤 방법
    
      기존 Region Proposal을 얻기 위해 시간이 오래 걸리는 Selective Search 알고리즘 연산이 필요했음
    
  
  객체 후보 추정 시간이 짧아짐
  개선점
    
      Selective Search 알고리즘을 사용한 Region Proposal 대신 객체 후보 영역도 학습하는 네트워크(RPN)을 사용
      Region Proposal Network(RPN)
        
          입력받은 feature map의 마지막 conv 층을 sliding해서 저차원으로 매핑하고 Regression과 classification을 수행
            
              이미지 안에 어떤 물체가 있을 지 미리 알고 템플릿을 만들어 두고 템플릿이 바뀌는 정도를 보고 판단해서 Region Proposal과 score를 출력
            
          
        
      
    
  




YOLO


  이미지 자체를 가지고 분류를 하는 방법
  분리, 분류를 한번에 하기 때문에 빠름
  이미지를 SxS으로 나누어 bonding box를 찾고, 실제로 의미있는지를 동시에 예측
  Tensor = SxSx(B*5+C)
    
      SxS: 그리드에서 나눈 셀 개수
      
        
          
            
              B*5: 박스 크기(offsets
              x,y,w,h)와 중요한 정도(confidence)
            
          
        
      
      C: classes 개수
    
  




RNN - Sequential Models

RNN

Recurrent Neural Network


  Markov를 기반으로 만들어져 시계열 데이터 처리에 특화된 방법
  Sequential Data
    
      몇 차원인지, 몇 개의 입력인지 알지 못하는 연속적인 데이터
    
  
  Sequential Model
    
      Sequential Data를 고려해서 다음 데이터가 무엇인지 예측하는 모델
      Naive sequence model
        
          늘어나는 과거 데이터를 모두 고려하여 다음 데이터를 예측
        
      
      Autoregressive model
        
          과거 N개만 정해서 고려하고 다음 데이터를 예측
        
      
      Markov model(first-order autoregressive model)
        
          바로 전 과거만 보고 고려하여 다음 데이터를 예측
        
      
      Latent autoregressive model
        
          과거 데이터 중간에 이전 데이터까지 요약된 Hidden state를 두고, Hidden state를 이용해서 다음 데이터를 예측
        
      
    
  
  RNN을 만들 때는 Sigmoid나 ReLU를 사용해서 Hidden state를 구하면 문제가 있기 때문에 Tanh를 사용
    
      Sigmoid를 사용하면 값이 줄어드는 문제가 있음(Vanising)
      ReLU를 사용하면 Weight가 양수일 때, 단계를 거듭하며 값이 너무 커져서 학습할 때 네트워크가 폭발하는 문제가 있음(exploding gradient)
    
  
  Transformer가 나온 이후, 잘 사용되지 않음




Long Short Term Memory(LSTM)


  Short-term dependencies / Long-term Dependencies 문제를 해결하기 위해 나온 방법
    
      요약 과정에서 과거 데이터가 점점 희미해져서 현재에 고려되기 어려움
      Short-term을 해결할 경우 Long-term Dependencies 문제가 생김
    
  
  Previous cell state, Previous hidden state, X_t를 Sigmoid, Tanh 연산이 포함된 각 Gate에 통과시켜 적절한 출력을 얻음
  용어 정의
    
      Previous cell state(이전 데이터까지 요약된 정보)
      Previous hidden state(이전의 출력값)
      X_t(현재 값)
    
  
  Gate
    
      데이터의 사용 조작 여부를 판단하는 것
    
  
  Gate 구성
    
      Forget gate
        
          Previous cell state 중 어떤 정보를 버릴 지 판단
        
      
      Input gate
        
          현재 입력 중 어떤 정보를 사용할 지 판단
          Update cell
            
              Forget gate, Input gate의 출력 결과를 업데이트해서 사용할 것을 구분해서 cell state를 구성
            
          
        
      
      Output gate
        
          업데이트한 cell state로 현재 hidden state를 만듦
        
      
    
  




Gated Recurrent Unit(GRU)


  cell state 없이, hidden state로만 구현한 방법
  파라미터가 적어 학습 성능이 좋음
  Gate 구성
    
      Reset gate
      Update gate
    
  




Transformer


  재귀적 구조 없이 attention 구조를 활용해서 번역, 문장 구성, 단어 등 Sequential Data를 다루는 일에 사용하는 방법
  특징
    
      입력과 출력 Sequence의 수가 다를 수 있음
      1개의 모델로 구성되어 있음
      attention 구조
        
          입력을 한번에 처리하는 구조
          같은 개수의 Encoder, Decoder로 구성
          Encoder
            
              Self-Attention
                
                  각 Embedding 입력을 고려하여 입력 개수만큼의 Vector 출력을 반환
                    
                      e.g. x1, x2, x3 &gt; z1(x1, x2, x3), z2(x1, x2, x3), z3(x1, x2, x3)
                    
                  
                  Queries vector, Keys vector, Values vector를 이용해서 x1을 변환
                    
                      생성 과정에서 각각의 Neural Network로 입력 개수별 3개의 Vector를 만듦
                        
                          Queries vector와 Keys vector를 내적해서 score를 구함
                            
                              내적을 위해 2개 vector는 항상 차원이 같아야 함
                            
                          
                          score가 의미있는 범위에 들어가도록 계산하기 위해, keys dimension에 제곱근을 하고 score를 나눠서 Normalize함
                          그리고 Normalize된 score에 softmax를 연산해서 attention rate를 측정
                          vector 간의 유사도인 attention rate에 value vectors의 weighted sum을 하면 z1을 구할 수 있음
                            
                              value의 dimension은 queries dimension이나 keys dimension과 같지 않아도 됨
                            
                          
                        
                      
                    
                  
                  문장 관계 파악 예시에서 행렬로 표현할 때
                    
                      x의 row는 단어의 개수가 될 수 있고, x의 col은 각 단어가 가지는 Embedding dimension을 의미
                      keys vector에서 col은 attention dimension을 의미
                    
                  
                
              
              Feed Forward Neural Network
                
                  기존 NN과 유사
                
              
            
          
          Decoder
            
              attention map을 만들기 위해 Input에 해당하는 Keys vector, Values vector를 전달 받아 최종 출력인 Decoder에 전달함
              Linear + Softmax를 해서 출력
            
          
        
      
    
  
  많이 사용되는 라이브러리로 HuggingFace가 있음




Vision Transformer


  Transformer를 활용해서 이미지 분류하는 방법
  Transformer Encoder와 Multi-head attention(MHA)를 거쳐 이미지 분류
  활용 예시 (DALL-E)
    
      문장을 가지고 이미지를 만듦
        
          Transformer Encoder만 이용
          이미지를 분할한 Sequential Data와 문장으로 구성된 Sequential Data를 입력으로 사용
        
      
    
  




Reference


  제대로 된 인공지능 스터디를 찾고 있다면? AI 심화 스터디
  [코칭스터디 10기] Beyond AI Basic 2023 스터디 전용강좌
  ILSVRC 예시


">



<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://dong-jun-shin.github.io/images/CS_AI_ML/logo.png">


<!-- Open Graph -->
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="코칭스터디 Beyond AI Basic 2023 - 4주차 학습">

<meta property="og:image" content="https://dong-jun-shin.github.io/images/CS_AI_ML/logo.png">


<meta property="og:description" content="CNN

CNN - Convolution은 무엇인가?

Convolution


  이미지를 특정 형태로 변환하는 것
  용어 정의
    
      I(Image): 이미지
      K(Kernel): 적용할 컨볼루션 필터
      O(Output): 필터가 적용된 이미지
      Dimension = Channel = Depth: 층
    
  
  원리
    
      필터를 Image의 시작부터 크기대로 적용해서 Output을 얻음
        
          3x3 filter가 있고, 7x7 image가 있는 경우
            
              필터의 크기인 Image(1, 1) ~ Image(3, 3)의 범위에 적용하여 Output(1, 1)을 얻음
            
          
        
      
      필터를 적용 시, Depth는 항상 타겟과 동일한 것을 사용
      필터는 Output의 Depth만큼 필요함
    
  




RGB Image Convolution


  수학적으로 표현할 때, tensor(32x32)로 표현됨
  R, G, B 표현을 위해 Depth는 3으로 구성
  e.g. I(32x32x3), K_R(5x5x3), K_G(5x5x3), K_B(5x5x3) -&gt; O(28x28x3)




Stack of Convolutions


  여러개의 Convolutions을 적용하는 것




Convolutional Neural Networks


  구성
    
      convolution layer
        
          도장을 찍듯이 한 부분을 선택하여 훑어서 값을 얻는 층
        
      
      pooling layer
        
          convolution layer로 얻은 값들을 다 합쳐서 최종적인 값을 출력함
        
      
      fully connected layer (dense layer)
        
          분류, 회귀를 통해 원하는 값을 얻는 층
        
      
    
  
  각 구성 요소의 목적
    
      convolution layer, pooling layer는 이미지에서 유용한 정보를 추출하는데 사용
      fully connected layer는 의사 결정을 하기 위한 값을 추출하는데 사용
    
  
  CNN 성능 향상을 위한 목표
    
      convolution layer를 최대한 많이 쌓고, 파라미터 숫자와 관련있는 fully connected layer를 최소로 하는데 집중
    
  




Convolution Arithmetic


  Output size를 구하는 수식
    
      W: width, F: Filter, S: Stride
      (W-F)/S+1
    
  
  Patch size / Stride
    
      다음 필터를 적용하기까지의 이동 간격
      e.g. 1이면 한 칸씩 이동
    
  
  Padding
    
      추가적으로 부여하는 가상의 테두리 크기
      e.g. 1이면 한 칸씩 더 붙임
    
  
  params
    
      convolution layer의 param 개수
        
          K(NxMxD) O_depth(출력 Depth) N(나누어진 네트워크 개수)
        
      
      fully connected layer의 param 개수
        
          I(NxMxD) C(channel 개수) O_depth(출력 Depth) * N(나누어진 네트워크 개수)
        
      
    
  




Exercise


  GPU 메모리 크기가 부족할 경우, 네트워크가 여러개로 나누어져 각각의 GPU로 병렬 수행될 수 있음
    
      N(나누어진 네트워크 수)를 곱해야 전체 파라미터를 계산할 수 있음
    
  
  Kernel이 모든 위치에 1:1 매핑되기 때문에 fully connected layer가 더 많은 param 개수를 가짐
    
      Convolution operator는 모두 동일하게 적용되기 때문에 일종의 Shared parameter임
    
  




1X1 Convolution


  이미지에서 1픽셀씩 매치시키고 Channel 방향으로 줄이는 방법
  Dimension reduction
    
      Convolution(1x1)을 이용해서 이전 Dimension(NxM)을 유지하면서 Channel(Depth)만 줄임
      이를 통해, depth를 증가시키면서 parameter 개수를 줄일 수 있음
    
  
  e.g. bottleneck architecture




Modern CNN - 1x1 convolution의 중요성


  layer는 깊게 쌓으면서 param을 줄인 방법




ILSVRC


  ImageNet Large-Scale Visual Recognition Challenge
    
      Classification(분류) / Detection(물체 인식) / Localization(하나의 물체 인식)/ Segmentation(pixel 단위로 특징을 분류)
      Classification
        
          AlexNet / VGG / ResNet
          e.g. image가 강아지 사진인지 맞추는 문제
        
      
      Localization / Detection
        
          R-CNN / Fast R-CNN / Faster R-CNN
          e.g. image 안에 강아지가 어디 있는지 찾는 문제
        
      
      Segmentation
        
          FCN / DeepLab
          e.g. image를 보고 하나가 아닌 전체를 이해하는 문제
        
      
    

    
ILSVRC 예시
  
  1000개 정도의 분류
  1백만개 이상의 이미지들
  훈련 데이터로 약 46만장 사용




AlexNet


  특징
    
      Rectified Linear Unit(ReLU) activation function을 사용
        
          0보다 작으면 0, 크면 그대로 사용
          activation function의 특징인 Non-linear 성질을 가지고 있음
          e.g. max(0, x)
        
      
      2개의 GPU 사용
      Local response normalization(LRN)
        
          ReLU로 0이상의 값이 주변 픽셀에 영향주는 것을 방지하기 위해 정규화하는 방법
          측면 억제 원리를 사용(lateral inhibition)
          현재는 잘 사용되지 않음
        
      
      Overlapping pooling
        
          3x3 영역을 2픽셀 단위로 pooling해서 조금씩 겹치는 부분을 만들어 처리하는 방법
        
      
      Data Augmentation
        
          overfitting을 방지하기 위해 학습 데이터를 증가시키는 방법
        
      
      Dropout
        
          랜덤하게 일부 데이터를 제외하는 방법
        
      
    
  




VGGNet


  특징
    
      3x3 convolution filters(with stride 1)만 사용
        
          3x3 2번하는 게, 5x5로 filter 했을 때보다 params의 receptive field를 유지하면서 네트워크를 더 깊게 쌓을 수 있음
        
      
      1x1 convolution for fully connected layers
      Dropout (p=0.5)
      VGG16, VGG19
    
  




GoogLeNet


  특징
    
      1x1 convolution filters를 사용해서 Params를 줄임
        
          inception blocks을 활용한 network-in-network(NIN) 구조
          inception blocks
            
              Input에 3x3, 5x5 conv를 적용하기 전에 1x1 conv를 적용하는 구성
              1x1를 중간에 사용해서 3x3, 5x5에 적용되는 Depth를 줄여 전체적인 params를 줄임
            
          
        
      
    
  




ResNet


  특징
    
      데이터를 학습할 때, 기존 학습된 데이터랑 다른 차이만을 학습한 방법
      네트워크가 커짐에 따라, 오히려 학습 에러가 커져서 학습을 더 시킬 수 없는 상황을 개선하기 위해 identity map이라는 개념을 추가
      identity map (Residual connection, skip connection)
        
          network의 출력 또는 1x1 conv에 input 결과의 차이만 더함
          종류
            
              Simple Shortcut
              Projected Shortcut
            
          
        
      
      Bottleneck architecture
        
          3x3 2번 conv하는 것에서 params를 줄이기 위해, 1x1을 먼저 적용하여 3x3의 input을 줄이고 input을 늘리기 위해 Depth를 늘린 1x1을 다시 conv함
        
      
    
  




DenseNet


  ResNet에서 차이를 더하면 값이 섞이니, 섞지 말고 이어주는 방법
  이어줄 경우, channel이 커짐에 따라 param이 기하급수적으로 늘어날 수 있어서 Transition Block으로 param을 줄여줌
  구조
    
      Dense Block
        
          각 학습의 차이를 이어주는 층
        
      
      Transition Block
        
          1x1 conv를 해서 param을 줄여주는 층
        
      
    
  




Summary


  VGG: 3x3 conv를 사용.
  GoogLeNet: 1x1 conv를 사용. params를 줄일 수 있음
  ResNet: skip-connection 방법 사용. 네트워크를 깊이 쌓을 수 있음
  DenseNet: concatenation으로 차이를 쌓는 방법 사용.




Computer Vision Applications

Semantic Segmentation


  pixel마다 분류하여 labeling하는 것




Fully Convolutional Network


  Convolutionalization(컨볼루션화)
    
      Fully Connected Layer(dense layer)를 없애기 위해 convolution을 이용한 방법
      Fully Connected Layer(dense layer) 대신 conv filter를 적용해서 conv layer로 변환
        
          convolution이 가지는 shared parameter의 성질로 인해, input 이미지의 크기에 상관없이 conv filter를 적용 가능
        
      
      변환된 conv layer는 2차원 정보(이미지의 객체와 위치)를 가지고 있음 (결과가 heat map과 유사)
    
  




Deconvolution


  conv 연산을 하면 output은 절반으로 줄게 되는데 conv로 줄어든 output을 다시 늘려주는 것




Detection

R-CNN


  CNN에 입력할 때 고정된 크기로 Crop 또는 Warp한 뒤 네트워크에 입력하는 방법
  Crop 또는 Warp하는 과정에서 객체가 가지고 있는 형태적인 정보가 왜곡될 수 있음
  순서
    
      이미지를 받음
      이미지 안에서 2000개의 region을 랜덤하게 선택
      AlexNet으로 각 region의 특징을 계산
      linear SVMs로 분류
    
  




SPPNet


  Spatial Pyramid Pooling을 사용해서, R-CNN의 과도한 연산량 문제와 정보 왜곡 문제를 해결한 방법
    
      Spatial Pyramid Pooling은 잘라낸 영역에 대해 미리 정한 영역(1분할, 4분할, …)으로 나누고 각 결과를 고정된 크기의 벡터로 이어서 반환하는 방법
    
  
  이미지 안에서 CNN을 한 번만 함




Fast R-CNN


  binary SVM + bounding box regression을 하나로 합쳐서 multi-pipeline구조의 번거로움을 제거하고 최적화를 위한 방법
    
      기존 물체 분류를 위해 binary SVM을 학습하고, 객체 위치를 보정하기 위해 bounding box regression을 별도로 학습해야 하는 단점이 있었음
    
  
  구조 개선으로 네트워크 추론 시간이 짧아짐
  개선점
    
      binary SVM 대신 FC layer와 softmax를 활용
      CNN으로 얻은 Output(feature vector)를 bounding box regression의 입력으로 사용
    
  




Faster R-CNN


  하나로 합쳐진 bounding box regression와 Region Proposal(객체 후보)도 하나로 합쳐서 Selective Search 알고리즘의 시간을 없앤 방법
    
      기존 Region Proposal을 얻기 위해 시간이 오래 걸리는 Selective Search 알고리즘 연산이 필요했음
    
  
  객체 후보 추정 시간이 짧아짐
  개선점
    
      Selective Search 알고리즘을 사용한 Region Proposal 대신 객체 후보 영역도 학습하는 네트워크(RPN)을 사용
      Region Proposal Network(RPN)
        
          입력받은 feature map의 마지막 conv 층을 sliding해서 저차원으로 매핑하고 Regression과 classification을 수행
            
              이미지 안에 어떤 물체가 있을 지 미리 알고 템플릿을 만들어 두고 템플릿이 바뀌는 정도를 보고 판단해서 Region Proposal과 score를 출력
            
          
        
      
    
  




YOLO


  이미지 자체를 가지고 분류를 하는 방법
  분리, 분류를 한번에 하기 때문에 빠름
  이미지를 SxS으로 나누어 bonding box를 찾고, 실제로 의미있는지를 동시에 예측
  Tensor = SxSx(B*5+C)
    
      SxS: 그리드에서 나눈 셀 개수
      
        
          
            
              B*5: 박스 크기(offsets
              x,y,w,h)와 중요한 정도(confidence)
            
          
        
      
      C: classes 개수
    
  




RNN - Sequential Models

RNN

Recurrent Neural Network


  Markov를 기반으로 만들어져 시계열 데이터 처리에 특화된 방법
  Sequential Data
    
      몇 차원인지, 몇 개의 입력인지 알지 못하는 연속적인 데이터
    
  
  Sequential Model
    
      Sequential Data를 고려해서 다음 데이터가 무엇인지 예측하는 모델
      Naive sequence model
        
          늘어나는 과거 데이터를 모두 고려하여 다음 데이터를 예측
        
      
      Autoregressive model
        
          과거 N개만 정해서 고려하고 다음 데이터를 예측
        
      
      Markov model(first-order autoregressive model)
        
          바로 전 과거만 보고 고려하여 다음 데이터를 예측
        
      
      Latent autoregressive model
        
          과거 데이터 중간에 이전 데이터까지 요약된 Hidden state를 두고, Hidden state를 이용해서 다음 데이터를 예측
        
      
    
  
  RNN을 만들 때는 Sigmoid나 ReLU를 사용해서 Hidden state를 구하면 문제가 있기 때문에 Tanh를 사용
    
      Sigmoid를 사용하면 값이 줄어드는 문제가 있음(Vanising)
      ReLU를 사용하면 Weight가 양수일 때, 단계를 거듭하며 값이 너무 커져서 학습할 때 네트워크가 폭발하는 문제가 있음(exploding gradient)
    
  
  Transformer가 나온 이후, 잘 사용되지 않음




Long Short Term Memory(LSTM)


  Short-term dependencies / Long-term Dependencies 문제를 해결하기 위해 나온 방법
    
      요약 과정에서 과거 데이터가 점점 희미해져서 현재에 고려되기 어려움
      Short-term을 해결할 경우 Long-term Dependencies 문제가 생김
    
  
  Previous cell state, Previous hidden state, X_t를 Sigmoid, Tanh 연산이 포함된 각 Gate에 통과시켜 적절한 출력을 얻음
  용어 정의
    
      Previous cell state(이전 데이터까지 요약된 정보)
      Previous hidden state(이전의 출력값)
      X_t(현재 값)
    
  
  Gate
    
      데이터의 사용 조작 여부를 판단하는 것
    
  
  Gate 구성
    
      Forget gate
        
          Previous cell state 중 어떤 정보를 버릴 지 판단
        
      
      Input gate
        
          현재 입력 중 어떤 정보를 사용할 지 판단
          Update cell
            
              Forget gate, Input gate의 출력 결과를 업데이트해서 사용할 것을 구분해서 cell state를 구성
            
          
        
      
      Output gate
        
          업데이트한 cell state로 현재 hidden state를 만듦
        
      
    
  




Gated Recurrent Unit(GRU)


  cell state 없이, hidden state로만 구현한 방법
  파라미터가 적어 학습 성능이 좋음
  Gate 구성
    
      Reset gate
      Update gate
    
  




Transformer


  재귀적 구조 없이 attention 구조를 활용해서 번역, 문장 구성, 단어 등 Sequential Data를 다루는 일에 사용하는 방법
  특징
    
      입력과 출력 Sequence의 수가 다를 수 있음
      1개의 모델로 구성되어 있음
      attention 구조
        
          입력을 한번에 처리하는 구조
          같은 개수의 Encoder, Decoder로 구성
          Encoder
            
              Self-Attention
                
                  각 Embedding 입력을 고려하여 입력 개수만큼의 Vector 출력을 반환
                    
                      e.g. x1, x2, x3 &gt; z1(x1, x2, x3), z2(x1, x2, x3), z3(x1, x2, x3)
                    
                  
                  Queries vector, Keys vector, Values vector를 이용해서 x1을 변환
                    
                      생성 과정에서 각각의 Neural Network로 입력 개수별 3개의 Vector를 만듦
                        
                          Queries vector와 Keys vector를 내적해서 score를 구함
                            
                              내적을 위해 2개 vector는 항상 차원이 같아야 함
                            
                          
                          score가 의미있는 범위에 들어가도록 계산하기 위해, keys dimension에 제곱근을 하고 score를 나눠서 Normalize함
                          그리고 Normalize된 score에 softmax를 연산해서 attention rate를 측정
                          vector 간의 유사도인 attention rate에 value vectors의 weighted sum을 하면 z1을 구할 수 있음
                            
                              value의 dimension은 queries dimension이나 keys dimension과 같지 않아도 됨
                            
                          
                        
                      
                    
                  
                  문장 관계 파악 예시에서 행렬로 표현할 때
                    
                      x의 row는 단어의 개수가 될 수 있고, x의 col은 각 단어가 가지는 Embedding dimension을 의미
                      keys vector에서 col은 attention dimension을 의미
                    
                  
                
              
              Feed Forward Neural Network
                
                  기존 NN과 유사
                
              
            
          
          Decoder
            
              attention map을 만들기 위해 Input에 해당하는 Keys vector, Values vector를 전달 받아 최종 출력인 Decoder에 전달함
              Linear + Softmax를 해서 출력
            
          
        
      
    
  
  많이 사용되는 라이브러리로 HuggingFace가 있음




Vision Transformer


  Transformer를 활용해서 이미지 분류하는 방법
  Transformer Encoder와 Multi-head attention(MHA)를 거쳐 이미지 분류
  활용 예시 (DALL-E)
    
      문장을 가지고 이미지를 만듦
        
          Transformer Encoder만 이용
          이미지를 분할한 Sequential Data와 문장으로 구성된 Sequential Data를 입력으로 사용
        
      
    
  




Reference


  제대로 된 인공지능 스터디를 찾고 있다면? AI 심화 스터디
  [코칭스터디 10기] Beyond AI Basic 2023 스터디 전용강좌
  ILSVRC 예시


">
<meta property="og:url" content="https://dong-jun-shin.github.io/2023/05/28/Beyond_AI_Basic_2023_Week_4/">
<meta property="og:site_name" content="Jun's Dev_Blog">
  <link rel="canonical" href="https://dong-jun-shin.github.io/2023/05/28/Beyond_AI_Basic_2023_Week_4/" />
  <link
    rel="alternate"
    type="application/rss+xml"
    title="Jun's Dev_Blog"
    href="https://dong-jun-shin.github.io/feed.xml"
  />
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css?family=Volkhov:400,700" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@200;300;400;500;700;900&display=swap" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Courier+Prime:wght@700&display=swap" rel="stylesheet" />
  <!-- Common -->
  <style>
    
    /*! normalize.css v7.0.0 | MIT License | github.com/necolas/normalize.css */html,body{scroll-behavior:smooth}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{font-weight:normal;letter-spacing:0;margin:0}article,aside,footer,header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}figcaption,figure,main{display:block}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:rgba(0,0,0,0);-webkit-text-decoration-skip:objects}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}dfn{font-style:italic}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-0.25em}sup{top:-0.5em}audio,video{display:inline-block}audio:not([controls]){display:none;height:0}img{border-style:none}svg:not(:root){overflow:hidden}button,input,optgroup,select,textarea{font-family:"Noto Sans KR",sans-serif;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}button,html [type=button],[type=reset],[type=submit]{-webkit-appearance:button}button::-moz-focus-inner,[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring,[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{display:inline-block;vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details,menu{display:block}summary{display:list-item}canvas{display:inline-block}template{display:none}[hidden]{display:none}body,h1,h2,h3,h4,h5,h6,p,blockquote,pre,dl,dd,ol,ul,fieldset,legend,figure,hr{margin:0;padding:0}li>ul,li>ol{margin-bottom:0}table{border:.14em solid #000;border-collapse:collapse;border-spacing:0;word-break:initial;width:100%}table tr:nth-child(even){background-color:#f2fafd}thead{background-color:#a0d0ee}table th{text-align:center;padding:6px 13px;border:.1em solid #757575}table td{padding:6px 13px;border:.1em solid #757575}table tr{padding:6px 13px;border:.1em solid #757575}@-webkit-keyframes spin{100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes spin{100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.icon{position:relative;display:inline-block;width:25px;height:25px;overflow:hidden;fill:currentColor}.icon__cnt{width:100%;height:100%;background:inherit;fill:inherit;pointer-events:none;transform:translateX(0);-ms-transform:translate(0.5px, -0.3px)}.icon--m{width:50px;height:50px}.icon--l{width:100px;height:100px}.icon--xl{width:150px;height:150px}.icon--xxl{width:200px;height:200px}.icon__spinner{position:absolute;top:0;left:0;width:100%;height:100%}.icon--ei-spinner .icon__spinner,.icon--ei-spinner-2 .icon__spinner{-webkit-animation:spin 1s steps(12) infinite;animation:spin 1s steps(12) infinite}.icon--ei-spinner-3 .icon__spinner{-webkit-animation:spin 1.5s linear infinite;animation:spin 1.5s linear infinite}.icon--ei-sc-facebook{fill:#3b5998}.icon--ei-sc-github{fill:#333}.icon--ei-sc-google-plus{fill:#dd4b39}.icon--ei-sc-instagram{fill:#3f729b}.icon--ei-sc-linkedin{fill:#0976b4}.icon--ei-sc-odnoklassniki{fill:#ed812b}.icon--ei-sc-skype{fill:#00aff0}.icon--ei-sc-soundcloud{fill:#f80}.icon--ei-sc-tumblr{fill:#35465c}.icon--ei-sc-twitter{fill:#55acee}.icon--ei-sc-vimeo{fill:#1ab7ea}.icon--ei-sc-vk{fill:#45668e}.icon--ei-sc-youtube{fill:#e52d27}.icon--ei-sc-pinterest{fill:#bd081c}.icon--ei-sc-telegram{fill:#08c}*,*::after,*::before{box-sizing:border-box}h1,h2,h3,h4,h5,h6,ul,ol,dl,blockquote,p,address,hr,table,fieldset,figure,pre{margin-bottom:20px}ul,ol,dd{margin-left:20px}.highlight{background:#f7f7f7}.highlighter-rouge .highlight{background:#0d1117;color:#e6edf3;border-radius:10px}.highlight .c{color:#7ca668;font-style:italic}.highlight .ch{color:#7ca668;font-style:italic}.highlight .cm{color:#7ca668;font-style:italic}.highlight .cp{color:#7ca668;font-style:italic;font-weight:normal}.highlight .cpf{color:#7ca668;font-style:italic}.highlight .c1{color:#7ca668;font-style:italic}.highlight .cs{color:#7ca668;font-style:italic}.highlight .err{color:#f85149}.highlight .esc{color:#e6edf3}.highlight .g{color:#e6edf3}.highlight .k{color:#c586c0}.highlight .l{color:#a5d6ff}.highlight .n{color:#e6edf3}.highlight .o{color:#d4d4d4}.highlight .x{color:#e6edf3}.highlight .p{color:#d4d4d4}.highlight .gd{color:#ffa198;background-color:#490202}.highlight .ge{color:#e6edf3;font-style:italic}.highlight .ges{color:#e6edf3;font-weight:bold;font-style:italic}.highlight .gr{color:#ffa198}.highlight .gh{color:#79c0ff;font-weight:bold}.highlight .gi{color:#56d364;background-color:#0f5323}.highlight .go{color:#8b949e}.highlight .gp{color:#8b949e}.highlight .gs{color:#e6edf3;font-weight:bold}.highlight .gu{color:#79c0ff}.highlight .gt{color:#ff7b72}.highlight .g-Underline{color:#e6edf3;text-decoration:underline}.highlight .kc{color:#79c0ff}.highlight .kd{color:#569cd6}.highlight .kn{color:#ff7b72}.highlight .kp{color:#79c0ff}.highlight .kr{color:#4ec9b0}.highlight .kt{color:#4ec9b0}.highlight .ld{color:#ce9178}.highlight .sd{color:#a5d6ff}.highlight .na{color:#e6edf3}.highlight .nb{color:#4ec9b0}.highlight .nc{color:#4ec9b0;font-weight:bold}.highlight .no{color:#79c0ff;font-weight:bold}.highlight .nd{color:#d2a8ff;font-weight:bold}.highlight .ni{color:#ffa657}.highlight .ne{color:#f0883e;font-weight:bold}.highlight .nf{color:#dcdcaa;font-weight:bold}.highlight .nl{color:#4ec9b0;font-weight:bold}.highlight .nn{color:#ff7b72}.highlight .nx{color:#9cdcfe}.highlight .py{color:#79c0ff}.highlight .nt{color:#4ec9b0}.highlight .nv{color:#79c0ff}.highlight .ow{color:#ff7b72;font-weight:bold}.highlight .pm{color:#e6edf3}.highlight .w{color:#6e7681}.highlight .mb{color:#a5d6ff}.highlight .mf{color:#b5cea8}.highlight .mh{color:#a5d6ff}.highlight .mi{color:#b5cea8}.highlight .mo{color:#a5d6ff}.highlight .sa{color:#79c0ff}.highlight .sb{color:#a5d6ff}.highlight .sc{color:#a5d6ff}.highlight .dl{color:#ce9178}.highlight .sd{color:#a5d6ff}.highlight .s{color:#ce9178}.highlight .s1{color:#ce9178}.highlight .s2{color:#ce9178}.highlight .se{color:#79c0ff}.highlight .sh{color:#79c0ff}.highlight .si{color:#a5d6ff}.highlight .sx{color:#a5d6ff}.highlight .sr{color:#79c0ff}.highlight .ss{color:#a5d6ff}.highlight .bp{color:#e6edf3}.highlight .fm{color:#d2a8ff;font-weight:bold}.highlight .vc{color:#79c0ff}.highlight .vg{color:#79c0ff}.highlight .vi{color:#79c0ff}.highlight .vm{color:#79c0ff}.highlight .il{color:#a5d6ff}body{font-family:"Open Sans",Helvetica Neue,Helvetica,"Noto Sans KR",Arial,sans-serif;font-size:16px;line-height:28px;color:#404040;background-color:#fbfbfb;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}*::selection{color:#fff;background-color:#3af}.toc-container{margin-bottom:20px;display:flex;justify-content:center}.toc{background-color:#fff;border:1px solid #edeeee;border-radius:8px;padding:20px;box-shadow:0 2px 4px rgba(0,0,0,.1);width:100%;font-family:"Noto Sans KR",sans-serif;text-align:left}.toc-title{font-family:"Noto Sans KR",sans-serif;font-size:32px;font-weight:700;color:#05b;margin-bottom:20px;text-align:left;background-color:rgba(204,238,255,.5);padding:15px;border-radius:5px;border:1px solid #ccc}.toc ul{list-style-type:disc;padding-left:15px}.toc a{font-family:"Noto Sans KR",sans-serif;font-size:16px;color:hsl(205,100%,45%);text-decoration:none;display:block;transition:color .3s ease,background-color .3s ease}.toc a:hover{background-color:#eee;color:rgb(0,89.25,153);text-decoration:underline}.toc a:active{color:#05b}.toc .list-group-item.active{background-color:rgba(34,34,34,.8);color:#fff;font-weight:bold;border-radius:5px}.toc .list-group-item.disabled{color:#6c757d;background-color:rgba(0,0,0,0)}.toc .list-group-item+.list-group-item{margin-top:8px}h1,h2,h3,h4,h5,h6{font-family:"Noto Sans KR",serif;font-weight:700;line-height:initial}h1{font-weight:700;font-size:36px;line-height:110%;margin-top:1.6em;margin-bottom:.8em}h2{font-weight:700;font-size:32px;margin-top:1.6em;margin-bottom:.8em}h3{font-weight:700;font-size:28px;margin-top:1.8em;margin-bottom:.9em}h4{font-weight:700;font-size:24px;margin-top:2em;margin-bottom:1em}h5{font-weight:700;font-size:22px;margin-top:2em;margin-bottom:1em;color:#333}h6{font-weight:700;font-size:20px;margin-top:2em;margin-bottom:1em;color:#444}img{max-width:100%;height:auto;vertical-align:middle}img+strong:before{display:inline-block;content:"▲";padding-right:5px;font-size:14px}img+strong{display:block;font-size:14px}p:has(>img):has(>strong){max-width:90%;margin:50px auto;text-align:center}a{text-decoration:none;color:hsl(205,100%,45%);transition:.35s}a:hover{color:rgb(0,89.25,153)}blockquote{padding-left:20px;border-left:4px solid #3af;font-family:"Noto Sans KR",serif;font-style:normal;font-size:14px;background-color:rgb(237.58,248.3,252.32)}blockquote p{padding:10px}hr{height:4px;margin:20px 0;border:0;background-color:#6b6b6b}pre{overflow:auto;padding:14px;font-size:14px;white-space:pre-wrap;word-wrap:break-word;word-break:break-all;font-family:Courier,"Noto Sans KR",monospace}pre.highlight{padding:15px 20px}code{border-radius:10px;overflow:auto;white-space:pre-wrap;word-wrap:break-word;word-break:break-all;font-family:Menlo,Monaco,"Courier New",monospace;font-weight:bold;font-size:14px;vertical-align:middle}p code,li code{color:#eb5757;background-color:#edeeee;margin:0 2px;padding:5px 6px}pre code{font-size:14px;color:#ddd;background-color:rgba(0,0,0,0)}.language-plaintext code{color:#ddd}.o-wrapper{max-width:1440px;position:relative}.o-opacity{animation-duration:.7s;animation-delay:.2s;animation-fill-mode:both;animation-name:opacity}@keyframes opacity{from{opacity:0}to{opacity:1}}.c-btn{display:inline-block;white-space:nowrap;vertical-align:middle;font-family:"Noto Sans KR",serif;font-size:14px;text-align:center;padding:5px 15px;cursor:pointer;transition:.35s}.c-btn--primary{color:#fff;background-color:#3af;background:linear-gradient(135deg, #33aaff 0%, #62d5ff 100%)}.c-btn--secondary{color:#fff;background-color:#cfcfdd;background:linear-gradient(135deg, #a2a2bd 0%, #cfcfdd 100%)}.c-btn--bar{color:#fff;background-color:#444;background:#525252;font-size:14px;width:76%;height:40px}.c-btn--round{border-radius:30px}.c-btn--shadow{box-shadow:8px 10px 20px 0 rgba(46,61,73,.15)}.c-btn--shadow:hover{box-shadow:2px 4px 8px 0px rgba(46,61,73,.2)}.c-btn--middle{display:block;width:300px;max-width:100%}.c-btn--big{display:block;width:100%}.c-btn:hover{color:#fff;transition:.35s}.c-btn:active{transform:translateY(2px)}.c-sidebar{display:flex;flex-direction:column;justify-content:space-between;position:fixed;top:0;left:0;bottom:0;width:360px;padding:40px 20px 20px;text-align:center;box-shadow:1px 1px 0 rgba(31,35,46,.15);background-color:#fff}.c-sidebar-author{display:flex;flex-direction:column}.c-sidebar-author .c-author__cover{width:100px;height:100px;margin:0 auto 10px;border-radius:50%;overflow:hidden;background-color:#cfcfdd}.c-sidebar-author .c-author__cover img{width:100%;height:100%;border-radius:50%;transition:.35s}.c-sidebar-author .c-author__cover img:hover{transform:scale3d(0.9, 0.9, 1)}.c-sidebar-author .c-contact-menu .c-btn{min-width:110px}.c-sidebar-author .c-contact-menu .c-btn .icon{vertical-align:text-bottom;fill:#fff}.c-sidebar-author .c-author__info{font-family:"Noto Sans KR",serif}.c-sidebar-author .c-author__name{font-size:18px;font-weight:700;line-height:21px}.c-sidebar-author .c-author__job{font-size:12px;color:#a0a0a0;margin:5px 0 0}.c-sidebar-author .c-contact-menu{justify-items:center;margin:30px 0px 10px 0px}.c-sidebar-author .c-contact-menu .c-btn{width:130px}.c-sidebar-author .c-contact-menu-bar{justify-items:center;margin:0px 0px 25px 0px}.c-sidebar-author .c-contact-menu-bar .c-btn{font-size:14px;width:260px}.c-sidebar-author .c-author__about{max-width:400px;margin:0 auto 15px;font-size:13px}.c-sidebar-footer .c-social__title{position:relative;font-family:"Noto Sans KR",serif;font-size:16px;font-weight:700;color:#444}.c-sidebar-footer .c-social__title::before{content:"";display:block;height:2px;width:calc(50% - 40px);transform:translateY(-50%);position:absolute;top:50%;left:0;background-color:#444}.c-sidebar-footer .c-social__title::after{content:"";display:block;height:2px;width:calc(50% - 40px);transform:translateY(-50%);position:absolute;top:50%;right:0;background-color:#444}.c-sidebar-footer .c-social__list{list-style-type:none;padding:0;margin:15px 0}.c-sidebar-footer .c-social__list .c-social__item{display:inline-block;width:27px;height:27px}.c-sidebar-footer .c-social__list .icon{width:27px;height:27px;fill:#444;vertical-align:middle;transition:.35s}.c-sidebar-footer .c-social__list .icon:hover{fill:#3af;transform:scale(1.2);transition:.35s}.c-sidebar-footer .c-copyright p{font-size:13px;margin:0}@media only screen and (max-width: 900px){.c-sidebar{position:relative;width:100%;padding:20px}.c-sidebar .c-contact-menu{margin:20px 0}.c-sidebar-footer .c-social .c-social__title{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}.c-sidebar-footer .c-social__list{margin:0}.c-sidebar-footer .c-copyright{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}}@media only screen and (max-width: 480px){.c-sidebar-author{display:flex;flex-direction:column}.c-sidebar-author .c-author__cover{width:80px;height:80px}.c-sidebar-author .c-author__cover img{width:100%;height:100%}.c-sidebar-author .c-contact-menu{justify-items:center;margin:15px 0px 0px 0px;min-width:245px}.c-sidebar-author .c-contact-menu .c-btn{min-width:80px;font-size:12px;width:120px;height:36px;margin-bottom:10px}.c-sidebar-author .c-contact-menu-bar{justify-items:center;margin:0px 0px 25px 0px}.c-sidebar-author .c-contact-menu-bar .c-btn{min-width:80px;font-size:12px;width:244px;height:36px;padding:4px 15px}.c-sidebar-footer .c-social .c-social__title{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}.c-sidebar-footer .c-social__list{margin:0}.c-sidebar-footer .c-social__list .icon{width:25px;height:25px}.c-sidebar-footer .c-copyright{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}}.c-content{position:relative;display:flex;flex-direction:row;flex-wrap:wrap;align-items:stretch;padding:0 20px 0;margin-left:360px}@media only screen and (max-width: 900px){.c-content{position:static;padding:0 15px 0;margin-left:0}}.c-posts{width:100%;display:flex;flex-direction:row;flex-wrap:wrap}.c-post{width:100%;max-width:100%;margin-bottom:20px;display:flex;flex-direction:row;align-items:stretch;min-height:180px;border-radius:10px;overflow:hidden;transition:.35s;background-color:#fff;box-shadow:0 1px 1px 0 rgba(31,35,46,.15)}.c-post:hover{transform:translate(0px, -2px);box-shadow:0 15px 45px -10px rgba(10,16,34,.2)}.c-post .c-post-thumbnail{display:block;width:30%;max-width:100%;min-height:180px;border-radius:10px 0 0 10px;background-color:rgba(220,235,245,.2);background-size:cover;background-position:50% 50%}.c-post .c-post-content{padding:15px;width:70%}.c-post .c-post-content .c-post-title{font-size:30px;font-weight:400;margin:0 0 15px}.c-post .c-post-content .c-post-title a{text-decoration:none;color:#263959}.c-post .c-post-content .c-post-tags{padding:3px 5px;border-radius:3px;background-color:rgba(135,131,120,.2);color:#eb5757;font-size:85%;font-family:"Courier Prime","Noto Sans KR"}.c-post .c-post-content .c-post-date,.c-post .c-post-content .c-post-words{font-size:12px}.c-load-more{padding:20px;margin:20px auto 40px;font-size:13px;color:#fff;border:none;background-color:#3af;outline:none}.c-load-more:hover{background-color:hsl(205,100%,45%)}@media only screen and (max-width: 1200px){.c-post{width:48%;max-width:100%;margin:0 1% 20px;flex-direction:column}.c-post .c-post-thumbnail{width:100%;border-radius:10px 10px 0 0}.c-post .c-post-content{width:100%}.c-post .c-post-content .c-post-title{font-size:21px;margin:15px 0}}@media only screen and (max-width: 480px){.c-post{width:100%;max-width:100%;margin:0 0 20px}.c-post .c-post-content{width:100%}.c-post .c-post-content .c-post-title{font-size:21px;margin:15px 0}}.c-article{width:100%;margin:20px 0}.c-wrap-content{padding:7%;background-color:#fff}.c-article__image{position:relative;background-color:rgba(220,235,245,.2);background-position:center;background-size:cover;background-repeat:no-repeat}.c-article__image:after{content:"";display:block;padding-top:56%}.c-article__header{margin-bottom:60px;padding-bottom:10px;text-align:center;border-bottom:1px solid #6b6b6b}.c-article__header .c-article__title{margin-bottom:10px}.c-article__date span{font-size:13px;text-transform:uppercase;color:#a0a0a0}.c-article__footer{margin:60px 0 0;padding-top:20px;padding-bottom:10px;text-align:center;border-top:1px solid #6b6b6b}.c-article__footer .c-article__share{transition:.35s}.c-article__footer .c-article__share a .icon{vertical-align:middle;transition:.35s}.c-article__footer .c-article__share a .icon:hover{opacity:.7;transition:.35s}.c-article__footer .c-article__tag{margin-bottom:5px}.c-article__footer .c-article__tag a{display:inline-block;vertical-align:middle;padding:5px 10px;font-family:"Noto Sans KR",serif;font-size:10px;line-height:10px;text-transform:uppercase;background-color:rgba(115,138,160,.6);color:#fff}.c-article__footer .c-article__tag a:hover{background-color:rgba(80.2446808511,99.6723404255,118.2553191489,.6)}.c-article__footer .c-article__tag a:last-child{margin-right:0}.c-recent-post{padding:30px 0}.c-recent-post .c-recent__title{font-size:14px;text-align:center;text-transform:uppercase;margin-bottom:30px}.c-recent-post .c-recent__box{display:flex;flex-direction:row;flex-wrap:wrap}.c-recent-post .c-recent__item{max-width:23%;flex-basis:23%;margin:0 1% 20px;border-radius:10px;overflow:hidden;text-align:center;background-color:#fff;box-shadow:0 1px 1px 0 rgba(31,35,46,.15);transition:.35s}.c-recent-post .c-recent__item h4{margin-bottom:5px;font-size:12px;text-transform:uppercase}.c-recent-post .c-recent__item h4 a{color:#444}.c-recent-post .c-recent__item:hover{box-shadow:2px 4px 8px 0px rgba(46,61,73,.2)}.c-recent-post .c-recent__footer{padding:15px}.c-recent-post .c-recent__image{display:block;width:100%;min-height:180px;background-color:rgba(220,235,245,.2);background-size:cover;background-position:center;background-repeat:no-repeat}.c-recent-post .c-recent__date{color:#a0a0a0;font-size:12px}@media only screen and (max-width: 1200px){.c-recent-post .c-recent__item{max-width:48%;flex-basis:48%}}@media only screen and (max-width: 900px){.c-article{margin:15px 0}}@media only screen and (max-width: 480px){.c-wrap-content{padding:15px}.c-article__header{margin-bottom:5px}.c-article__header .c-article__title{font-size:24px;margin-bottom:5px}.c-recent-post .c-recent__item{max-width:100%;flex-basis:100%;margin:0 0 20px}}.c-blog-tags{width:100%;padding:20px;margin:20px 0 40px;background-color:#fff}.c-blog-tags h1{text-align:center;margin-bottom:0}.c-blog-tags h2{font-size:18px;text-transform:uppercase;margin:30px 0;color:#757575}.c-tag__list{list-style:none;padding:0 0 40px;margin:40px 0 0;border-bottom:1px solid #6b6b6b}.c-tag__list li{display:inline-block;margin-right:15px}.c-tag__list li a{color:#404040;text-transform:uppercase;font-size:12px}.c-tag__list li a:hover{color:hsl(0,0%,-4.9019607843%)}.c-tag__item{margin-bottom:15px}.c-tag__image{width:50px;height:50px;border-radius:50%;margin-right:5px}@media only screen and (max-width: 480px){.c-blog-tags{padding:15px}.c-blog-tags h1{font-size:27px}.c-blog-tags h2{font-size:16px;margin:15px 0}.c-tag__list{padding:0 0 30px;margin:30px 0 0}.c-tag__item{margin-bottom:5px}.c-tag__image{display:none}}.c-header{position:relative;width:100%;margin:20px 0}.c-header__box{position:relative;display:flex;flex-direction:row;justify-content:space-between;align-items:center}.c-header__box .icon--ei-search{position:absolute;top:7px;left:15px;fill:#ccc}.c-search{width:80%}.c-search .c-search__box{display:flex;align-items:center}.c-search .c-search__text{position:relative;width:100%;padding:10px 10px 10px 40px;border:1px solid #f2fafd;border-radius:30px;outline:none;color:#a0a0a0}.c-search .c-search__text::placeholder{color:#ccc}.c-search .c-search__text:hover{box-shadow:0 1px 0px rgba(132,135,138,.1);transition:.35s}.c-search .c-search-results-list{position:absolute;width:100%;margin:10px 0 0;list-style-type:none;background-color:#fff;z-index:1}.c-search .c-search-results-list li{display:flex;flex-wrap:wrap;align-items:center;margin:0;padding:20px 25px 0;background-color:#fff;line-height:1.4;border-left:solid 1px #edeeee;border-right:solid 1px #edeeee}.c-search .c-search-results-list li:first-child{border-top-left-radius:5px;border-top-right-radius:5px;border-top:solid 1px #edeeee}.c-search .c-search-results-list li:last-child{border-bottom-left-radius:5px;border-bottom-right-radius:5px;padding-bottom:25px;border-bottom:solid 1px #edeeee}.c-search .c-search-results-list li a{font-size:16px}.c-nav{flex-grow:1;padding-left:20px}.c-nav .c-nav__list{display:flex;justify-content:flex-end}.c-nav .c-nav__list .c-nav__item{display:flex;align-items:center;float:left;padding:4px 10px;font-size:10px;text-transform:uppercase;white-space:nowrap;border:1px solid #f2fafd;box-shadow:0 1px 0px rgba(132,135,138,.4);will-change:transform;transform:translateY(0px);cursor:pointer}.c-nav .c-nav__list .c-nav__item:hover{color:#222;background-color:#fff}.c-nav .c-nav__list .c-nav__item.is-active{box-shadow:0 0 0 rgba(132,135,138,.5);transform:translateY(1px);color:#cfcfdd}.c-nav .c-nav__list .c-nav__item.is-active:hover{background-color:#fbfbfb}.c-nav .c-nav__list .c-nav__item:first-child{border-radius:10px 0 0 10px}.c-nav .c-nav__list .c-nav__item:last-child{border-radius:0 10px 10px 0}.c-nav .c-nav__list .c-nav__item .icon{width:18px;height:18px;margin-right:3px}@media only screen and (max-width: 900px){.c-header{margin:15px 0}}@media only screen and (max-width: 480px){.c-header .c-header__box{flex-direction:column}.c-header .c-search{width:100%}.c-header .c-search .c-search__text{padding:8px 8px 8px 40px}.c-header .c-nav{margin-top:15px}.c-header .c-nav .c-nav__list{justify-content:center}.c-header .c-nav .c-nav__item{padding:4px 8px}}.c-categories{width:100%}.c-categories__list{width:100%;display:flex;flex-direction:row;flex-wrap:wrap}.c-categories__item{max-width:25%;flex-basis:25%;padding:0 10px 20px}.c-categories__link{height:100%;display:flex;flex-direction:column;align-items:center;padding:20px 10px;border-radius:5px;box-shadow:5px 5px 25px rgba(46,61,73,.15);background-color:#fff;transition:.35s}.c-categories__link:hover{box-shadow:2px 4px 8px 0px rgba(46,61,73,.2)}.c-categories__link:hover .c-categories__img .c-categories__more{opacity:1;transition:.35s}.c-categories__link .c-categories__container{width:100%;word-wrap:break-word}.c-categories__link .c-categories__container.c-empty-figure{display:flex;flex-direction:column;justify-content:center;flex-grow:1}.c-categories__img{position:relative;max-width:100%}.c-categories__img figure{position:relative;width:200px;max-width:100%;margin-bottom:20px;overflow:hidden;background-size:cover;background-repeat:no-repeat;background-position:center;border-radius:50%;box-shadow:inset 0 1px 3px rgba(141,165,185,.3)}.c-categories__img figure:after{content:"";display:block;padding-top:100%}.c-categories__img figure:before{content:"";position:absolute;top:0;bottom:0;left:0;right:0;background-color:rgba(0,0,0,.15)}.c-categories__img .c-categories__more{display:inline-block;position:absolute;top:50%;left:50%;transform:translate(-50%, -50%);font-weight:700;color:#fff;text-transform:uppercase;text-shadow:0 1px 0 rgba(104,172,191,.3);opacity:0;transition:.35s}.c-categories__container{text-align:center}.c-categories__container .c-categories__header{font-size:13px;margin-bottom:10px;font-weight:normal;text-transform:uppercase;color:#404040}.c-categories__container .c-categories__count{font-family:"Noto Sans KR",serif;font-size:12px;color:#404040;margin-bottom:0}.c-categories__container .c-categories__count span{display:inline-block;width:20px;height:20px;line-height:20px;vertical-align:baseline;margin-right:5px;border-radius:50%;color:#fff;background-color:#ee6c6c}@media only screen and (max-width: 1200px){.c-categories .c-categories__item{max-width:33.333%;flex-basis:33.333%}}@media only screen and (max-width: 1050px){.c-categories .c-categories__item{max-width:50%;flex-basis:50%}}@media only screen and (max-width: 480px){.c-categories .c-categories__item{max-width:100%;flex-basis:100%;padding:0 0 20px}}.c-form-box{position:absolute;top:0;width:calc(100% - 40px);min-height:100vh;padding:0 20px;background-color:#fff;z-index:1}.c-form-bnt__close{position:absolute;top:0px;left:0;width:30px;height:30px;cursor:pointer;transition:.35s}.c-form-bnt__close:hover{transform:scale(0.8);opacity:.8}.c-form{position:relative;width:750px;max-width:100%;margin:40px auto}.c-form .c-form__title{margin:0 0 40px;min-width:0;border:0;padding:0;font-family:"Noto Sans KR",serif;text-transform:uppercase;text-align:center}.c-form a{color:#404040}.c-form__group{margin-bottom:20px}.c-form__group label{display:block;text-transform:uppercase;font-size:10px}.c-form__group input,.c-form__group textarea{width:100%;padding:10px 15px;color:#404040;border:1px solid #f2fafd;outline:none;transition:.35s}.c-form__group input:focus,.c-form__group textarea:focus{box-shadow:0 4px 25px rgba(132,135,138,.1)}.c-form__group button{padding:20px;text-transform:uppercase;outline:none;border:none}.c-thank-you p{position:relative;padding:20px 40px;width:750px;max-width:100%;text-transform:uppercase;font-size:12px;line-height:18px;font-weight:700;margin:40px auto 0;text-align:center;color:#fff;background:linear-gradient(135deg, #55b5ad 0%, #5ec9c5 100%)}.c-thank-you p .c-form-bnt__close{width:25px;height:25px;background:rgba(0,0,0,0)}.c-thank-you p a{color:#fff}@media only screen and (max-width: 900px){.c-form-box{width:100%;left:0;right:0}}@media only screen and (max-width: 480px){.c-form-bnt__close{width:25px;height:25px}}.c-newsletter{padding:30px 0 60px;margin:0 auto;border-bottom:1px solid #6b6b6b}.c-newsletter__header{text-align:center}.c-newsletter__header .c-newsletter__title{font-size:14px;text-transform:uppercase;text-align:center}.c-newsletter__header .c-newsletter__subtitle{margin-bottom:15px}.c-newsletter-form{width:100%;max-width:750px;margin:0 auto}.c-newsletter-form .c-newsletter-form__group{display:flex}.c-newsletter__email{width:70%;height:40px;padding:10px 15px;border:1px solid #ddd;border-right-color:rgba(0,0,0,0);outline:none;transition:.35s}.c-newsletter__email:focus{box-shadow:0 4px 25px rgba(132,135,138,.1)}.c-newsletter__button{width:30%;height:40px;color:#fff;background-color:#3af;transition:.35s;border:none;outline:none;cursor:pointer}.c-newsletter__button:hover{background-color:hsl(205,100%,45%)}@media only screen and (max-width: 480px){.c-newsletter__button{font-size:13px}}.c-comments{padding:30px 0;border-top:1px solid #6b6b6b}.c-top{position:fixed;width:40px;height:40px;bottom:20px;color:#05b;background-color:#cef;border-radius:50%;cursor:pointer;transition:.35s;right:-100px;z-index:10;opacity:.5}.c-top--active{right:15px}.c-top:hover{color:#757575;opacity:1}.u-text-left{text-align:left}.u-text-right{text-align:right}.u-text-center{text-align:center}.u-text-justify{text-align:justify}.u-block{display:block}.u-inline-block{display:inline-block}.u-inline{display:inline}.u-full-width{display:block;width:100%}.u-vertical-center{display:flex;align-items:center;justify-content:center}.u-responsive-image{max-width:100%;height:auto;vertical-align:middle}.u-show{display:block !important}.u-hide{display:none !important}.u-invisible{visibility:hidden}.u-float-left{float:left}.u-float-right{float:right}.u-no-padding-top{padding-top:0}.u-no-padding-bottom{padding-bottom:0}.u-no-padding-left{padding-left:0}.u-no-padding-right{padding-right:0}.u-no-padding{padding:0}.u-no-margin-top{margin-top:0}.u-no-margin-bottom{margin-bottom:0}.u-no-margin-left{margin-left:0}.u-no-margin-right{margin-right:0}.u-no-margin{margin:0}.u-lists-reset{list-style-type:none;margin:0;padding:0}.u-clearfix::before,.u-clearfix::after{content:"";display:table;clear:both}.u-screen-reader-text{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}
  </style>
  <!-- KaTeX 관련 파일 -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" />
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "$", right: "$", display: false },
          { left: "\\(", right: "\\)", display: false },
          { left: "\\[", right: "\\]", display: true },
        ],
      });
    });
  </script>
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VNMTFT1R2R"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "G-VNMTFT1R2R");
  </script>
</head>


<body>
  
    <script>
  (function (i, s, o, g, r, a, m) {
  i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
    (i[r].q = i[r].q || []).push(arguments)
}, i[r].l = 1 * new Date(); a = s.createElement(o),
    m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'G-VNMTFT1R2R', 'auto');
  ga('send', 'pageview');

</script> <!-- /google analytics -->
  
  <div class="o-wrapper">
    <aside class="c-sidebar">
  <div class="c-sidebar-author">
    <div class="c-author__cover">
      <a href="/">
        <img src="/images/Profile/profile.png" alt="Dong-Jun Shin">
      </a>
    </div>
    <div class="c-author__info">
      <div class="c-author__name">Dong-Jun Shin</div>
      <span class="c-author__job">Web Developer</span>
    </div>
    <div class="c-contact-menu">
      <div style="width: 100%">
        <a href="/about/profile" class="c-contact-btn c-btn c-btn--secondary c-btn--round c-btn--shadow">
          <div style="
            display: flex;
            flex-direction: row;
            align-items: center;">
            <span data-icon='ei-tag' data-size='s'></span>
            <span>About me</span>
          </div>
        </a>
        
        <a target="_blank" href="https://github.com/Dong-Jun-Shin" class="c-btn c-btn--primary c-btn--round c-btn--shadow">
          <div style="
            display: flex;
            flex-direction: row;
            align-items: center;">
            <span data-icon='ei-sc-github' data-size='s'></span>
            <span>Visit Github</span>
          </div>  
        </a>
      </div>
      
    </div>
    <div class="c-contact-menu-bar">
      <div style="width: 100%">
        <a href="/" class="c-contact-btn c-btn c-btn--bar c-btn--round c-btn--shadow">All Posts</a>
      </div>
    </div>
    <p class="c-author__about">개발자로 성장하며 배운 것을 정리한 블로그</p>
  </div>

  <div class="c-sidebar-footer">
    <div class="c-social">
      <div class="c-social__title">Social</div>
      <ul class="c-social__list u-lists-reset">
        
        <li class="c-social__item"><a href="https://www.linkedin.com/in/kr-jun-shin" target="_blank"><div data-icon='ei-sc-linkedin' data-size='s'></div></a></li>
        
        
        
        
        
        
        
        
        
        
          <li class="c-social__item"><a href="https://youtube.com/channel/UCIHM7drY2vvvFhrhXtpbbzw" target="_blank"><div data-icon='ei-sc-youtube' data-size='s'></div></a></li>
        
        
        
      </ul>
    </div>
    <div class="c-copyright">
      <p>2025 &copy; Dong-Jun Shin</p>
      <!-- <a target="_blank" href="https://analytics.google.com/"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fdong-jun-shin.github.io&count_bg=%23FFD540&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=true"/></a> -->
      <a target="_blank" href="https://analytics.google.com/"><img src="https://img.shields.io/badge/Info-Analytics-infomation?style=flat-square&color=yellow"/></a>
      <a target="_blank" href="https://search.google.com/search-console"><img src="https://img.shields.io/badge/Info-Search-informational?style=flat-square"/></a>
    </div>
  </div>
</aside> <!-- /.c-sidebar -->

<main class="c-content">
  <article class="c-article">
  <div class="c-article__content">
    <header class="c-header u-hide u-no-margin-top">
      <div class="c-header__box">
        <div class="c-search u-full-width">
          <div class="c-search__box">
            <label for="js-search-input" class="u-screen-reader-text">Search for Blog</label>
            <input type="text" id="js-search-input" class="c-search__text" autocomplete="off" placeholder="Type to search...">
            <div data-icon='ei-search' data-size='s'></div>
          </div>
          <ul id="js-results-container" class="c-search-results-list"></ul>
        </div>
      </div>
    </header>
    
    <div class="c-article__image o-opacity" style="background-image: url( /images/CS_AI_ML/logo.png )"></div>
    
    <div class="c-wrap-content">
      <header class="c-article__header">
        <h1 class="c-article__title">코칭스터디 Beyond AI Basic 2023 - 4주차 학습</h1>
        <div class="c-article__date">
          <span>2023, May 28</span>
        </div>
      </header>
      
      <div class="toc-container">
        <div class="toc">
          <div class="toc-title">Index</div>
          <ul><li><a href="#cnn">CNN</a></li><li><a href="#cnn---convolution은-무엇인가">CNN - Convolution은 무엇인가?</a><ul><li><a href="#convolution">Convolution</a></li><li><a href="#rgb-image-convolution">RGB Image Convolution</a></li><li><a href="#stack-of-convolutions">Stack of Convolutions</a></li><li><a href="#convolutional-neural-networks">Convolutional Neural Networks</a></li><li><a href="#convolution-arithmetic">Convolution Arithmetic</a></li><li><a href="#exercise">Exercise</a></li><li><a href="#1x1-convolution">1X1 Convolution</a></li></ul></li><li><a href="#modern-cnn---1x1-convolution의-중요성">Modern CNN - 1x1 convolution의 중요성</a><ul><li><a href="#ilsvrc">ILSVRC</a></li><li><a href="#alexnet">AlexNet</a></li><li><a href="#vggnet">VGGNet</a></li><li><a href="#googlenet">GoogLeNet</a></li><li><a href="#resnet">ResNet</a></li><li><a href="#densenet">DenseNet</a></li><li><a href="#summary">Summary</a></li></ul></li><li><a href="#computer-vision-applications">Computer Vision Applications</a><ul><li><a href="#semantic-segmentation">Semantic Segmentation</a></li><li><a href="#fully-convolutional-network">Fully Convolutional Network</a></li><li><a href="#deconvolution">Deconvolution</a></li></ul></li><li><a href="#detection">Detection</a><ul><li><a href="#r-cnn">R-CNN</a></li><li><a href="#sppnet">SPPNet</a></li><li><a href="#fast-r-cnn">Fast R-CNN</a></li><li><a href="#faster-r-cnn">Faster R-CNN</a></li><li><a href="#yolo">YOLO</a></li></ul></li><li><a href="#rnn---sequential-models">RNN - Sequential Models</a></li><li><a href="#rnn">RNN</a><ul><li><a href="#recurrent-neural-network">Recurrent Neural Network</a></li><li><a href="#long-short-term-memorylstm">Long Short Term Memory(LSTM)</a></li><li><a href="#gated-recurrent-unitgru">Gated Recurrent Unit(GRU)</a></li></ul></li><li><a href="#transformer">Transformer</a><ul><li><a href="#vision-transformer">Vision Transformer</a></li></ul></li><li><a href="#reference">Reference</a></li></ul>
        </div>
      </div>
      
      <h1 id="cnn">CNN</h1>

<h1 id="cnn---convolution은-무엇인가">CNN - Convolution은 무엇인가?</h1>

<h2 id="convolution">Convolution</h2>

<ul>
  <li>이미지를 특정 형태로 변환하는 것</li>
  <li>용어 정의
    <ul>
      <li>I(Image): 이미지</li>
      <li>K(Kernel): 적용할 컨볼루션 필터</li>
      <li>O(Output): 필터가 적용된 이미지</li>
      <li>Dimension = Channel = Depth: 층</li>
    </ul>
  </li>
  <li>원리
    <ul>
      <li>필터를 Image의 시작부터 크기대로 적용해서 Output을 얻음
        <ul>
          <li>3x3 filter가 있고, 7x7 image가 있는 경우
            <ul>
              <li>필터의 크기인 Image(1, 1) ~ Image(3, 3)의 범위에 적용하여 Output(1, 1)을 얻음</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>필터를 적용 시, Depth는 항상 타겟과 동일한 것을 사용</li>
      <li>필터는 Output의 Depth만큼 필요함</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="rgb-image-convolution">RGB Image Convolution</h2>

<ul>
  <li>수학적으로 표현할 때, tensor(32x32)로 표현됨</li>
  <li>R, G, B 표현을 위해 Depth는 3으로 구성</li>
  <li>e.g. I(32x32x3), K_R(5x5x3), K_G(5x5x3), K_B(5x5x3) -&gt; O(28x28x3)</li>
</ul>

<p><br /></p>

<h2 id="stack-of-convolutions">Stack of Convolutions</h2>

<ul>
  <li>여러개의 Convolutions을 적용하는 것</li>
</ul>

<p><br /></p>

<h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2>

<ul>
  <li>구성
    <ul>
      <li>convolution layer
        <ul>
          <li>도장을 찍듯이 한 부분을 선택하여 훑어서 값을 얻는 층</li>
        </ul>
      </li>
      <li>pooling layer
        <ul>
          <li>convolution layer로 얻은 값들을 다 합쳐서 최종적인 값을 출력함</li>
        </ul>
      </li>
      <li>fully connected layer (dense layer)
        <ul>
          <li>분류, 회귀를 통해 원하는 값을 얻는 층</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>각 구성 요소의 목적
    <ul>
      <li>convolution layer, pooling layer는 이미지에서 유용한 정보를 추출하는데 사용</li>
      <li>fully connected layer는 의사 결정을 하기 위한 값을 추출하는데 사용</li>
    </ul>
  </li>
  <li>CNN 성능 향상을 위한 목표
    <ul>
      <li>convolution layer를 최대한 많이 쌓고, 파라미터 숫자와 관련있는 fully connected layer를 최소로 하는데 집중</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="convolution-arithmetic">Convolution Arithmetic</h2>

<ul>
  <li>Output size를 구하는 수식
    <ul>
      <li>W: width, F: Filter, S: Stride</li>
      <li>(W-F)/S+1</li>
    </ul>
  </li>
  <li>Patch size / Stride
    <ul>
      <li>다음 필터를 적용하기까지의 이동 간격</li>
      <li>e.g. 1이면 한 칸씩 이동</li>
    </ul>
  </li>
  <li>Padding
    <ul>
      <li>추가적으로 부여하는 가상의 테두리 크기</li>
      <li>e.g. 1이면 한 칸씩 더 붙임</li>
    </ul>
  </li>
  <li>params
    <ul>
      <li>convolution layer의 param 개수
        <ul>
          <li>K(NxMxD) <em>O_depth(출력 Depth)</em> N(나누어진 네트워크 개수)</li>
        </ul>
      </li>
      <li>fully connected layer의 param 개수
        <ul>
          <li>I(NxMxD) <em>C(channel 개수)</em> O_depth(출력 Depth) * N(나누어진 네트워크 개수)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="exercise">Exercise</h2>

<ul>
  <li>GPU 메모리 크기가 부족할 경우, 네트워크가 여러개로 나누어져 각각의 GPU로 병렬 수행될 수 있음
    <ul>
      <li>N(나누어진 네트워크 수)를 곱해야 전체 파라미터를 계산할 수 있음</li>
    </ul>
  </li>
  <li>Kernel이 모든 위치에 1:1 매핑되기 때문에 fully connected layer가 더 많은 param 개수를 가짐
    <ul>
      <li>Convolution operator는 모두 동일하게 적용되기 때문에 일종의 Shared parameter임</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="1x1-convolution">1X1 Convolution</h2>

<ul>
  <li>이미지에서 1픽셀씩 매치시키고 Channel 방향으로 줄이는 방법</li>
  <li>Dimension reduction
    <ul>
      <li>Convolution(1x1)을 이용해서 이전 Dimension(NxM)을 유지하면서 Channel(Depth)만 줄임</li>
      <li>이를 통해, depth를 증가시키면서 parameter 개수를 줄일 수 있음</li>
    </ul>
  </li>
  <li>e.g. bottleneck architecture</li>
</ul>

<p><br /></p>

<h1 id="modern-cnn---1x1-convolution의-중요성">Modern CNN - 1x1 convolution의 중요성</h1>

<ul>
  <li>layer는 깊게 쌓으면서 param을 줄인 방법</li>
</ul>

<p><br /></p>

<h2 id="ilsvrc">ILSVRC</h2>

<ul>
  <li>ImageNet Large-Scale Visual Recognition Challenge
    <ul>
      <li>Classification(분류) / Detection(물체 인식) / Localization(하나의 물체 인식)/ Segmentation(pixel 단위로 특징을 분류)</li>
      <li>Classification
        <ul>
          <li>AlexNet / VGG / ResNet</li>
          <li>e.g. image가 강아지 사진인지 맞추는 문제</li>
        </ul>
      </li>
      <li>Localization / Detection
        <ul>
          <li>R-CNN / Fast R-CNN / Faster R-CNN</li>
          <li>e.g. image 안에 강아지가 어디 있는지 찾는 문제</li>
        </ul>
      </li>
      <li>Segmentation
        <ul>
          <li>FCN / DeepLab</li>
          <li>e.g. image를 보고 하나가 아닌 전체를 이해하는 문제</li>
        </ul>
      </li>
    </ul>

    <p><img src="/images/CS_AI_ML/2023/05/Beyond_AI_Basic_2023_Week_4/1.png" alt="" />
<em>ILSVRC 예시</em></p>
  </li>
  <li>1000개 정도의 분류</li>
  <li>1백만개 이상의 이미지들</li>
  <li>훈련 데이터로 약 46만장 사용</li>
</ul>

<p><br /></p>

<h2 id="alexnet">AlexNet</h2>

<ul>
  <li>특징
    <ul>
      <li>Rectified Linear Unit(ReLU) activation function을 사용
        <ul>
          <li>0보다 작으면 0, 크면 그대로 사용</li>
          <li>activation function의 특징인 Non-linear 성질을 가지고 있음</li>
          <li>e.g. max(0, x)</li>
        </ul>
      </li>
      <li>2개의 GPU 사용</li>
      <li>Local response normalization(LRN)
        <ul>
          <li>ReLU로 0이상의 값이 주변 픽셀에 영향주는 것을 방지하기 위해 정규화하는 방법</li>
          <li>측면 억제 원리를 사용(lateral inhibition)</li>
          <li>현재는 잘 사용되지 않음</li>
        </ul>
      </li>
      <li>Overlapping pooling
        <ul>
          <li>3x3 영역을 2픽셀 단위로 pooling해서 조금씩 겹치는 부분을 만들어 처리하는 방법</li>
        </ul>
      </li>
      <li>Data Augmentation
        <ul>
          <li>overfitting을 방지하기 위해 학습 데이터를 증가시키는 방법</li>
        </ul>
      </li>
      <li>Dropout
        <ul>
          <li>랜덤하게 일부 데이터를 제외하는 방법</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="vggnet">VGGNet</h2>

<ul>
  <li>특징
    <ul>
      <li>3x3 convolution filters(with stride 1)만 사용
        <ul>
          <li>3x3 2번하는 게, 5x5로 filter 했을 때보다 params의 receptive field를 유지하면서 네트워크를 더 깊게 쌓을 수 있음</li>
        </ul>
      </li>
      <li>1x1 convolution for fully connected layers</li>
      <li>Dropout (p=0.5)</li>
      <li>VGG16, VGG19</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="googlenet">GoogLeNet</h2>

<ul>
  <li>특징
    <ul>
      <li>1x1 convolution filters를 사용해서 Params를 줄임
        <ul>
          <li>inception blocks을 활용한 network-in-network(NIN) 구조</li>
          <li>inception blocks
            <ul>
              <li>Input에 3x3, 5x5 conv를 적용하기 전에 1x1 conv를 적용하는 구성</li>
              <li>1x1를 중간에 사용해서 3x3, 5x5에 적용되는 Depth를 줄여 전체적인 params를 줄임</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="resnet">ResNet</h2>

<ul>
  <li>특징
    <ul>
      <li>데이터를 학습할 때, 기존 학습된 데이터랑 다른 차이만을 학습한 방법</li>
      <li>네트워크가 커짐에 따라, 오히려 학습 에러가 커져서 학습을 더 시킬 수 없는 상황을 개선하기 위해 <code class="language-plaintext highlighter-rouge">identity map</code>이라는 개념을 추가</li>
      <li>identity map (Residual connection, skip connection)
        <ul>
          <li>network의 출력 또는 1x1 conv에 input 결과의 차이만 더함</li>
          <li>종류
            <ul>
              <li>Simple Shortcut</li>
              <li>Projected Shortcut</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Bottleneck architecture
        <ul>
          <li>3x3 2번 conv하는 것에서 params를 줄이기 위해, 1x1을 먼저 적용하여 3x3의 input을 줄이고 input을 늘리기 위해 Depth를 늘린 1x1을 다시 conv함</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="densenet">DenseNet</h2>

<ul>
  <li>ResNet에서 차이를 더하면 값이 섞이니, 섞지 말고 이어주는 방법</li>
  <li>이어줄 경우, channel이 커짐에 따라 param이 기하급수적으로 늘어날 수 있어서 Transition Block으로 param을 줄여줌</li>
  <li>구조
    <ul>
      <li>Dense Block
        <ul>
          <li>각 학습의 차이를 이어주는 층</li>
        </ul>
      </li>
      <li>Transition Block
        <ul>
          <li>1x1 conv를 해서 param을 줄여주는 층</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="summary">Summary</h2>

<ul>
  <li>VGG: 3x3 conv를 사용.</li>
  <li>GoogLeNet: 1x1 conv를 사용. params를 줄일 수 있음</li>
  <li>ResNet: skip-connection 방법 사용. 네트워크를 깊이 쌓을 수 있음</li>
  <li>DenseNet: concatenation으로 차이를 쌓는 방법 사용.</li>
</ul>

<p><br /></p>

<h1 id="computer-vision-applications">Computer Vision Applications</h1>

<h2 id="semantic-segmentation">Semantic Segmentation</h2>

<ul>
  <li>pixel마다 분류하여 labeling하는 것</li>
</ul>

<p><br /></p>

<h2 id="fully-convolutional-network">Fully Convolutional Network</h2>

<ul>
  <li>Convolutionalization(컨볼루션화)
    <ul>
      <li>Fully Connected Layer(dense layer)를 없애기 위해 convolution을 이용한 방법</li>
      <li>Fully Connected Layer(dense layer) 대신 conv filter를 적용해서 conv layer로 변환
        <ul>
          <li>convolution이 가지는 shared parameter의 성질로 인해, input 이미지의 크기에 상관없이 conv filter를 적용 가능</li>
        </ul>
      </li>
      <li>변환된 conv layer는 2차원 정보(이미지의 객체와 위치)를 가지고 있음 (결과가 heat map과 유사)</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="deconvolution">Deconvolution</h2>

<ul>
  <li>conv 연산을 하면 output은 절반으로 줄게 되는데 conv로 줄어든 output을 다시 늘려주는 것</li>
</ul>

<p><br /></p>

<h1 id="detection">Detection</h1>

<h2 id="r-cnn">R-CNN</h2>

<ul>
  <li>CNN에 입력할 때 고정된 크기로 Crop 또는 Warp한 뒤 네트워크에 입력하는 방법</li>
  <li>Crop 또는 Warp하는 과정에서 객체가 가지고 있는 형태적인 정보가 왜곡될 수 있음</li>
  <li>순서
    <ul>
      <li>이미지를 받음</li>
      <li>이미지 안에서 2000개의 region을 랜덤하게 선택</li>
      <li>AlexNet으로 각 region의 특징을 계산</li>
      <li>linear SVMs로 분류</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="sppnet">SPPNet</h2>

<ul>
  <li>Spatial Pyramid Pooling을 사용해서, R-CNN의 과도한 연산량 문제와 정보 왜곡 문제를 해결한 방법
    <ul>
      <li>Spatial Pyramid Pooling은 잘라낸 영역에 대해 미리 정한 영역(1분할, 4분할, …)으로 나누고 각 결과를 고정된 크기의 벡터로 이어서 반환하는 방법</li>
    </ul>
  </li>
  <li>이미지 안에서 CNN을 한 번만 함</li>
</ul>

<p><br /></p>

<h2 id="fast-r-cnn">Fast R-CNN</h2>

<ul>
  <li>binary SVM + bounding box regression을 하나로 합쳐서 multi-pipeline구조의 번거로움을 제거하고 최적화를 위한 방법
    <ul>
      <li>기존 물체 분류를 위해 binary SVM을 학습하고, 객체 위치를 보정하기 위해 bounding box regression을 별도로 학습해야 하는 단점이 있었음</li>
    </ul>
  </li>
  <li>구조 개선으로 네트워크 추론 시간이 짧아짐</li>
  <li>개선점
    <ul>
      <li>binary SVM 대신 FC layer와 softmax를 활용</li>
      <li>CNN으로 얻은 Output(feature vector)를 bounding box regression의 입력으로 사용</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="faster-r-cnn">Faster R-CNN</h2>

<ul>
  <li>하나로 합쳐진 bounding box regression와 Region Proposal(객체 후보)도 하나로 합쳐서 Selective Search 알고리즘의 시간을 없앤 방법
    <ul>
      <li>기존 Region Proposal을 얻기 위해 시간이 오래 걸리는 Selective Search 알고리즘 연산이 필요했음</li>
    </ul>
  </li>
  <li>객체 후보 추정 시간이 짧아짐</li>
  <li>개선점
    <ul>
      <li>Selective Search 알고리즘을 사용한 Region Proposal 대신 객체 후보 영역도 학습하는 네트워크(RPN)을 사용</li>
      <li>Region Proposal Network(RPN)
        <ul>
          <li>입력받은 feature map의 마지막 conv 층을 sliding해서 저차원으로 매핑하고 Regression과 classification을 수행
            <ul>
              <li>이미지 안에 어떤 물체가 있을 지 미리 알고 템플릿을 만들어 두고 템플릿이 바뀌는 정도를 보고 판단해서 Region Proposal과 score를 출력</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="yolo">YOLO</h2>

<ul>
  <li>이미지 자체를 가지고 분류를 하는 방법</li>
  <li>분리, 분류를 한번에 하기 때문에 빠름</li>
  <li>이미지를 SxS으로 나누어 bonding box를 찾고, 실제로 의미있는지를 동시에 예측</li>
  <li>Tensor = SxSx(B*5+C)
    <ul>
      <li>SxS: 그리드에서 나눈 셀 개수</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>B*5: 박스 크기(offsets</td>
              <td>x,y,w,h)와 중요한 정도(confidence)</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>C: classes 개수</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h1 id="rnn---sequential-models">RNN - Sequential Models</h1>

<h1 id="rnn">RNN</h1>

<h2 id="recurrent-neural-network">Recurrent Neural Network</h2>

<ul>
  <li>Markov를 기반으로 만들어져 시계열 데이터 처리에 특화된 방법</li>
  <li>Sequential Data
    <ul>
      <li>몇 차원인지, 몇 개의 입력인지 알지 못하는 연속적인 데이터</li>
    </ul>
  </li>
  <li>Sequential Model
    <ul>
      <li>Sequential Data를 고려해서 다음 데이터가 무엇인지 예측하는 모델</li>
      <li>Naive sequence model
        <ul>
          <li>늘어나는 과거 데이터를 모두 고려하여 다음 데이터를 예측</li>
        </ul>
      </li>
      <li>Autoregressive model
        <ul>
          <li>과거 N개만 정해서 고려하고 다음 데이터를 예측</li>
        </ul>
      </li>
      <li>Markov model(first-order autoregressive model)
        <ul>
          <li>바로 전 과거만 보고 고려하여 다음 데이터를 예측</li>
        </ul>
      </li>
      <li>Latent autoregressive model
        <ul>
          <li>과거 데이터 중간에 이전 데이터까지 요약된 Hidden state를 두고, Hidden state를 이용해서 다음 데이터를 예측</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>RNN을 만들 때는 Sigmoid나 ReLU를 사용해서 Hidden state를 구하면 문제가 있기 때문에 Tanh를 사용
    <ul>
      <li>Sigmoid를 사용하면 값이 줄어드는 문제가 있음(Vanising)</li>
      <li>ReLU를 사용하면 Weight가 양수일 때, 단계를 거듭하며 값이 너무 커져서 학습할 때 네트워크가 폭발하는 문제가 있음(exploding gradient)</li>
    </ul>
  </li>
  <li>Transformer가 나온 이후, 잘 사용되지 않음</li>
</ul>

<p><br /></p>

<h2 id="long-short-term-memorylstm">Long Short Term Memory(LSTM)</h2>

<ul>
  <li>Short-term dependencies / Long-term Dependencies 문제를 해결하기 위해 나온 방법
    <ul>
      <li>요약 과정에서 과거 데이터가 점점 희미해져서 현재에 고려되기 어려움</li>
      <li>Short-term을 해결할 경우 Long-term Dependencies 문제가 생김</li>
    </ul>
  </li>
  <li>Previous cell state, Previous hidden state, X_t를 Sigmoid, Tanh 연산이 포함된 각 Gate에 통과시켜 적절한 출력을 얻음</li>
  <li>용어 정의
    <ul>
      <li>Previous cell state(이전 데이터까지 요약된 정보)</li>
      <li>Previous hidden state(이전의 출력값)</li>
      <li>X_t(현재 값)</li>
    </ul>
  </li>
  <li>Gate
    <ul>
      <li>데이터의 사용 조작 여부를 판단하는 것</li>
    </ul>
  </li>
  <li>Gate 구성
    <ul>
      <li>Forget gate
        <ul>
          <li>Previous cell state 중 어떤 정보를 버릴 지 판단</li>
        </ul>
      </li>
      <li>Input gate
        <ul>
          <li>현재 입력 중 어떤 정보를 사용할 지 판단</li>
          <li>Update cell
            <ul>
              <li>Forget gate, Input gate의 출력 결과를 업데이트해서 사용할 것을 구분해서 cell state를 구성</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Output gate
        <ul>
          <li>업데이트한 cell state로 현재 hidden state를 만듦</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="gated-recurrent-unitgru">Gated Recurrent Unit(GRU)</h2>

<ul>
  <li>cell state 없이, hidden state로만 구현한 방법</li>
  <li>파라미터가 적어 학습 성능이 좋음</li>
  <li>Gate 구성
    <ul>
      <li>Reset gate</li>
      <li>Update gate</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h1 id="transformer">Transformer</h1>

<ul>
  <li>재귀적 구조 없이 attention 구조를 활용해서 번역, 문장 구성, 단어 등 Sequential Data를 다루는 일에 사용하는 방법</li>
  <li>특징
    <ul>
      <li>입력과 출력 Sequence의 수가 다를 수 있음</li>
      <li>1개의 모델로 구성되어 있음</li>
      <li>attention 구조
        <ul>
          <li>입력을 한번에 처리하는 구조</li>
          <li>같은 개수의 Encoder, Decoder로 구성</li>
          <li>Encoder
            <ul>
              <li>Self-Attention
                <ul>
                  <li>각 Embedding 입력을 고려하여 입력 개수만큼의 Vector 출력을 반환
                    <ul>
                      <li>e.g. x1, x2, x3 &gt; z1(x1, x2, x3), z2(x1, x2, x3), z3(x1, x2, x3)</li>
                    </ul>
                  </li>
                  <li>Queries vector, Keys vector, Values vector를 이용해서 x1을 변환
                    <ul>
                      <li>생성 과정에서 각각의 Neural Network로 입력 개수별 3개의 Vector를 만듦
                        <ul>
                          <li>Queries vector와 Keys vector를 내적해서 score를 구함
                            <ul>
                              <li>내적을 위해 2개 vector는 항상 차원이 같아야 함</li>
                            </ul>
                          </li>
                          <li>score가 의미있는 범위에 들어가도록 계산하기 위해, keys dimension에 제곱근을 하고 score를 나눠서 Normalize함</li>
                          <li>그리고 Normalize된 score에 softmax를 연산해서 attention rate를 측정</li>
                          <li>vector 간의 유사도인 attention rate에 value vectors의 weighted sum을 하면 z1을 구할 수 있음
                            <ul>
                              <li>value의 dimension은 queries dimension이나 keys dimension과 같지 않아도 됨</li>
                            </ul>
                          </li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                  <li>문장 관계 파악 예시에서 행렬로 표현할 때
                    <ul>
                      <li>x의 row는 단어의 개수가 될 수 있고, x의 col은 각 단어가 가지는 Embedding dimension을 의미</li>
                      <li>keys vector에서 col은 attention dimension을 의미</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>Feed Forward Neural Network
                <ul>
                  <li>기존 NN과 유사</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Decoder
            <ul>
              <li>attention map을 만들기 위해 Input에 해당하는 Keys vector, Values vector를 전달 받아 최종 출력인 Decoder에 전달함</li>
              <li>Linear + Softmax를 해서 출력</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>많이 사용되는 라이브러리로 <code class="language-plaintext highlighter-rouge">HuggingFace</code>가 있음</li>
</ul>

<p><br /></p>

<h2 id="vision-transformer">Vision Transformer</h2>

<ul>
  <li>Transformer를 활용해서 이미지 분류하는 방법</li>
  <li>Transformer Encoder와 Multi-head attention(MHA)를 거쳐 이미지 분류</li>
  <li>활용 예시 (DALL-E)
    <ul>
      <li>문장을 가지고 이미지를 만듦
        <ul>
          <li>Transformer Encoder만 이용</li>
          <li>이미지를 분할한 Sequential Data와 문장으로 구성된 Sequential Data를 입력으로 사용</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h1 id="reference">Reference</h1>

<ul>
  <li><a href="https://m.post.naver.com/viewer/postView.naver?volumeNo=35664080&amp;memberNo=34635212">제대로 된 인공지능 스터디를 찾고 있다면? AI 심화 스터디</a></li>
  <li><a href="https://www.boostcourse.org/study-ai111-2023">[코칭스터디 10기] Beyond AI Basic 2023 스터디 전용강좌</a></li>
  <li><a href="https://arxiv.org/abs/1704.06857">ILSVRC 예시</a></li>
</ul>


      <div class="c-article__footer u-clearfix">
        <div class="c-article__tag">
          
        </div>
        <div class="c-article__share">
          <a href="https://twitter.com/intent/tweet?text=%EC%BD%94%EC%B9%AD%EC%8A%A4%ED%84%B0%EB%94%94%20Beyond%20AI%20Basic%202023%20-%204%EC%A3%BC%EC%B0%A8%20%ED%95%99%EC%8A%B5&url=https://dong-jun-shin.github.io/2023/05/28/Beyond_AI_Basic_2023_Week_4/" title="Share
          on Twitter" rel="nofollow" target="_blank"><div data-icon='ei-sc-twitter' data-size='s'></div></a>
          <a href="https://facebook.com/sharer.php?u=https://dong-jun-shin.github.io/2023/05/28/Beyond_AI_Basic_2023_Week_4/" title="Share on Facebook" rel="nofollow" target="_blank"><div data-icon='ei-sc-facebook' data-size='s'></div></a>
          <a href="https://plus.google.com/share?url=https://dong-jun-shin.github.io/2023/05/28/Beyond_AI_Basic_2023_Week_4/" title="Share on Google+" rel="nofollow" target="_blank"><div data-icon='ei-sc-google-plus' data-size='s'></div></a>
        </div>
      </div>
      <div class="c-newsletter">
  <div class="c-newsletter__header">
    <h4 class="c-newsletter__title">Newsletter</h4>
    <div class="c-newsletter__subtitle">Subscribe to this blog and receive notifications of new posts by email.</div>
  </div>
  <form class="c-newsletter-form validate" action="#" method="POST" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" target="_blank" novalidate>
    <div class="c-newsletter-form__group">
      <label class="u-screen-reader-text" for="mce-EMAIL">Email address</label>
      <input class="c-newsletter__email required email" id="mce-EMAIL" type="email" name="EMAIL" placeholder="Your email address" autocomplete="on">
      <input class="c-newsletter__button" id="mc-embedded-subscribe" type="submit" name="subscribe" value="Subscribe">
    </div>
  </form>
</div> <!-- /.c-newsletter -->
      <div class="c-recent-post">
        <h4 class="c-recent__title">You might also enjoy</h4>
        <div class="c-recent__box">
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2025/01/10/permutation_and_combination_implements/" style="background-image: url( /images/CS_Algorithm/2025/01/permutation_and_combination_implements/thumbnail.png)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2025/01/10/permutation_and_combination_implements/">순열과 조합 - 2, 경우의 수 뿐만 아니라 경우를 직접 구해보자</a></h4>
              <div class="c-recent__date">
                <time datetime="2025-01-10T17:45:03+09:00">January 10, 2025</time>
              </div>
            </div>
          </div>
        
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2025/01/10/permutation_and_combination_theory/" style="background-image: url( /images/CS_Algorithm/2025/01/permutation_and_combination_theory/thumbnail.png)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2025/01/10/permutation_and_combination_theory/">순열과 조합 - 1, 어떤 원리로 경우의 수를 계산할 수 있는걸까?</a></h4>
              <div class="c-recent__date">
                <time datetime="2025-01-10T17:45:02+09:00">January 10, 2025</time>
              </div>
            </div>
          </div>
        
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2024/08/21/2_years_of_experience_from_2022/" style="background-image: url( /images/Life/2024/08/2_years_of_experience_from_2022/thumbnail_retrospective-journey.jpg)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2024/08/21/2_years_of_experience_from_2022/">퇴사 회고(라 쓰고, 2022년 ~ 2024년 정리)</a></h4>
              <div class="c-recent__date">
                <time datetime="2024-08-21T23:47:38+09:00">August 21, 2024</time>
              </div>
            </div>
          </div>
        
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2024/05/21/Aws_Summit_Seoul_2024/" style="background-image: url( /images/IT_Tech/2024/05/Aws_Summit_Seoul_2024/thumbnail.png)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2024/05/21/Aws_Summit_Seoul_2024/">AWS Summit Seoul 2024 방문기</a></h4>
              <div class="c-recent__date">
                <time datetime="2024-05-21T21:55:48+09:00">May 21, 2024</time>
              </div>
            </div>
          </div>
        
        
        </div>
      </div> <!-- /.c-recent-post -->
      
        <div class="c-comments">
  <div id="disqus_thread" class="article-comments"></div>
  <script>
    (function () {
      var d = document, s = d.createElement('script');
      s.src = '//dong-jun-shin-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
</div> <!-- /.c-comments -->
      
    </div> <!-- /.c-wrap-content -->
  </div> <!-- /.c-article__content -->
</article> <!-- /.c-article-page -->

</main> <!-- /.c-content -->
  </div> <!-- /.o-wrapper -->
  <div class="c-top" data-icon='ei-chevron-up' data-size='s' title="Scroll To Top"></div> <!-- /.c-top -->
  <script src="/js/jquery-3.3.1.min.js"></script>
<script src="/js/evil-icons.min.js"></script>
<script src="/js/jquery.fitvids.js"></script>
<script src="/js/simple-jekyll-search.min.js"></script>
<script src="/js/main.js"></script>
<!-- /javascripts -->
</body>
</html>