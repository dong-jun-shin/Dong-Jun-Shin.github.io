<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>코칭스터디 Beyond AI Basic 2023 - 4주 과정 정리</title>
  <meta
    name="description"
    content="시작하기까지주차별 과정1주차  쇼핑 데이터를 활용한 머신러닝          정형, 비정형 데이터를 다룰 수 있게 되는 것이 목표        정형 데이터: 엑셀과 같이 행, 열로 구성된 데이터  비정형 데이터: 이미지, 비디오, 소리 등  Confusion Matrix      ..."
  />
  <!-- Twitter Cards -->
<meta name="twitter:title" content="코칭스터디 Beyond AI Basic 2023 - 4주 과정 정리">
<meta name="twitter:description" content="시작하기까지

주차별 과정

1주차


  쇼핑 데이터를 활용한 머신러닝
    
      정형, 비정형 데이터를 다룰 수 있게 되는 것이 목표
    
  
  정형 데이터: 엑셀과 같이 행, 열로 구성된 데이터
  비정형 데이터: 이미지, 비디오, 소리 등
  Confusion Matrix
    
      분류 모델을 사용하는 머신러닝으로 얻은 산출물 평가를 위한 표
      TP, TN, FP, FN
    
  
  ROC
    
      TPR(TP 비율), FPR(FP 비율)로 구성된 그래프
      Cutoff Value
    
  
  AUC
    
      평가할 때 사용하는 최소 기준(?)
    
  
  EDA
    
      탐색적 데이터 분석
      데이터를 탐색하고 가설을 세우고 증명 등 가설 검정 과정을 반복하면서 데이터를 이해하는 것
    
  
  데이터 전처리
    
      연속형, 범주형 처리
        
          continuous type
            
              bagging, binning
            
          
          categorical type
            
              encoding (One hot, Label, Frequency, Target, Embedding)
            
          
        
      
      결측치 처리
        
          pattern
            
              Random, Rule
            
          
          univariate
            
              제거
              삽입 (평균값, 중위값, 상수값)
            
          
          multivariate
            
              회귀 분석: 이전 단계를 이용해서 예측하길 반복하는 방법
              KNN nearest
            
          
        
      
      이상치 처리
        
          이상치 정의
          정성적인 측면
          성능적인 측면
        
      
    
  
  머신러닝 기본 개념
    
      Underfitting, Overfitting
        
          모델이 학습을 너무 못하거나, 너무 맞춤으로 되어 성능이 떨어지는 것
          Regularization (모델을 살짝씩 변형하여 데이터셋으로 사용, SMOTE Dataset, Random한 노드 사용)
        
      
      Validation strategy
        
          머신성능의 성능을 향상시키기 위해 검증을 하고 개선을 찾는 과정(?)
        
      
      Reproducibility
        
          Fix seed: 동일한 결과를 받기 위해 seed를 고정하는 것
        
      
      Machine learning workflow
        
          Raw Data 추출, 전처리 &gt; 머신러닝 알고리즘을 활용하여 모델 생성 &gt; 학습, 검증 &gt; 적용, 배포
        
      
      트리 모델
        
          의사결정트리
          Decision Tree &gt; Random Forest &gt; AdaBoost &gt; GBM &gt; XGBoost, LightGBM, CatBoost
        
      
    
  




2주차


  Feature Engineering
    
      원본 데이터로부터 도움이 되는 Feature를 생성, 변환하고 모델에 적합한 형식으로 변환하는 작업
      방법
        
          Pandas Group By Aggregation을 이용한 방법
            
              sum, mean, max, min 등의 방법으로 Feature를 생성하는 방법
            
          
          Pandas Group By 누적합을 이용한 방법
            
              값을 누적해서 얻은 데이터에 대해 sum, mean, max, min 등으로 Feature를 생성하는 방법
            
          
          주문, 상품 데이터를 활용한 방법
            
              데이터의 관계를 이용해서 Feature를 생성하는 방법
            
          
          Time Series 특성을 이용한 방법
            
              연도, 월 등을 이용해서 데이터의 관계를 파악하고 Feature를 생성하는 방법
            
          
        
      
    
  
  Feature Important
    
      Boosting Tree 피처 중요도
        
          GBM(Gradient Boosting Machine)
            
              오차를 학습하면서 진행하는 방법
              Example
            
          
          LightGBM
          XGBoost
          CatBoost
          고려대학교 강필성 교수님의 Business Analytics
        
      
    
  
  Feature Selection
    
      모델에 사용할 피처를 선택하는 과정
      방법
        
          Filter Method
          Wrapper Method
          Embedded Method
        
      
    
  
  하이퍼 파라미터 튜닝
    
      어떻게 학습할 지를 맞춤형으로 조정해서 더 높은 성능이 나오도록 하는 과정
      방법
        
          Grid Layout
            
              하이퍼 파라미터의 경우의 수 중, 모든 경우를 테스트하고 선택
            
          
          Random Layout
            
              하이퍼 파라미터의 경우의 수 중, 무차별로 테스트하고 선택
            
          
          Bayesian Layout
            
              높은 성능을 보인 곳을 중점으로 테스트하고 선택
            
          
        
      
      활용
        
          전달할 파라미터에 하이퍼 파라미터를 직접 수치를 설정
          Optuna 라이브러리로 범위를 지정해서 설정
        
      
    
  
  앙상블 러닝
    
      여러 모델을 조합해서 학습을 하는 과정
      방법
        
          Bagging
            
              샘플을 다양하게 생성
              각자 자기 영역에 대해 학습하고 합치기
            
          
          Voting
            
              투표를 통해 도출
            
          
          Boosting
            
              이전 오차를 보완해서 가중치 부여
              영역을 단계로 나눠서 학습
            
          
          Stacking
            
              여러 모델을 기반으로 meta 모델을 생성(모델들을 이용해서 만든 모델)
            
          
        
      
    
  
  보너스
    
      논문을 읽을 때 중점적으로 읽는 부분
        
          내용이 많은 경우, abstract &gt; results &gt; methods를 우선으로 확인
          중요한 논문은 상세히, 아닌 것은 아이디어의 핵심과 결과만 확인(스터디 효과 극대화를 위함)
          어려운 논문은 해설한 유튜브, 블로그를 함께 참고
          LinkedIn을 통해 최신 트랜드를 파악
        
      
    
  




3주차


  딥러닝
    
      수학적 표현 학습, 계층적 표현 학습(Hierarchical Representation)이라 불림
        
          계층이 깊어질수록 비선형 연산에 의해 표현력이 좋아짐 (기존 데이터를 다른 형태로 매끄럽게 표현)
        
      
      Feature extraction과 Classification을 한번에 처리함
    
  
  딥러닝의 역사 (중요 표시 Bold)
    
      AlexNet(2012)
      DQN(2013)
      Encoder/Decoder(2014)
      Adam Optimizer(2014)
      Generative Adversarial Network(2015)
      Transformer(2017)
      BERT(fine-tuned NLP models)(2018)
      BIG Language Models(2019)
      Self Supervised Learning(2020)
    
  
  딥러닝의 키 컴포넌트
    
      Data
        
          다양한 형태의 학습 대상
          Regularization
            
              Early stopping
              Parameter norm penalty
              Data augmentation
              Noise robustness
              Label smoothing
              Dropout
              Batch normalization
            
          
        
      
      Model
        
          데이터의 형태를 변환하는 처리 패턴
        
      
      Loss Function
        
          예측값과 실제값의 차이로 모델의 학습 정도를 얼마나 안 좋은지 측정하는 함수
          반의어로 ‘Objective Function’이 있음
        
      
      Algorithm
        
          Loss Function의 값을 줄이기 위한 방법
          e.g. 눈가리고 산을 내려가야 하는 상황과 유사
            
              어느쪽으로 내려가야 할 지 알기 위한 여러가지 방법들이 있음
              Gradient Descent, Adagrad, RMSprop, Momentum, Adam 등
              Gradient Descent(경사 하강법)
                
                  미분을 이용해서 Loss Function의 기울기를 구함
                    
                      w1(현재 값), Loss_w1(현재 손실 값), gradient((현재 손실값 / 현재 값) 미분)
                      w1 - [(현재 손실값 / 현재 값) 미분]로 판단할 값을 계산
                        
                          양수(좌하우상)면 왼쪽, 음수(좌상우하)면 오른쪽으로 내려감
                        
                      
                      학습율
                        
                          학습하는 정도를 나타내는 값(내려갈 정도를 나타내는 값)
                          경사도를 이용해서 판단할 때, 조금씩 내려가는 게 아닌 점프를 했다가 다시 올라가는 구간으로 이동하는 문제를 방지하기 위해 조금씩 내려가도록 하는 하이퍼 파라미터
                        
                      
                    
                  
                
              
            
          
        
      
    
  
  Neural Networks
    
      뇌의 복잡한 기능을 단순화(추상화)한 수학적 모델
        
          가중치의 조정으로 원하는 결과를 얻는 수학적 표현임
          if else를 쌓듯이 matrix 연산을 쌓아서 만든 방식
          연속된 가중치의 곱셈으로 이루어짐
            
              이 과정에서 ReLU와 같은 활성화함수(비선형성)이 섞여 있음
            
          
        
      
      e.g. GoogLeNet, ResNet
    
  
  Linear Neural Networks
    
      공간을 늘리고 줄여서 비선형 데이터로 만들고 이를 선형적으로 나눌 수 있게 변형시킴
      affine transform (선형 변환 + 평행 이동)
        
          변형 전, 후에 점, 선, 면 등의 특징이 달라지지 않고 보존된 변형 방법
          가중치(W^T) * 데이터(X) + 바이어스(B)의 연산
          시각적으로 보면 공간을 땡기듯 늘림
          W^T * X로 선형 변환
            
              선형변환은 기울기를 변환시킴 (e.g. 수평 -&gt; 대각선)
            
          
          +B로 평행 이동
        
      
      Non-linear Activation Function (비선형 변환)
        
          데이터에 선형 변환만 해서는 계속 선형이기 떄문에 레이어를 여러개 쌓아도 1개의 층과 똑같은 결과를 만들게 됨. 그래서 비선형 변환이 필요
          시그모이드, 활성화 함수 등을 이용한 연산
          시각적으로 보면 공간을 찌그러뜨려서 줄임
        
      
    
  
  Beyond Linear neural Networks




4주차


  Hierarchical Representation Learning (계층적 표현 학습)
    
      Linear Neural Networks처럼 Deep Neural Networks의 학습 방법
    
  
  딥러닝의 진화
  MLP(Multi Layer Perceptron)
    
      인간처럼 학습을 할 수 있게 하는 방법
    
  
  CNN(Convolutional Neural Networks)
    
      이미지를 보고 특징을 분류할 수 있게 하는 방법
      LeNet 1(MNIST, 숫자 인식)
        
          Multilayer Neural Network의 한계를 극복하기 위해 CNN(Convolutional Neural NEtworks)을 활용한 모델
          Convolutional layer + Pooling layer를 반복하면서 전역적으로 의미있는 특징을 찾아냄
        
      
      LeNet 5
        
          LeNet 1의 구조를 계승하고 입력 이미지의 크기, feature-map의 개수 및 fully-connected layer의 크기에 변화를 준 모델
          RGB 빛의 세기를 통해 이미지를 표현

손으로 쓴 8에 대해 저장된 빛의 세기의 정보를 나타낸 이미지 매트릭스
        
      
      Pooling Layer
        
          영상에 빗댄 특징
            
              2차원 공간적 특징을 가짐
              크기에 따라 같은 영역도 다른 특징을 가짐(확대/축소에 따른 눈, 눈코입, 얼굴 등)
              다운 샘플링이 가능(이미지 크기를 줄임)
            
          
        
      
    
  
  RNN(Recurrent Neural Networks)
    
      책을 보고 공부하는 것처럼 글(Sequential Data)을 읽을 수 있도록 하기 위한 방법
      Sequence Model
      Computational Graph
        
          Input에 이전 Hidden Layer가 추가되어, 이전 Input의 정보를 포함할 수 있는 Neural Networks 구조
        
      
      Many to Many
        
          Computational Graph 구조의 Hidden layer에서 Output을 뽑아, 손실함수를 구하는 구조
        
      
      자동완성 등으로 활용
    
  
  LSTM(Long Short Term Memory)
    
      이전 Input을 모두 기억하기 위해 Long Term(외장 하드)를 적용한 방법
      RNN으로는 이전 내용을 전부 기억할 수 없기 때문에 등장
      동작 원리
        
          forget으로 이전 데이터에서 지울 부분 결정
          input modulation에서 표현할 정보를 반영여부인 input의 값과 곱해서 반영할 데이터에 대한 정보 생성
          RNN의 결과값인 output과 Long Term Memory에 저장되어 있는 값인 context vector를 비선형화하고 곱해서 출력에 반영
        
      
      4 gate
        
          forget
            
              0이면 삭제, 1이면 보존
            
          
          input
            
              0이면 미반영, 1이면 반영
            
          
          input modulation
            
              -1, 1로 나타낼 정보를 표현
            
          
          output
            
              0, 1로 구성된 RNN 결과

LSTM Architecture
            
          
        
      
    
  
  Attention
    
      전체 데이터를 보지 않고, 일부 데이터만 압축해놓고 활용하는 방법
      LSTM으로는 문장의 구성을 하는데 한계가 있기 떄문에 등장
      Attention 모델을 통해 문장 구성이 가능해져 대화할 수 있게 됨
      Sequence-to-Sequence with RNNs 모델을 활용 시, 문장이 길수록 C(Memory cell)의 크기가 매우 커지는 문제가 있었음
      동작원리
        
          이전 값에 대한 정보를 담고 있는 hidden layer들과 s들을 각각 얼마나 연관성이 있는지 계산
          계산된 값을 전부 합한 값이 1이 되는 soft max에 통과시켜 연관성에 대한 확률을 얻음
          각 input들에 매치되는 연관성 확률만큼만 데이터를 반영
          context vector를 생성하고 이전 결과값인 output과 계산해서 S를 생성
          새로 생성된 S와 hidden layer들과의 연관성을 계산
          위의 과정들을 반복하면서 Sequential한 데이터에 대한 정보를 쌓음
        
      
      ‘Attention Architecture 1’ 이미지는 별도 보관
    
  
  Attention Layer
    
      Sequence Data의 Attention Score 얻는 과정을 General하게 행렬 연산으로 표현한 방법
      Attention 구조를 그대로 가져온 상태에서 Hidden을 Key, S(context + output)를 Query라 봄
      동작원리
        
          각 Key, Query들을 곱한 경우의 수를 행렬로 표현
          곱에 대한 행렬을 soft max에 통과시켜 연관성에 대한 확률들을 얻음
          hidden과 행렬의 row별로 곱(product)하고 더해서(sum) column 개수만큼의 결과를 얻음
          결과로 얻은 값들은 다시 각 Query들이 되어, 위의 과정들을 반복하면서 정보를 쌓음
        
      
      ‘Attention Architecture 2’ 이미지는 별도 보관
    
  
  Self-Attention Layer
    
      input을 Query에도 반영해서 나와 나, 나와 다른 것의 연관성을 스스로 고려하는 방법
    
  
  Multi head Self - Attention Layer
    
      각 input을 분리해서 각각 Self-Attention을 수행하는 방법
    
  
  Transformer
    
      Self-Attention을 수행하는 Transformer Block을 쌓아 만든 방법
      동작원리 (Transformer Block)
        
          각 input을 Self-Attention하고 출력값과 input들로 Residual connection 수행
          Layer Normalization 수행
          각 input들에 대해 Multi Layer Perceptron을 수행
          MLP 전의 값과 MLP 후의 값으로 Residual connection 수행
          Layer Normalization 수행
          결과값을 도출
        
      
    
  
  ChatGPT
    
      Transformer 모델로 학습된 LLM(Large Language Model) 모델
    
  


Reference


  제대로 된 인공지능 스터디를 찾고 있다면? AI 심화 스터디
  [코칭스터디 10기] Beyond AI Basic 2023 스터디 전용강좌
  손으로 쓴 8에 대해 저장된 이미지 매트릭스
  LSTM Architecture
  13. Attention


">



<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://dong-jun-shin.github.io/images/CS_AI_ML/logo.png">


<!-- Open Graph -->
<meta property="og:locale" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="코칭스터디 Beyond AI Basic 2023 - 4주 과정 정리">

<meta property="og:image" content="https://dong-jun-shin.github.io/images/CS_AI_ML/logo.png">


<meta property="og:description" content="시작하기까지

주차별 과정

1주차


  쇼핑 데이터를 활용한 머신러닝
    
      정형, 비정형 데이터를 다룰 수 있게 되는 것이 목표
    
  
  정형 데이터: 엑셀과 같이 행, 열로 구성된 데이터
  비정형 데이터: 이미지, 비디오, 소리 등
  Confusion Matrix
    
      분류 모델을 사용하는 머신러닝으로 얻은 산출물 평가를 위한 표
      TP, TN, FP, FN
    
  
  ROC
    
      TPR(TP 비율), FPR(FP 비율)로 구성된 그래프
      Cutoff Value
    
  
  AUC
    
      평가할 때 사용하는 최소 기준(?)
    
  
  EDA
    
      탐색적 데이터 분석
      데이터를 탐색하고 가설을 세우고 증명 등 가설 검정 과정을 반복하면서 데이터를 이해하는 것
    
  
  데이터 전처리
    
      연속형, 범주형 처리
        
          continuous type
            
              bagging, binning
            
          
          categorical type
            
              encoding (One hot, Label, Frequency, Target, Embedding)
            
          
        
      
      결측치 처리
        
          pattern
            
              Random, Rule
            
          
          univariate
            
              제거
              삽입 (평균값, 중위값, 상수값)
            
          
          multivariate
            
              회귀 분석: 이전 단계를 이용해서 예측하길 반복하는 방법
              KNN nearest
            
          
        
      
      이상치 처리
        
          이상치 정의
          정성적인 측면
          성능적인 측면
        
      
    
  
  머신러닝 기본 개념
    
      Underfitting, Overfitting
        
          모델이 학습을 너무 못하거나, 너무 맞춤으로 되어 성능이 떨어지는 것
          Regularization (모델을 살짝씩 변형하여 데이터셋으로 사용, SMOTE Dataset, Random한 노드 사용)
        
      
      Validation strategy
        
          머신성능의 성능을 향상시키기 위해 검증을 하고 개선을 찾는 과정(?)
        
      
      Reproducibility
        
          Fix seed: 동일한 결과를 받기 위해 seed를 고정하는 것
        
      
      Machine learning workflow
        
          Raw Data 추출, 전처리 &gt; 머신러닝 알고리즘을 활용하여 모델 생성 &gt; 학습, 검증 &gt; 적용, 배포
        
      
      트리 모델
        
          의사결정트리
          Decision Tree &gt; Random Forest &gt; AdaBoost &gt; GBM &gt; XGBoost, LightGBM, CatBoost
        
      
    
  




2주차


  Feature Engineering
    
      원본 데이터로부터 도움이 되는 Feature를 생성, 변환하고 모델에 적합한 형식으로 변환하는 작업
      방법
        
          Pandas Group By Aggregation을 이용한 방법
            
              sum, mean, max, min 등의 방법으로 Feature를 생성하는 방법
            
          
          Pandas Group By 누적합을 이용한 방법
            
              값을 누적해서 얻은 데이터에 대해 sum, mean, max, min 등으로 Feature를 생성하는 방법
            
          
          주문, 상품 데이터를 활용한 방법
            
              데이터의 관계를 이용해서 Feature를 생성하는 방법
            
          
          Time Series 특성을 이용한 방법
            
              연도, 월 등을 이용해서 데이터의 관계를 파악하고 Feature를 생성하는 방법
            
          
        
      
    
  
  Feature Important
    
      Boosting Tree 피처 중요도
        
          GBM(Gradient Boosting Machine)
            
              오차를 학습하면서 진행하는 방법
              Example
            
          
          LightGBM
          XGBoost
          CatBoost
          고려대학교 강필성 교수님의 Business Analytics
        
      
    
  
  Feature Selection
    
      모델에 사용할 피처를 선택하는 과정
      방법
        
          Filter Method
          Wrapper Method
          Embedded Method
        
      
    
  
  하이퍼 파라미터 튜닝
    
      어떻게 학습할 지를 맞춤형으로 조정해서 더 높은 성능이 나오도록 하는 과정
      방법
        
          Grid Layout
            
              하이퍼 파라미터의 경우의 수 중, 모든 경우를 테스트하고 선택
            
          
          Random Layout
            
              하이퍼 파라미터의 경우의 수 중, 무차별로 테스트하고 선택
            
          
          Bayesian Layout
            
              높은 성능을 보인 곳을 중점으로 테스트하고 선택
            
          
        
      
      활용
        
          전달할 파라미터에 하이퍼 파라미터를 직접 수치를 설정
          Optuna 라이브러리로 범위를 지정해서 설정
        
      
    
  
  앙상블 러닝
    
      여러 모델을 조합해서 학습을 하는 과정
      방법
        
          Bagging
            
              샘플을 다양하게 생성
              각자 자기 영역에 대해 학습하고 합치기
            
          
          Voting
            
              투표를 통해 도출
            
          
          Boosting
            
              이전 오차를 보완해서 가중치 부여
              영역을 단계로 나눠서 학습
            
          
          Stacking
            
              여러 모델을 기반으로 meta 모델을 생성(모델들을 이용해서 만든 모델)
            
          
        
      
    
  
  보너스
    
      논문을 읽을 때 중점적으로 읽는 부분
        
          내용이 많은 경우, abstract &gt; results &gt; methods를 우선으로 확인
          중요한 논문은 상세히, 아닌 것은 아이디어의 핵심과 결과만 확인(스터디 효과 극대화를 위함)
          어려운 논문은 해설한 유튜브, 블로그를 함께 참고
          LinkedIn을 통해 최신 트랜드를 파악
        
      
    
  




3주차


  딥러닝
    
      수학적 표현 학습, 계층적 표현 학습(Hierarchical Representation)이라 불림
        
          계층이 깊어질수록 비선형 연산에 의해 표현력이 좋아짐 (기존 데이터를 다른 형태로 매끄럽게 표현)
        
      
      Feature extraction과 Classification을 한번에 처리함
    
  
  딥러닝의 역사 (중요 표시 Bold)
    
      AlexNet(2012)
      DQN(2013)
      Encoder/Decoder(2014)
      Adam Optimizer(2014)
      Generative Adversarial Network(2015)
      Transformer(2017)
      BERT(fine-tuned NLP models)(2018)
      BIG Language Models(2019)
      Self Supervised Learning(2020)
    
  
  딥러닝의 키 컴포넌트
    
      Data
        
          다양한 형태의 학습 대상
          Regularization
            
              Early stopping
              Parameter norm penalty
              Data augmentation
              Noise robustness
              Label smoothing
              Dropout
              Batch normalization
            
          
        
      
      Model
        
          데이터의 형태를 변환하는 처리 패턴
        
      
      Loss Function
        
          예측값과 실제값의 차이로 모델의 학습 정도를 얼마나 안 좋은지 측정하는 함수
          반의어로 ‘Objective Function’이 있음
        
      
      Algorithm
        
          Loss Function의 값을 줄이기 위한 방법
          e.g. 눈가리고 산을 내려가야 하는 상황과 유사
            
              어느쪽으로 내려가야 할 지 알기 위한 여러가지 방법들이 있음
              Gradient Descent, Adagrad, RMSprop, Momentum, Adam 등
              Gradient Descent(경사 하강법)
                
                  미분을 이용해서 Loss Function의 기울기를 구함
                    
                      w1(현재 값), Loss_w1(현재 손실 값), gradient((현재 손실값 / 현재 값) 미분)
                      w1 - [(현재 손실값 / 현재 값) 미분]로 판단할 값을 계산
                        
                          양수(좌하우상)면 왼쪽, 음수(좌상우하)면 오른쪽으로 내려감
                        
                      
                      학습율
                        
                          학습하는 정도를 나타내는 값(내려갈 정도를 나타내는 값)
                          경사도를 이용해서 판단할 때, 조금씩 내려가는 게 아닌 점프를 했다가 다시 올라가는 구간으로 이동하는 문제를 방지하기 위해 조금씩 내려가도록 하는 하이퍼 파라미터
                        
                      
                    
                  
                
              
            
          
        
      
    
  
  Neural Networks
    
      뇌의 복잡한 기능을 단순화(추상화)한 수학적 모델
        
          가중치의 조정으로 원하는 결과를 얻는 수학적 표현임
          if else를 쌓듯이 matrix 연산을 쌓아서 만든 방식
          연속된 가중치의 곱셈으로 이루어짐
            
              이 과정에서 ReLU와 같은 활성화함수(비선형성)이 섞여 있음
            
          
        
      
      e.g. GoogLeNet, ResNet
    
  
  Linear Neural Networks
    
      공간을 늘리고 줄여서 비선형 데이터로 만들고 이를 선형적으로 나눌 수 있게 변형시킴
      affine transform (선형 변환 + 평행 이동)
        
          변형 전, 후에 점, 선, 면 등의 특징이 달라지지 않고 보존된 변형 방법
          가중치(W^T) * 데이터(X) + 바이어스(B)의 연산
          시각적으로 보면 공간을 땡기듯 늘림
          W^T * X로 선형 변환
            
              선형변환은 기울기를 변환시킴 (e.g. 수평 -&gt; 대각선)
            
          
          +B로 평행 이동
        
      
      Non-linear Activation Function (비선형 변환)
        
          데이터에 선형 변환만 해서는 계속 선형이기 떄문에 레이어를 여러개 쌓아도 1개의 층과 똑같은 결과를 만들게 됨. 그래서 비선형 변환이 필요
          시그모이드, 활성화 함수 등을 이용한 연산
          시각적으로 보면 공간을 찌그러뜨려서 줄임
        
      
    
  
  Beyond Linear neural Networks




4주차


  Hierarchical Representation Learning (계층적 표현 학습)
    
      Linear Neural Networks처럼 Deep Neural Networks의 학습 방법
    
  
  딥러닝의 진화
  MLP(Multi Layer Perceptron)
    
      인간처럼 학습을 할 수 있게 하는 방법
    
  
  CNN(Convolutional Neural Networks)
    
      이미지를 보고 특징을 분류할 수 있게 하는 방법
      LeNet 1(MNIST, 숫자 인식)
        
          Multilayer Neural Network의 한계를 극복하기 위해 CNN(Convolutional Neural NEtworks)을 활용한 모델
          Convolutional layer + Pooling layer를 반복하면서 전역적으로 의미있는 특징을 찾아냄
        
      
      LeNet 5
        
          LeNet 1의 구조를 계승하고 입력 이미지의 크기, feature-map의 개수 및 fully-connected layer의 크기에 변화를 준 모델
          RGB 빛의 세기를 통해 이미지를 표현

손으로 쓴 8에 대해 저장된 빛의 세기의 정보를 나타낸 이미지 매트릭스
        
      
      Pooling Layer
        
          영상에 빗댄 특징
            
              2차원 공간적 특징을 가짐
              크기에 따라 같은 영역도 다른 특징을 가짐(확대/축소에 따른 눈, 눈코입, 얼굴 등)
              다운 샘플링이 가능(이미지 크기를 줄임)
            
          
        
      
    
  
  RNN(Recurrent Neural Networks)
    
      책을 보고 공부하는 것처럼 글(Sequential Data)을 읽을 수 있도록 하기 위한 방법
      Sequence Model
      Computational Graph
        
          Input에 이전 Hidden Layer가 추가되어, 이전 Input의 정보를 포함할 수 있는 Neural Networks 구조
        
      
      Many to Many
        
          Computational Graph 구조의 Hidden layer에서 Output을 뽑아, 손실함수를 구하는 구조
        
      
      자동완성 등으로 활용
    
  
  LSTM(Long Short Term Memory)
    
      이전 Input을 모두 기억하기 위해 Long Term(외장 하드)를 적용한 방법
      RNN으로는 이전 내용을 전부 기억할 수 없기 때문에 등장
      동작 원리
        
          forget으로 이전 데이터에서 지울 부분 결정
          input modulation에서 표현할 정보를 반영여부인 input의 값과 곱해서 반영할 데이터에 대한 정보 생성
          RNN의 결과값인 output과 Long Term Memory에 저장되어 있는 값인 context vector를 비선형화하고 곱해서 출력에 반영
        
      
      4 gate
        
          forget
            
              0이면 삭제, 1이면 보존
            
          
          input
            
              0이면 미반영, 1이면 반영
            
          
          input modulation
            
              -1, 1로 나타낼 정보를 표현
            
          
          output
            
              0, 1로 구성된 RNN 결과

LSTM Architecture
            
          
        
      
    
  
  Attention
    
      전체 데이터를 보지 않고, 일부 데이터만 압축해놓고 활용하는 방법
      LSTM으로는 문장의 구성을 하는데 한계가 있기 떄문에 등장
      Attention 모델을 통해 문장 구성이 가능해져 대화할 수 있게 됨
      Sequence-to-Sequence with RNNs 모델을 활용 시, 문장이 길수록 C(Memory cell)의 크기가 매우 커지는 문제가 있었음
      동작원리
        
          이전 값에 대한 정보를 담고 있는 hidden layer들과 s들을 각각 얼마나 연관성이 있는지 계산
          계산된 값을 전부 합한 값이 1이 되는 soft max에 통과시켜 연관성에 대한 확률을 얻음
          각 input들에 매치되는 연관성 확률만큼만 데이터를 반영
          context vector를 생성하고 이전 결과값인 output과 계산해서 S를 생성
          새로 생성된 S와 hidden layer들과의 연관성을 계산
          위의 과정들을 반복하면서 Sequential한 데이터에 대한 정보를 쌓음
        
      
      ‘Attention Architecture 1’ 이미지는 별도 보관
    
  
  Attention Layer
    
      Sequence Data의 Attention Score 얻는 과정을 General하게 행렬 연산으로 표현한 방법
      Attention 구조를 그대로 가져온 상태에서 Hidden을 Key, S(context + output)를 Query라 봄
      동작원리
        
          각 Key, Query들을 곱한 경우의 수를 행렬로 표현
          곱에 대한 행렬을 soft max에 통과시켜 연관성에 대한 확률들을 얻음
          hidden과 행렬의 row별로 곱(product)하고 더해서(sum) column 개수만큼의 결과를 얻음
          결과로 얻은 값들은 다시 각 Query들이 되어, 위의 과정들을 반복하면서 정보를 쌓음
        
      
      ‘Attention Architecture 2’ 이미지는 별도 보관
    
  
  Self-Attention Layer
    
      input을 Query에도 반영해서 나와 나, 나와 다른 것의 연관성을 스스로 고려하는 방법
    
  
  Multi head Self - Attention Layer
    
      각 input을 분리해서 각각 Self-Attention을 수행하는 방법
    
  
  Transformer
    
      Self-Attention을 수행하는 Transformer Block을 쌓아 만든 방법
      동작원리 (Transformer Block)
        
          각 input을 Self-Attention하고 출력값과 input들로 Residual connection 수행
          Layer Normalization 수행
          각 input들에 대해 Multi Layer Perceptron을 수행
          MLP 전의 값과 MLP 후의 값으로 Residual connection 수행
          Layer Normalization 수행
          결과값을 도출
        
      
    
  
  ChatGPT
    
      Transformer 모델로 학습된 LLM(Large Language Model) 모델
    
  


Reference


  제대로 된 인공지능 스터디를 찾고 있다면? AI 심화 스터디
  [코칭스터디 10기] Beyond AI Basic 2023 스터디 전용강좌
  손으로 쓴 8에 대해 저장된 이미지 매트릭스
  LSTM Architecture
  13. Attention


">
<meta property="og:url" content="https://dong-jun-shin.github.io/2023/06/06/Beyond_AI_Basic_2023/">
<meta property="og:site_name" content="Jun's Dev_Blog">
  <link rel="canonical" href="https://dong-jun-shin.github.io/2023/06/06/Beyond_AI_Basic_2023/" />
  <link
    rel="alternate"
    type="application/rss+xml"
    title="Jun's Dev_Blog"
    href="https://dong-jun-shin.github.io/feed.xml"
  />
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css?family=Volkhov:400,700" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@200;300;400;500;700;900&display=swap" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Courier+Prime:wght@700&display=swap" rel="stylesheet" />
  <!-- Common -->
  <style>
    
    /*! normalize.css v7.0.0 | MIT License | github.com/necolas/normalize.css */html,body{scroll-behavior:smooth}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{font-weight:normal;letter-spacing:0;margin:0}article,aside,footer,header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}figcaption,figure,main{display:block}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:rgba(0,0,0,0);-webkit-text-decoration-skip:objects}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}dfn{font-style:italic}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-0.25em}sup{top:-0.5em}audio,video{display:inline-block}audio:not([controls]){display:none;height:0}img{border-style:none}svg:not(:root){overflow:hidden}button,input,optgroup,select,textarea{font-family:"Noto Sans KR",sans-serif;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}button,html [type=button],[type=reset],[type=submit]{-webkit-appearance:button}button::-moz-focus-inner,[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring,[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{display:inline-block;vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details,menu{display:block}summary{display:list-item}canvas{display:inline-block}template{display:none}[hidden]{display:none}body,h1,h2,h3,h4,h5,h6,p,blockquote,pre,dl,dd,ol,ul,fieldset,legend,figure,hr{margin:0;padding:0}li>ul,li>ol{margin-bottom:0}table{border:.14em solid #000;border-collapse:collapse;border-spacing:0;word-break:initial;width:100%}table tr:nth-child(even){background-color:#f2fafd}thead{background-color:#a0d0ee}table th{text-align:center;padding:6px 13px;border:.1em solid #757575}table td{padding:6px 13px;border:.1em solid #757575}table tr{padding:6px 13px;border:.1em solid #757575}@-webkit-keyframes spin{100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes spin{100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.icon{position:relative;display:inline-block;width:25px;height:25px;overflow:hidden;fill:currentColor}.icon__cnt{width:100%;height:100%;background:inherit;fill:inherit;pointer-events:none;transform:translateX(0);-ms-transform:translate(0.5px, -0.3px)}.icon--m{width:50px;height:50px}.icon--l{width:100px;height:100px}.icon--xl{width:150px;height:150px}.icon--xxl{width:200px;height:200px}.icon__spinner{position:absolute;top:0;left:0;width:100%;height:100%}.icon--ei-spinner .icon__spinner,.icon--ei-spinner-2 .icon__spinner{-webkit-animation:spin 1s steps(12) infinite;animation:spin 1s steps(12) infinite}.icon--ei-spinner-3 .icon__spinner{-webkit-animation:spin 1.5s linear infinite;animation:spin 1.5s linear infinite}.icon--ei-sc-facebook{fill:#3b5998}.icon--ei-sc-github{fill:#333}.icon--ei-sc-google-plus{fill:#dd4b39}.icon--ei-sc-instagram{fill:#3f729b}.icon--ei-sc-linkedin{fill:#0976b4}.icon--ei-sc-odnoklassniki{fill:#ed812b}.icon--ei-sc-skype{fill:#00aff0}.icon--ei-sc-soundcloud{fill:#f80}.icon--ei-sc-tumblr{fill:#35465c}.icon--ei-sc-twitter{fill:#55acee}.icon--ei-sc-vimeo{fill:#1ab7ea}.icon--ei-sc-vk{fill:#45668e}.icon--ei-sc-youtube{fill:#e52d27}.icon--ei-sc-pinterest{fill:#bd081c}.icon--ei-sc-telegram{fill:#08c}*,*::after,*::before{box-sizing:border-box}h1,h2,h3,h4,h5,h6,ul,ol,dl,blockquote,p,address,hr,table,fieldset,figure,pre{margin-bottom:20px}ul,ol,dd{margin-left:20px}.highlight{background:#f7f7f7}.highlighter-rouge .highlight{background:#0d1117;color:#e6edf3;border-radius:10px}.highlight .c{color:#7ca668;font-style:italic}.highlight .ch{color:#7ca668;font-style:italic}.highlight .cm{color:#7ca668;font-style:italic}.highlight .cp{color:#7ca668;font-style:italic;font-weight:normal}.highlight .cpf{color:#7ca668;font-style:italic}.highlight .c1{color:#7ca668;font-style:italic}.highlight .cs{color:#7ca668;font-style:italic}.highlight .err{color:#f85149}.highlight .esc{color:#e6edf3}.highlight .g{color:#e6edf3}.highlight .k{color:#c586c0}.highlight .l{color:#a5d6ff}.highlight .n{color:#e6edf3}.highlight .o{color:#d4d4d4}.highlight .x{color:#e6edf3}.highlight .p{color:#d4d4d4}.highlight .gd{color:#ffa198;background-color:#490202}.highlight .ge{color:#e6edf3;font-style:italic}.highlight .ges{color:#e6edf3;font-weight:bold;font-style:italic}.highlight .gr{color:#ffa198}.highlight .gh{color:#79c0ff;font-weight:bold}.highlight .gi{color:#56d364;background-color:#0f5323}.highlight .go{color:#8b949e}.highlight .gp{color:#8b949e}.highlight .gs{color:#e6edf3;font-weight:bold}.highlight .gu{color:#79c0ff}.highlight .gt{color:#ff7b72}.highlight .g-Underline{color:#e6edf3;text-decoration:underline}.highlight .kc{color:#79c0ff}.highlight .kd{color:#569cd6}.highlight .kn{color:#ff7b72}.highlight .kp{color:#79c0ff}.highlight .kr{color:#4ec9b0}.highlight .kt{color:#4ec9b0}.highlight .ld{color:#ce9178}.highlight .sd{color:#a5d6ff}.highlight .na{color:#e6edf3}.highlight .nb{color:#4ec9b0}.highlight .nc{color:#4ec9b0;font-weight:bold}.highlight .no{color:#79c0ff;font-weight:bold}.highlight .nd{color:#d2a8ff;font-weight:bold}.highlight .ni{color:#ffa657}.highlight .ne{color:#f0883e;font-weight:bold}.highlight .nf{color:#dcdcaa;font-weight:bold}.highlight .nl{color:#4ec9b0;font-weight:bold}.highlight .nn{color:#ff7b72}.highlight .nx{color:#9cdcfe}.highlight .py{color:#79c0ff}.highlight .nt{color:#4ec9b0}.highlight .nv{color:#79c0ff}.highlight .ow{color:#ff7b72;font-weight:bold}.highlight .pm{color:#e6edf3}.highlight .w{color:#6e7681}.highlight .mb{color:#a5d6ff}.highlight .mf{color:#b5cea8}.highlight .mh{color:#a5d6ff}.highlight .mi{color:#b5cea8}.highlight .mo{color:#a5d6ff}.highlight .sa{color:#79c0ff}.highlight .sb{color:#a5d6ff}.highlight .sc{color:#a5d6ff}.highlight .dl{color:#ce9178}.highlight .sd{color:#a5d6ff}.highlight .s{color:#ce9178}.highlight .s1{color:#ce9178}.highlight .s2{color:#ce9178}.highlight .se{color:#79c0ff}.highlight .sh{color:#79c0ff}.highlight .si{color:#a5d6ff}.highlight .sx{color:#a5d6ff}.highlight .sr{color:#79c0ff}.highlight .ss{color:#a5d6ff}.highlight .bp{color:#e6edf3}.highlight .fm{color:#d2a8ff;font-weight:bold}.highlight .vc{color:#79c0ff}.highlight .vg{color:#79c0ff}.highlight .vi{color:#79c0ff}.highlight .vm{color:#79c0ff}.highlight .il{color:#a5d6ff}body{font-family:"Open Sans",Helvetica Neue,Helvetica,"Noto Sans KR",Arial,sans-serif;font-size:16px;line-height:28px;color:#404040;background-color:#fbfbfb;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}*::selection{color:#fff;background-color:#3af}.toc-container{margin-bottom:20px;display:flex;justify-content:center}.toc{background-color:#fff;border:1px solid #edeeee;border-radius:8px;padding:20px;box-shadow:0 2px 4px rgba(0,0,0,.1);width:100%;font-family:"Noto Sans KR",sans-serif;text-align:left}.toc-title{font-family:"Noto Sans KR",sans-serif;font-size:32px;font-weight:700;color:#05b;margin-bottom:20px;text-align:left;background-color:rgba(204,238,255,.5);padding:15px;border-radius:5px;border:1px solid #ccc}.toc ul{list-style-type:disc;padding-left:15px}.toc a{font-family:"Noto Sans KR",sans-serif;font-size:16px;color:hsl(205,100%,45%);text-decoration:none;display:block;transition:color .3s ease,background-color .3s ease}.toc a:hover{background-color:#eee;color:rgb(0,89.25,153);text-decoration:underline}.toc a:active{color:#05b}.toc .list-group-item.active{background-color:rgba(34,34,34,.8);color:#fff;font-weight:bold;border-radius:5px}.toc .list-group-item.disabled{color:#6c757d;background-color:rgba(0,0,0,0)}.toc .list-group-item+.list-group-item{margin-top:8px}h1,h2,h3,h4,h5,h6{font-family:"Noto Sans KR",serif;font-weight:700;line-height:initial}h1{font-weight:700;font-size:36px;line-height:110%;margin-top:1.6em;margin-bottom:.8em}h2{font-weight:700;font-size:32px;margin-top:1.6em;margin-bottom:.8em}h3{font-weight:700;font-size:28px;margin-top:1.8em;margin-bottom:.9em}h4{font-weight:700;font-size:24px;margin-top:2em;margin-bottom:1em}h5{font-weight:700;font-size:22px;margin-top:2em;margin-bottom:1em;color:#333}h6{font-weight:700;font-size:20px;margin-top:2em;margin-bottom:1em;color:#444}img{max-width:100%;height:auto;vertical-align:middle}img+strong:before{display:inline-block;content:"▲";padding-right:5px;font-size:14px}img+strong{display:block;font-size:14px}p:has(>img):has(>strong){max-width:90%;margin:50px auto;text-align:center}a{text-decoration:none;color:hsl(205,100%,45%);transition:.35s}a:hover{color:rgb(0,89.25,153)}blockquote{padding-left:20px;border-left:4px solid #3af;font-family:"Noto Sans KR",serif;font-style:normal;font-size:14px;background-color:rgb(237.58,248.3,252.32)}blockquote p{padding:10px}hr{height:4px;margin:20px 0;border:0;background-color:#6b6b6b}pre{overflow:auto;padding:14px;font-size:14px;white-space:pre-wrap;word-wrap:break-word;word-break:break-all;font-family:Courier,"Noto Sans KR",monospace}pre.highlight{padding:15px 20px}code{border-radius:10px;overflow:auto;white-space:pre-wrap;word-wrap:break-word;word-break:break-all;font-family:Menlo,Monaco,"Courier New",monospace;font-weight:bold;font-size:14px;vertical-align:middle}p code,li code{color:#eb5757;background-color:#edeeee;margin:0 2px;padding:5px 6px}pre code{font-size:14px;color:#ddd;background-color:rgba(0,0,0,0)}.language-plaintext code{color:#ddd}.o-wrapper{max-width:1440px;position:relative}.o-opacity{animation-duration:.7s;animation-delay:.2s;animation-fill-mode:both;animation-name:opacity}@keyframes opacity{from{opacity:0}to{opacity:1}}.c-btn{display:inline-block;white-space:nowrap;vertical-align:middle;font-family:"Noto Sans KR",serif;font-size:14px;text-align:center;padding:5px 15px;cursor:pointer;transition:.35s}.c-btn--primary{color:#fff;background-color:#3af;background:linear-gradient(135deg, #33aaff 0%, #62d5ff 100%)}.c-btn--secondary{color:#fff;background-color:#cfcfdd;background:linear-gradient(135deg, #a2a2bd 0%, #cfcfdd 100%)}.c-btn--bar{color:#fff;background-color:#444;background:#525252;font-size:14px;width:76%;height:40px}.c-btn--round{border-radius:30px}.c-btn--shadow{box-shadow:8px 10px 20px 0 rgba(46,61,73,.15)}.c-btn--shadow:hover{box-shadow:2px 4px 8px 0px rgba(46,61,73,.2)}.c-btn--middle{display:block;width:300px;max-width:100%}.c-btn--big{display:block;width:100%}.c-btn:hover{color:#fff;transition:.35s}.c-btn:active{transform:translateY(2px)}.c-sidebar{display:flex;flex-direction:column;justify-content:space-between;position:fixed;top:0;left:0;bottom:0;width:360px;padding:40px 20px 20px;text-align:center;box-shadow:1px 1px 0 rgba(31,35,46,.15);background-color:#fff}.c-sidebar-author{display:flex;flex-direction:column}.c-sidebar-author .c-author__cover{width:100px;height:100px;margin:0 auto 10px;border-radius:50%;overflow:hidden;background-color:#cfcfdd}.c-sidebar-author .c-author__cover img{width:100%;height:100%;border-radius:50%;transition:.35s}.c-sidebar-author .c-author__cover img:hover{transform:scale3d(0.9, 0.9, 1)}.c-sidebar-author .c-contact-menu .c-btn{min-width:110px}.c-sidebar-author .c-contact-menu .c-btn .icon{vertical-align:text-bottom;fill:#fff}.c-sidebar-author .c-author__info{font-family:"Noto Sans KR",serif}.c-sidebar-author .c-author__name{font-size:18px;font-weight:700;line-height:21px}.c-sidebar-author .c-author__job{font-size:12px;color:#a0a0a0;margin:5px 0 0}.c-sidebar-author .c-contact-menu{justify-items:center;margin:30px 0px 10px 0px}.c-sidebar-author .c-contact-menu .c-btn{width:130px}.c-sidebar-author .c-contact-menu-bar{justify-items:center;margin:0px 0px 25px 0px}.c-sidebar-author .c-contact-menu-bar .c-btn{font-size:14px;width:260px}.c-sidebar-author .c-author__about{max-width:400px;margin:0 auto 15px;font-size:13px}.c-sidebar-footer .c-social__title{position:relative;font-family:"Noto Sans KR",serif;font-size:16px;font-weight:700;color:#444}.c-sidebar-footer .c-social__title::before{content:"";display:block;height:2px;width:calc(50% - 40px);transform:translateY(-50%);position:absolute;top:50%;left:0;background-color:#444}.c-sidebar-footer .c-social__title::after{content:"";display:block;height:2px;width:calc(50% - 40px);transform:translateY(-50%);position:absolute;top:50%;right:0;background-color:#444}.c-sidebar-footer .c-social__list{list-style-type:none;padding:0;margin:15px 0}.c-sidebar-footer .c-social__list .c-social__item{display:inline-block;width:27px;height:27px}.c-sidebar-footer .c-social__list .icon{width:27px;height:27px;fill:#444;vertical-align:middle;transition:.35s}.c-sidebar-footer .c-social__list .icon:hover{fill:#3af;transform:scale(1.2);transition:.35s}.c-sidebar-footer .c-copyright p{font-size:13px;margin:0}@media only screen and (max-width: 900px){.c-sidebar{position:relative;width:100%;padding:20px}.c-sidebar .c-contact-menu{margin:20px 0}.c-sidebar-footer .c-social .c-social__title{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}.c-sidebar-footer .c-social__list{margin:0}.c-sidebar-footer .c-copyright{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}}@media only screen and (max-width: 480px){.c-sidebar-author{display:flex;flex-direction:column}.c-sidebar-author .c-author__cover{width:80px;height:80px}.c-sidebar-author .c-author__cover img{width:100%;height:100%}.c-sidebar-author .c-contact-menu{justify-items:center;margin:15px 0px 0px 0px;min-width:245px}.c-sidebar-author .c-contact-menu .c-btn{min-width:80px;font-size:12px;width:120px;height:36px;margin-bottom:10px}.c-sidebar-author .c-contact-menu-bar{justify-items:center;margin:0px 0px 25px 0px}.c-sidebar-author .c-contact-menu-bar .c-btn{min-width:80px;font-size:12px;width:244px;height:36px;padding:4px 15px}.c-sidebar-footer .c-social .c-social__title{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}.c-sidebar-footer .c-social__list{margin:0}.c-sidebar-footer .c-social__list .icon{width:25px;height:25px}.c-sidebar-footer .c-copyright{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}}.c-content{position:relative;display:flex;flex-direction:row;flex-wrap:wrap;align-items:stretch;padding:0 20px 0;margin-left:360px}@media only screen and (max-width: 900px){.c-content{position:static;padding:0 15px 0;margin-left:0}}.c-posts{width:100%;display:flex;flex-direction:row;flex-wrap:wrap}.c-post{width:100%;max-width:100%;margin-bottom:20px;display:flex;flex-direction:row;align-items:stretch;min-height:180px;border-radius:10px;overflow:hidden;transition:.35s;background-color:#fff;box-shadow:0 1px 1px 0 rgba(31,35,46,.15)}.c-post:hover{transform:translate(0px, -2px);box-shadow:0 15px 45px -10px rgba(10,16,34,.2)}.c-post .c-post-thumbnail{display:block;width:30%;max-width:100%;min-height:180px;border-radius:10px 0 0 10px;background-color:rgba(220,235,245,.2);background-size:cover;background-position:50% 50%}.c-post .c-post-content{padding:15px;width:70%}.c-post .c-post-content .c-post-title{font-size:30px;font-weight:400;margin:0 0 15px}.c-post .c-post-content .c-post-title a{text-decoration:none;color:#263959}.c-post .c-post-content .c-post-tags{padding:3px 5px;border-radius:3px;background-color:rgba(135,131,120,.2);color:#eb5757;font-size:85%;font-family:"Courier Prime","Noto Sans KR"}.c-post .c-post-content .c-post-date,.c-post .c-post-content .c-post-words{font-size:12px}.c-load-more{padding:20px;margin:20px auto 40px;font-size:13px;color:#fff;border:none;background-color:#3af;outline:none}.c-load-more:hover{background-color:hsl(205,100%,45%)}@media only screen and (max-width: 1200px){.c-post{width:48%;max-width:100%;margin:0 1% 20px;flex-direction:column}.c-post .c-post-thumbnail{width:100%;border-radius:10px 10px 0 0}.c-post .c-post-content{width:100%}.c-post .c-post-content .c-post-title{font-size:21px;margin:15px 0}}@media only screen and (max-width: 480px){.c-post{width:100%;max-width:100%;margin:0 0 20px}.c-post .c-post-content{width:100%}.c-post .c-post-content .c-post-title{font-size:21px;margin:15px 0}}.c-article{width:100%;margin:20px 0}.c-wrap-content{padding:7%;background-color:#fff}.c-article__image{position:relative;background-color:rgba(220,235,245,.2);background-position:center;background-size:cover;background-repeat:no-repeat}.c-article__image:after{content:"";display:block;padding-top:56%}.c-article__header{margin-bottom:60px;padding-bottom:10px;text-align:center;border-bottom:1px solid #6b6b6b}.c-article__header .c-article__title{margin-bottom:10px}.c-article__date span{font-size:13px;text-transform:uppercase;color:#a0a0a0}.c-article__footer{margin:60px 0 0;padding-top:20px;padding-bottom:10px;text-align:center;border-top:1px solid #6b6b6b}.c-article__footer .c-article__share{transition:.35s}.c-article__footer .c-article__share a .icon{vertical-align:middle;transition:.35s}.c-article__footer .c-article__share a .icon:hover{opacity:.7;transition:.35s}.c-article__footer .c-article__tag{margin-bottom:5px}.c-article__footer .c-article__tag a{display:inline-block;vertical-align:middle;padding:5px 10px;font-family:"Noto Sans KR",serif;font-size:10px;line-height:10px;text-transform:uppercase;background-color:rgba(115,138,160,.6);color:#fff}.c-article__footer .c-article__tag a:hover{background-color:rgba(80.2446808511,99.6723404255,118.2553191489,.6)}.c-article__footer .c-article__tag a:last-child{margin-right:0}.c-recent-post{padding:30px 0}.c-recent-post .c-recent__title{font-size:14px;text-align:center;text-transform:uppercase;margin-bottom:30px}.c-recent-post .c-recent__box{display:flex;flex-direction:row;flex-wrap:wrap}.c-recent-post .c-recent__item{max-width:23%;flex-basis:23%;margin:0 1% 20px;border-radius:10px;overflow:hidden;text-align:center;background-color:#fff;box-shadow:0 1px 1px 0 rgba(31,35,46,.15);transition:.35s}.c-recent-post .c-recent__item h4{margin-bottom:5px;font-size:12px;text-transform:uppercase}.c-recent-post .c-recent__item h4 a{color:#444}.c-recent-post .c-recent__item:hover{box-shadow:2px 4px 8px 0px rgba(46,61,73,.2)}.c-recent-post .c-recent__footer{padding:15px}.c-recent-post .c-recent__image{display:block;width:100%;min-height:180px;background-color:rgba(220,235,245,.2);background-size:cover;background-position:center;background-repeat:no-repeat}.c-recent-post .c-recent__date{color:#a0a0a0;font-size:12px}@media only screen and (max-width: 1200px){.c-recent-post .c-recent__item{max-width:48%;flex-basis:48%}}@media only screen and (max-width: 900px){.c-article{margin:15px 0}}@media only screen and (max-width: 480px){.c-wrap-content{padding:15px}.c-article__header{margin-bottom:5px}.c-article__header .c-article__title{font-size:24px;margin-bottom:5px}.c-recent-post .c-recent__item{max-width:100%;flex-basis:100%;margin:0 0 20px}}.c-blog-tags{width:100%;padding:20px;margin:20px 0 40px;background-color:#fff}.c-blog-tags h1{text-align:center;margin-bottom:0}.c-blog-tags h2{font-size:18px;text-transform:uppercase;margin:30px 0;color:#757575}.c-tag__list{list-style:none;padding:0 0 40px;margin:40px 0 0;border-bottom:1px solid #6b6b6b}.c-tag__list li{display:inline-block;margin-right:15px}.c-tag__list li a{color:#404040;text-transform:uppercase;font-size:12px}.c-tag__list li a:hover{color:hsl(0,0%,-4.9019607843%)}.c-tag__item{margin-bottom:15px}.c-tag__image{width:50px;height:50px;border-radius:50%;margin-right:5px}@media only screen and (max-width: 480px){.c-blog-tags{padding:15px}.c-blog-tags h1{font-size:27px}.c-blog-tags h2{font-size:16px;margin:15px 0}.c-tag__list{padding:0 0 30px;margin:30px 0 0}.c-tag__item{margin-bottom:5px}.c-tag__image{display:none}}.c-header{position:relative;width:100%;margin:20px 0}.c-header__box{position:relative;display:flex;flex-direction:row;justify-content:space-between;align-items:center}.c-header__box .icon--ei-search{position:absolute;top:7px;left:15px;fill:#ccc}.c-search{width:80%}.c-search .c-search__box{display:flex;align-items:center}.c-search .c-search__text{position:relative;width:100%;padding:10px 10px 10px 40px;border:1px solid #f2fafd;border-radius:30px;outline:none;color:#a0a0a0}.c-search .c-search__text::placeholder{color:#ccc}.c-search .c-search__text:hover{box-shadow:0 1px 0px rgba(132,135,138,.1);transition:.35s}.c-search .c-search-results-list{position:absolute;width:100%;margin:10px 0 0;list-style-type:none;background-color:#fff;z-index:1}.c-search .c-search-results-list li{display:flex;flex-wrap:wrap;align-items:center;margin:0;padding:20px 25px 0;background-color:#fff;line-height:1.4;border-left:solid 1px #edeeee;border-right:solid 1px #edeeee}.c-search .c-search-results-list li:first-child{border-top-left-radius:5px;border-top-right-radius:5px;border-top:solid 1px #edeeee}.c-search .c-search-results-list li:last-child{border-bottom-left-radius:5px;border-bottom-right-radius:5px;padding-bottom:25px;border-bottom:solid 1px #edeeee}.c-search .c-search-results-list li a{font-size:16px}.c-nav{flex-grow:1;padding-left:20px}.c-nav .c-nav__list{display:flex;justify-content:flex-end}.c-nav .c-nav__list .c-nav__item{display:flex;align-items:center;float:left;padding:4px 10px;font-size:10px;text-transform:uppercase;white-space:nowrap;border:1px solid #f2fafd;box-shadow:0 1px 0px rgba(132,135,138,.4);will-change:transform;transform:translateY(0px);cursor:pointer}.c-nav .c-nav__list .c-nav__item:hover{color:#222;background-color:#fff}.c-nav .c-nav__list .c-nav__item.is-active{box-shadow:0 0 0 rgba(132,135,138,.5);transform:translateY(1px);color:#cfcfdd}.c-nav .c-nav__list .c-nav__item.is-active:hover{background-color:#fbfbfb}.c-nav .c-nav__list .c-nav__item:first-child{border-radius:10px 0 0 10px}.c-nav .c-nav__list .c-nav__item:last-child{border-radius:0 10px 10px 0}.c-nav .c-nav__list .c-nav__item .icon{width:18px;height:18px;margin-right:3px}@media only screen and (max-width: 900px){.c-header{margin:15px 0}}@media only screen and (max-width: 480px){.c-header .c-header__box{flex-direction:column}.c-header .c-search{width:100%}.c-header .c-search .c-search__text{padding:8px 8px 8px 40px}.c-header .c-nav{margin-top:15px}.c-header .c-nav .c-nav__list{justify-content:center}.c-header .c-nav .c-nav__item{padding:4px 8px}}.c-categories{width:100%}.c-categories__list{width:100%;display:flex;flex-direction:row;flex-wrap:wrap}.c-categories__item{max-width:25%;flex-basis:25%;padding:0 10px 20px}.c-categories__link{height:100%;display:flex;flex-direction:column;align-items:center;padding:20px 10px;border-radius:5px;box-shadow:5px 5px 25px rgba(46,61,73,.15);background-color:#fff;transition:.35s}.c-categories__link:hover{box-shadow:2px 4px 8px 0px rgba(46,61,73,.2)}.c-categories__link:hover .c-categories__img .c-categories__more{opacity:1;transition:.35s}.c-categories__link .c-categories__container{width:100%;word-wrap:break-word}.c-categories__link .c-categories__container.c-empty-figure{display:flex;flex-direction:column;justify-content:center;flex-grow:1}.c-categories__img{position:relative;max-width:100%}.c-categories__img figure{position:relative;width:200px;max-width:100%;margin-bottom:20px;overflow:hidden;background-size:cover;background-repeat:no-repeat;background-position:center;border-radius:50%;box-shadow:inset 0 1px 3px rgba(141,165,185,.3)}.c-categories__img figure:after{content:"";display:block;padding-top:100%}.c-categories__img figure:before{content:"";position:absolute;top:0;bottom:0;left:0;right:0;background-color:rgba(0,0,0,.15)}.c-categories__img .c-categories__more{display:inline-block;position:absolute;top:50%;left:50%;transform:translate(-50%, -50%);font-weight:700;color:#fff;text-transform:uppercase;text-shadow:0 1px 0 rgba(104,172,191,.3);opacity:0;transition:.35s}.c-categories__container{text-align:center}.c-categories__container .c-categories__header{font-size:13px;margin-bottom:10px;font-weight:normal;text-transform:uppercase;color:#404040}.c-categories__container .c-categories__count{font-family:"Noto Sans KR",serif;font-size:12px;color:#404040;margin-bottom:0}.c-categories__container .c-categories__count span{display:inline-block;width:20px;height:20px;line-height:20px;vertical-align:baseline;margin-right:5px;border-radius:50%;color:#fff;background-color:#ee6c6c}@media only screen and (max-width: 1200px){.c-categories .c-categories__item{max-width:33.333%;flex-basis:33.333%}}@media only screen and (max-width: 1050px){.c-categories .c-categories__item{max-width:50%;flex-basis:50%}}@media only screen and (max-width: 480px){.c-categories .c-categories__item{max-width:100%;flex-basis:100%;padding:0 0 20px}}.c-form-box{position:absolute;top:0;width:calc(100% - 40px);min-height:100vh;padding:0 20px;background-color:#fff;z-index:1}.c-form-bnt__close{position:absolute;top:0px;left:0;width:30px;height:30px;cursor:pointer;transition:.35s}.c-form-bnt__close:hover{transform:scale(0.8);opacity:.8}.c-form{position:relative;width:750px;max-width:100%;margin:40px auto}.c-form .c-form__title{margin:0 0 40px;min-width:0;border:0;padding:0;font-family:"Noto Sans KR",serif;text-transform:uppercase;text-align:center}.c-form a{color:#404040}.c-form__group{margin-bottom:20px}.c-form__group label{display:block;text-transform:uppercase;font-size:10px}.c-form__group input,.c-form__group textarea{width:100%;padding:10px 15px;color:#404040;border:1px solid #f2fafd;outline:none;transition:.35s}.c-form__group input:focus,.c-form__group textarea:focus{box-shadow:0 4px 25px rgba(132,135,138,.1)}.c-form__group button{padding:20px;text-transform:uppercase;outline:none;border:none}.c-thank-you p{position:relative;padding:20px 40px;width:750px;max-width:100%;text-transform:uppercase;font-size:12px;line-height:18px;font-weight:700;margin:40px auto 0;text-align:center;color:#fff;background:linear-gradient(135deg, #55b5ad 0%, #5ec9c5 100%)}.c-thank-you p .c-form-bnt__close{width:25px;height:25px;background:rgba(0,0,0,0)}.c-thank-you p a{color:#fff}@media only screen and (max-width: 900px){.c-form-box{width:100%;left:0;right:0}}@media only screen and (max-width: 480px){.c-form-bnt__close{width:25px;height:25px}}.c-newsletter{padding:30px 0 60px;margin:0 auto;border-bottom:1px solid #6b6b6b}.c-newsletter__header{text-align:center}.c-newsletter__header .c-newsletter__title{font-size:14px;text-transform:uppercase;text-align:center}.c-newsletter__header .c-newsletter__subtitle{margin-bottom:15px}.c-newsletter-form{width:100%;max-width:750px;margin:0 auto}.c-newsletter-form .c-newsletter-form__group{display:flex}.c-newsletter__email{width:70%;height:40px;padding:10px 15px;border:1px solid #ddd;border-right-color:rgba(0,0,0,0);outline:none;transition:.35s}.c-newsletter__email:focus{box-shadow:0 4px 25px rgba(132,135,138,.1)}.c-newsletter__button{width:30%;height:40px;color:#fff;background-color:#3af;transition:.35s;border:none;outline:none;cursor:pointer}.c-newsletter__button:hover{background-color:hsl(205,100%,45%)}@media only screen and (max-width: 480px){.c-newsletter__button{font-size:13px}}.c-comments{padding:30px 0;border-top:1px solid #6b6b6b}.c-top{position:fixed;width:40px;height:40px;bottom:20px;color:#05b;background-color:#cef;border-radius:50%;cursor:pointer;transition:.35s;right:-100px;z-index:10;opacity:.5}.c-top--active{right:15px}.c-top:hover{color:#757575;opacity:1}.u-text-left{text-align:left}.u-text-right{text-align:right}.u-text-center{text-align:center}.u-text-justify{text-align:justify}.u-block{display:block}.u-inline-block{display:inline-block}.u-inline{display:inline}.u-full-width{display:block;width:100%}.u-vertical-center{display:flex;align-items:center;justify-content:center}.u-responsive-image{max-width:100%;height:auto;vertical-align:middle}.u-show{display:block !important}.u-hide{display:none !important}.u-invisible{visibility:hidden}.u-float-left{float:left}.u-float-right{float:right}.u-no-padding-top{padding-top:0}.u-no-padding-bottom{padding-bottom:0}.u-no-padding-left{padding-left:0}.u-no-padding-right{padding-right:0}.u-no-padding{padding:0}.u-no-margin-top{margin-top:0}.u-no-margin-bottom{margin-bottom:0}.u-no-margin-left{margin-left:0}.u-no-margin-right{margin-right:0}.u-no-margin{margin:0}.u-lists-reset{list-style-type:none;margin:0;padding:0}.u-clearfix::before,.u-clearfix::after{content:"";display:table;clear:both}.u-screen-reader-text{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}
  </style>
  <!-- KaTeX 관련 파일 -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" />
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "$", right: "$", display: false },
          { left: "\\(", right: "\\)", display: false },
          { left: "\\[", right: "\\]", display: true },
        ],
      });
    });
  </script>
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VNMTFT1R2R"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "G-VNMTFT1R2R");
  </script>
</head>


<body>
  
    <script>
  (function (i, s, o, g, r, a, m) {
  i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
    (i[r].q = i[r].q || []).push(arguments)
}, i[r].l = 1 * new Date(); a = s.createElement(o),
    m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'G-VNMTFT1R2R', 'auto');
  ga('send', 'pageview');

</script> <!-- /google analytics -->
  
  <div class="o-wrapper">
    <aside class="c-sidebar">
  <div class="c-sidebar-author">
    <div class="c-author__cover">
      <a href="/">
        <img src="/images/Profile/profile.png" alt="Dong-Jun Shin">
      </a>
    </div>
    <div class="c-author__info">
      <div class="c-author__name">Dong-Jun Shin</div>
      <span class="c-author__job">Web Developer</span>
    </div>
    <div class="c-contact-menu">
      <div style="width: 100%">
        <a href="/about/profile" class="c-contact-btn c-btn c-btn--secondary c-btn--round c-btn--shadow">
          <div style="
            display: flex;
            flex-direction: row;
            align-items: center;">
            <span data-icon='ei-tag' data-size='s'></span>
            <span>About me</span>
          </div>
        </a>
        
        <a target="_blank" href="https://github.com/Dong-Jun-Shin" class="c-btn c-btn--primary c-btn--round c-btn--shadow">
          <div style="
            display: flex;
            flex-direction: row;
            align-items: center;">
            <span data-icon='ei-sc-github' data-size='s'></span>
            <span>Visit Github</span>
          </div>  
        </a>
      </div>
      
    </div>
    <div class="c-contact-menu-bar">
      <div style="width: 100%">
        <a href="/" class="c-contact-btn c-btn c-btn--bar c-btn--round c-btn--shadow">All Posts</a>
      </div>
    </div>
    <p class="c-author__about">개발자로 성장하며 배운 것을 정리한 블로그</p>
  </div>

  <div class="c-sidebar-footer">
    <div class="c-social">
      <div class="c-social__title">Social</div>
      <ul class="c-social__list u-lists-reset">
        
        <li class="c-social__item"><a href="https://www.linkedin.com/in/kr-jun-shin" target="_blank"><div data-icon='ei-sc-linkedin' data-size='s'></div></a></li>
        
        
        
        
        
        
        
        
        
        
          <li class="c-social__item"><a href="https://youtube.com/channel/UCIHM7drY2vvvFhrhXtpbbzw" target="_blank"><div data-icon='ei-sc-youtube' data-size='s'></div></a></li>
        
        
        
      </ul>
    </div>
    <div class="c-copyright">
      <p>2025 &copy; Dong-Jun Shin</p>
      <!-- <a target="_blank" href="https://analytics.google.com/"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fdong-jun-shin.github.io&count_bg=%23FFD540&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=true"/></a> -->
      <a target="_blank" href="https://analytics.google.com/"><img src="https://img.shields.io/badge/Info-Analytics-infomation?style=flat-square&color=yellow"/></a>
      <a target="_blank" href="https://search.google.com/search-console"><img src="https://img.shields.io/badge/Info-Search-informational?style=flat-square"/></a>
    </div>
  </div>
</aside> <!-- /.c-sidebar -->

<main class="c-content">
  <article class="c-article">
  <div class="c-article__content">
    <header class="c-header u-hide u-no-margin-top">
      <div class="c-header__box">
        <div class="c-search u-full-width">
          <div class="c-search__box">
            <label for="js-search-input" class="u-screen-reader-text">Search for Blog</label>
            <input type="text" id="js-search-input" class="c-search__text" autocomplete="off" placeholder="Type to search...">
            <div data-icon='ei-search' data-size='s'></div>
          </div>
          <ul id="js-results-container" class="c-search-results-list"></ul>
        </div>
      </div>
    </header>
    
    <div class="c-article__image o-opacity" style="background-image: url( /images/CS_AI_ML/logo.png )"></div>
    
    <div class="c-wrap-content">
      <header class="c-article__header">
        <h1 class="c-article__title">코칭스터디 Beyond AI Basic 2023 - 4주 과정 정리</h1>
        <div class="c-article__date">
          <span>2023, Jun 06</span>
        </div>
      </header>
      
      <div class="toc-container">
        <div class="toc">
          <div class="toc-title">Index</div>
          <ul><li><a href="#시작하기까지">시작하기까지</a></li><li><a href="#주차별-과정">주차별 과정</a></li><li><a href="#1주차">1주차</a></li><li><a href="#2주차">2주차</a></li><li><a href="#3주차">3주차</a></li><li><a href="#4주차">4주차</a></li><li><a href="#reference">Reference</a></li></ul>
        </div>
      </div>
      
      <h1 id="시작하기까지">시작하기까지</h1>

<h1 id="주차별-과정">주차별 과정</h1>

<h1 id="1주차">1주차</h1>

<ul>
  <li>쇼핑 데이터를 활용한 머신러닝
    <ul>
      <li>정형, 비정형 데이터를 다룰 수 있게 되는 것이 목표</li>
    </ul>
  </li>
  <li>정형 데이터: 엑셀과 같이 행, 열로 구성된 데이터</li>
  <li>비정형 데이터: 이미지, 비디오, 소리 등</li>
  <li>Confusion Matrix
    <ul>
      <li>분류 모델을 사용하는 머신러닝으로 얻은 산출물 평가를 위한 표</li>
      <li>TP, TN, FP, FN</li>
    </ul>
  </li>
  <li>ROC
    <ul>
      <li>TPR(TP 비율), FPR(FP 비율)로 구성된 그래프</li>
      <li>Cutoff Value</li>
    </ul>
  </li>
  <li>AUC
    <ul>
      <li>평가할 때 사용하는 최소 기준(?)</li>
    </ul>
  </li>
  <li>EDA
    <ul>
      <li>탐색적 데이터 분석</li>
      <li>데이터를 탐색하고 가설을 세우고 증명 등 가설 검정 과정을 반복하면서 데이터를 이해하는 것</li>
    </ul>
  </li>
  <li>데이터 전처리
    <ul>
      <li>연속형, 범주형 처리
        <ul>
          <li>continuous type
            <ul>
              <li>bagging, binning</li>
            </ul>
          </li>
          <li>categorical type
            <ul>
              <li>encoding (One hot, Label, Frequency, Target, Embedding)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>결측치 처리
        <ul>
          <li>pattern
            <ul>
              <li>Random, Rule</li>
            </ul>
          </li>
          <li>univariate
            <ul>
              <li>제거</li>
              <li>삽입 (평균값, 중위값, 상수값)</li>
            </ul>
          </li>
          <li>multivariate
            <ul>
              <li>회귀 분석: 이전 단계를 이용해서 예측하길 반복하는 방법</li>
              <li>KNN nearest</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>이상치 처리
        <ul>
          <li>이상치 정의</li>
          <li>정성적인 측면</li>
          <li>성능적인 측면</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>머신러닝 기본 개념
    <ul>
      <li>Underfitting, Overfitting
        <ul>
          <li>모델이 학습을 너무 못하거나, 너무 맞춤으로 되어 성능이 떨어지는 것</li>
          <li>Regularization (모델을 살짝씩 변형하여 데이터셋으로 사용, SMOTE Dataset, Random한 노드 사용)</li>
        </ul>
      </li>
      <li>Validation strategy
        <ul>
          <li>머신성능의 성능을 향상시키기 위해 검증을 하고 개선을 찾는 과정(?)</li>
        </ul>
      </li>
      <li>Reproducibility
        <ul>
          <li>Fix seed: 동일한 결과를 받기 위해 seed를 고정하는 것</li>
        </ul>
      </li>
      <li>Machine learning workflow
        <ul>
          <li>Raw Data 추출, 전처리 &gt; 머신러닝 알고리즘을 활용하여 모델 생성 &gt; 학습, 검증 &gt; 적용, 배포</li>
        </ul>
      </li>
      <li>트리 모델
        <ul>
          <li>의사결정트리</li>
          <li>Decision Tree &gt; Random Forest &gt; AdaBoost &gt; GBM &gt; XGBoost, LightGBM, CatBoost</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h1 id="2주차">2주차</h1>

<ul>
  <li>Feature Engineering
    <ul>
      <li>원본 데이터로부터 도움이 되는 Feature를 생성, 변환하고 모델에 적합한 형식으로 변환하는 작업</li>
      <li>방법
        <ul>
          <li>Pandas Group By Aggregation을 이용한 방법
            <ul>
              <li>sum, mean, max, min 등의 방법으로 Feature를 생성하는 방법</li>
            </ul>
          </li>
          <li>Pandas Group By 누적합을 이용한 방법
            <ul>
              <li>값을 누적해서 얻은 데이터에 대해 sum, mean, max, min 등으로 Feature를 생성하는 방법</li>
            </ul>
          </li>
          <li>주문, 상품 데이터를 활용한 방법
            <ul>
              <li>데이터의 관계를 이용해서 Feature를 생성하는 방법</li>
            </ul>
          </li>
          <li>Time Series 특성을 이용한 방법
            <ul>
              <li>연도, 월 등을 이용해서 데이터의 관계를 파악하고 Feature를 생성하는 방법</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Feature Important
    <ul>
      <li>Boosting Tree 피처 중요도
        <ul>
          <li>GBM(Gradient Boosting Machine)
            <ul>
              <li>오차를 학습하면서 진행하는 방법</li>
              <li><a href="https://github.com/pilsung-kang/Business-Analytics-IME654-">Example</a></li>
            </ul>
          </li>
          <li>LightGBM</li>
          <li>XGBoost</li>
          <li>CatBoost</li>
          <li><a href="https://github.com/pilsung-kang/Business-Analytics-IME654-">고려대학교 강필성 교수님의 Business Analytics</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Feature Selection
    <ul>
      <li>모델에 사용할 피처를 선택하는 과정</li>
      <li>방법
        <ul>
          <li>Filter Method</li>
          <li>Wrapper Method</li>
          <li>Embedded Method</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>하이퍼 파라미터 튜닝
    <ul>
      <li>어떻게 학습할 지를 맞춤형으로 조정해서 더 높은 성능이 나오도록 하는 과정</li>
      <li>방법
        <ul>
          <li>Grid Layout
            <ul>
              <li>하이퍼 파라미터의 경우의 수 중, 모든 경우를 테스트하고 선택</li>
            </ul>
          </li>
          <li>Random Layout
            <ul>
              <li>하이퍼 파라미터의 경우의 수 중, 무차별로 테스트하고 선택</li>
            </ul>
          </li>
          <li>Bayesian Layout
            <ul>
              <li>높은 성능을 보인 곳을 중점으로 테스트하고 선택</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>활용
        <ul>
          <li>전달할 파라미터에 하이퍼 파라미터를 직접 수치를 설정</li>
          <li>Optuna 라이브러리로 범위를 지정해서 설정</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>앙상블 러닝
    <ul>
      <li>여러 모델을 조합해서 학습을 하는 과정</li>
      <li>방법
        <ul>
          <li>Bagging
            <ul>
              <li>샘플을 다양하게 생성</li>
              <li>각자 자기 영역에 대해 학습하고 합치기</li>
            </ul>
          </li>
          <li>Voting
            <ul>
              <li>투표를 통해 도출</li>
            </ul>
          </li>
          <li>Boosting
            <ul>
              <li>이전 오차를 보완해서 가중치 부여</li>
              <li>영역을 단계로 나눠서 학습</li>
            </ul>
          </li>
          <li>Stacking
            <ul>
              <li>여러 모델을 기반으로 meta 모델을 생성(모델들을 이용해서 만든 모델)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>보너스
    <ul>
      <li>논문을 읽을 때 중점적으로 읽는 부분
        <ul>
          <li>내용이 많은 경우, abstract &gt; results &gt; methods를 우선으로 확인</li>
          <li>중요한 논문은 상세히, 아닌 것은 아이디어의 핵심과 결과만 확인(스터디 효과 극대화를 위함)</li>
          <li>어려운 논문은 해설한 유튜브, 블로그를 함께 참고</li>
          <li>LinkedIn을 통해 최신 트랜드를 파악</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h1 id="3주차">3주차</h1>

<ul>
  <li>딥러닝
    <ul>
      <li>수학적 표현 학습, 계층적 표현 학습(Hierarchical Representation)이라 불림
        <ul>
          <li>계층이 깊어질수록 비선형 연산에 의해 표현력이 좋아짐 (기존 데이터를 다른 형태로 매끄럽게 표현)</li>
        </ul>
      </li>
      <li>Feature extraction과 Classification을 한번에 처리함</li>
    </ul>
  </li>
  <li>딥러닝의 역사 (중요 표시 <strong>Bold</strong>)
    <ul>
      <li>AlexNet(2012)</li>
      <li>DQN(2013)</li>
      <li>Encoder/Decoder(2014)</li>
      <li>Adam Optimizer(2014)</li>
      <li>Generative Adversarial Network(2015)</li>
      <li><strong>Transformer(2017)</strong></li>
      <li>BERT(fine-tuned NLP models)(2018)</li>
      <li>BIG Language Models(2019)</li>
      <li><strong>Self Supervised Learning(2020)</strong></li>
    </ul>
  </li>
  <li>딥러닝의 키 컴포넌트
    <ul>
      <li>Data
        <ul>
          <li>다양한 형태의 학습 대상</li>
          <li>Regularization
            <ul>
              <li>Early stopping</li>
              <li>Parameter norm penalty</li>
              <li>Data augmentation</li>
              <li>Noise robustness</li>
              <li>Label smoothing</li>
              <li>Dropout</li>
              <li>Batch normalization</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Model
        <ul>
          <li>데이터의 형태를 변환하는 처리 패턴</li>
        </ul>
      </li>
      <li>Loss Function
        <ul>
          <li>예측값과 실제값의 차이로 모델의 학습 정도를 얼마나 안 좋은지 측정하는 함수</li>
          <li>반의어로 ‘Objective Function’이 있음</li>
        </ul>
      </li>
      <li>Algorithm
        <ul>
          <li>Loss Function의 값을 줄이기 위한 방법</li>
          <li>e.g. 눈가리고 산을 내려가야 하는 상황과 유사
            <ul>
              <li>어느쪽으로 내려가야 할 지 알기 위한 여러가지 방법들이 있음</li>
              <li>Gradient Descent, Adagrad, RMSprop, Momentum, Adam 등</li>
              <li>Gradient Descent(경사 하강법)
                <ul>
                  <li>미분을 이용해서 Loss Function의 기울기를 구함
                    <ul>
                      <li>w1(현재 값), Loss_w1(현재 손실 값), gradient((현재 손실값 / 현재 값) 미분)</li>
                      <li>w1 - [(현재 손실값 / 현재 값) 미분]로 판단할 값을 계산
                        <ul>
                          <li>양수(좌하우상)면 왼쪽, 음수(좌상우하)면 오른쪽으로 내려감</li>
                        </ul>
                      </li>
                      <li>학습율
                        <ul>
                          <li>학습하는 정도를 나타내는 값(내려갈 정도를 나타내는 값)</li>
                          <li>경사도를 이용해서 판단할 때, 조금씩 내려가는 게 아닌 점프를 했다가 다시 올라가는 구간으로 이동하는 문제를 방지하기 위해 조금씩 내려가도록 하는 하이퍼 파라미터</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Neural Networks
    <ul>
      <li>뇌의 복잡한 기능을 단순화(추상화)한 수학적 모델
        <ul>
          <li>가중치의 조정으로 원하는 결과를 얻는 수학적 표현임</li>
          <li>if else를 쌓듯이 matrix 연산을 쌓아서 만든 방식</li>
          <li>연속된 가중치의 곱셈으로 이루어짐
            <ul>
              <li>이 과정에서 ReLU와 같은 활성화함수(비선형성)이 섞여 있음</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>e.g. GoogLeNet, ResNet</li>
    </ul>
  </li>
  <li>Linear Neural Networks
    <ul>
      <li>공간을 늘리고 줄여서 비선형 데이터로 만들고 이를 선형적으로 나눌 수 있게 변형시킴</li>
      <li>affine transform (선형 변환 + 평행 이동)
        <ul>
          <li>변형 전, 후에 점, 선, 면 등의 특징이 달라지지 않고 보존된 변형 방법</li>
          <li>가중치(W^T) * 데이터(X) + 바이어스(B)의 연산</li>
          <li>시각적으로 보면 공간을 땡기듯 늘림</li>
          <li><code class="language-plaintext highlighter-rouge">W^T * X</code>로 선형 변환
            <ul>
              <li>선형변환은 기울기를 변환시킴 (e.g. 수평 -&gt; 대각선)</li>
            </ul>
          </li>
          <li><code class="language-plaintext highlighter-rouge">+B</code>로 평행 이동</li>
        </ul>
      </li>
      <li>Non-linear Activation Function (비선형 변환)
        <ul>
          <li>데이터에 선형 변환만 해서는 계속 선형이기 떄문에 레이어를 여러개 쌓아도 1개의 층과 똑같은 결과를 만들게 됨. 그래서 비선형 변환이 필요</li>
          <li>시그모이드, 활성화 함수 등을 이용한 연산</li>
          <li>시각적으로 보면 공간을 찌그러뜨려서 줄임</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Beyond Linear neural Networks</li>
</ul>

<p><br /></p>

<h1 id="4주차">4주차</h1>

<ul>
  <li>Hierarchical Representation Learning (계층적 표현 학습)
    <ul>
      <li>Linear Neural Networks처럼 Deep Neural Networks의 학습 방법</li>
    </ul>
  </li>
  <li>딥러닝의 진화</li>
  <li>MLP(Multi Layer Perceptron)
    <ul>
      <li>인간처럼 학습을 할 수 있게 하는 방법</li>
    </ul>
  </li>
  <li>CNN(Convolutional Neural Networks)
    <ul>
      <li>이미지를 보고 특징을 분류할 수 있게 하는 방법</li>
      <li>LeNet 1(MNIST, 숫자 인식)
        <ul>
          <li>Multilayer Neural Network의 한계를 극복하기 위해 CNN(Convolutional Neural NEtworks)을 활용한 모델</li>
          <li>Convolutional layer + Pooling layer를 반복하면서 전역적으로 의미있는 특징을 찾아냄</li>
        </ul>
      </li>
      <li>LeNet 5
        <ul>
          <li>LeNet 1의 구조를 계승하고 입력 이미지의 크기, feature-map의 개수 및 fully-connected layer의 크기에 변화를 준 모델</li>
          <li>RGB 빛의 세기를 통해 이미지를 표현
<img src="/images/CS_AI_ML/2023/06/Beyond_AI_Basic_2023/1.png" alt="" />
<em>손으로 쓴 8에 대해 저장된 빛의 세기의 정보를 나타낸 이미지 매트릭스</em></li>
        </ul>
      </li>
      <li>Pooling Layer
        <ul>
          <li>영상에 빗댄 특징
            <ul>
              <li>2차원 공간적 특징을 가짐</li>
              <li>크기에 따라 같은 영역도 다른 특징을 가짐(확대/축소에 따른 눈, 눈코입, 얼굴 등)</li>
              <li>다운 샘플링이 가능(이미지 크기를 줄임)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>RNN(Recurrent Neural Networks)
    <ul>
      <li>책을 보고 공부하는 것처럼 글(Sequential Data)을 읽을 수 있도록 하기 위한 방법</li>
      <li>Sequence Model</li>
      <li>Computational Graph
        <ul>
          <li>Input에 이전 Hidden Layer가 추가되어, 이전 Input의 정보를 포함할 수 있는 Neural Networks 구조</li>
        </ul>
      </li>
      <li>Many to Many
        <ul>
          <li>Computational Graph 구조의 Hidden layer에서 Output을 뽑아, 손실함수를 구하는 구조</li>
        </ul>
      </li>
      <li>자동완성 등으로 활용</li>
    </ul>
  </li>
  <li>LSTM(Long Short Term Memory)
    <ul>
      <li>이전 Input을 모두 기억하기 위해 Long Term(외장 하드)를 적용한 방법</li>
      <li>RNN으로는 이전 내용을 전부 기억할 수 없기 때문에 등장</li>
      <li>동작 원리
        <ul>
          <li>forget으로 이전 데이터에서 지울 부분 결정</li>
          <li>input modulation에서 표현할 정보를 반영여부인 input의 값과 곱해서 반영할 데이터에 대한 정보 생성</li>
          <li>RNN의 결과값인 output과 Long Term Memory에 저장되어 있는 값인 context vector를 비선형화하고 곱해서 출력에 반영</li>
        </ul>
      </li>
      <li>4 gate
        <ul>
          <li>forget
            <ul>
              <li>0이면 삭제, 1이면 보존</li>
            </ul>
          </li>
          <li>input
            <ul>
              <li>0이면 미반영, 1이면 반영</li>
            </ul>
          </li>
          <li>input modulation
            <ul>
              <li>-1, 1로 나타낼 정보를 표현</li>
            </ul>
          </li>
          <li>output
            <ul>
              <li>0, 1로 구성된 RNN 결과
<img src="/images/CS_AI_ML/2023/06/Beyond_AI_Basic_2023/2.png" alt="" />
<em>LSTM Architecture</em></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Attention
    <ul>
      <li>전체 데이터를 보지 않고, 일부 데이터만 압축해놓고 활용하는 방법</li>
      <li>LSTM으로는 문장의 구성을 하는데 한계가 있기 떄문에 등장</li>
      <li>Attention 모델을 통해 문장 구성이 가능해져 대화할 수 있게 됨</li>
      <li>Sequence-to-Sequence with RNNs 모델을 활용 시, 문장이 길수록 C(Memory cell)의 크기가 매우 커지는 문제가 있었음</li>
      <li>동작원리
        <ul>
          <li>이전 값에 대한 정보를 담고 있는 hidden layer들과 s들을 각각 얼마나 연관성이 있는지 계산</li>
          <li>계산된 값을 전부 합한 값이 1이 되는 soft max에 통과시켜 연관성에 대한 확률을 얻음</li>
          <li>각 input들에 매치되는 연관성 확률만큼만 데이터를 반영</li>
          <li>context vector를 생성하고 이전 결과값인 output과 계산해서 S를 생성</li>
          <li>새로 생성된 S와 hidden layer들과의 연관성을 계산</li>
          <li>위의 과정들을 반복하면서 Sequential한 데이터에 대한 정보를 쌓음</li>
        </ul>
      </li>
      <li><em>‘Attention Architecture 1’ 이미지는 별도 보관</em></li>
    </ul>
  </li>
  <li>Attention Layer
    <ul>
      <li>Sequence Data의 Attention Score 얻는 과정을 General하게 행렬 연산으로 표현한 방법</li>
      <li>Attention 구조를 그대로 가져온 상태에서 Hidden을 Key, S(context + output)를 Query라 봄</li>
      <li>동작원리
        <ul>
          <li>각 Key, Query들을 곱한 경우의 수를 행렬로 표현</li>
          <li>곱에 대한 행렬을 soft max에 통과시켜 연관성에 대한 확률들을 얻음</li>
          <li>hidden과 행렬의 row별로 곱(product)하고 더해서(sum) column 개수만큼의 결과를 얻음</li>
          <li>결과로 얻은 값들은 다시 각 Query들이 되어, 위의 과정들을 반복하면서 정보를 쌓음</li>
        </ul>
      </li>
      <li><em>‘Attention Architecture 2’ 이미지는 별도 보관</em></li>
    </ul>
  </li>
  <li>Self-Attention Layer
    <ul>
      <li>input을 Query에도 반영해서 <code class="language-plaintext highlighter-rouge">나와 나</code>, <code class="language-plaintext highlighter-rouge">나와 다른 것</code>의 연관성을 스스로 고려하는 방법</li>
    </ul>
  </li>
  <li>Multi head Self - Attention Layer
    <ul>
      <li>각 input을 분리해서 각각 Self-Attention을 수행하는 방법</li>
    </ul>
  </li>
  <li>Transformer
    <ul>
      <li>Self-Attention을 수행하는 Transformer Block을 쌓아 만든 방법</li>
      <li>동작원리 (Transformer Block)
        <ul>
          <li>각 input을 Self-Attention하고 출력값과 input들로 Residual connection 수행</li>
          <li>Layer Normalization 수행</li>
          <li>각 input들에 대해 Multi Layer Perceptron을 수행</li>
          <li>MLP 전의 값과 MLP 후의 값으로 Residual connection 수행</li>
          <li>Layer Normalization 수행</li>
          <li>결과값을 도출</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ChatGPT
    <ul>
      <li>Transformer 모델로 학습된 LLM(Large Language Model) 모델</li>
    </ul>
  </li>
</ul>

<h1 id="reference">Reference</h1>

<ul>
  <li><a href="https://m.post.naver.com/viewer/postView.naver?volumeNo=35664080&amp;memberNo=34635212">제대로 된 인공지능 스터디를 찾고 있다면? AI 심화 스터디</a></li>
  <li><a href="https://www.boostcourse.org/study-ai111-2023">[코칭스터디 10기] Beyond AI Basic 2023 스터디 전용강좌</a></li>
  <li><a href="https://blog.ovhcloud.com/deep-learning-explained-to-my-8-year-old-daughter/">손으로 쓴 8에 대해 저장된 이미지 매트릭스</a></li>
  <li><a href="https://aman.ai/cs231n/rnn/">LSTM Architecture</a></li>
  <li><a href="https://velog.io/@kangtae/EECS-498-007-598-005-13.-Attention">13. Attention</a></li>
</ul>


      <div class="c-article__footer u-clearfix">
        <div class="c-article__tag">
          
        </div>
        <div class="c-article__share">
          <a href="https://twitter.com/intent/tweet?text=%EC%BD%94%EC%B9%AD%EC%8A%A4%ED%84%B0%EB%94%94%20Beyond%20AI%20Basic%202023%20-%204%EC%A3%BC%20%EA%B3%BC%EC%A0%95%20%EC%A0%95%EB%A6%AC&url=https://dong-jun-shin.github.io/2023/06/06/Beyond_AI_Basic_2023/" title="Share
          on Twitter" rel="nofollow" target="_blank"><div data-icon='ei-sc-twitter' data-size='s'></div></a>
          <a href="https://facebook.com/sharer.php?u=https://dong-jun-shin.github.io/2023/06/06/Beyond_AI_Basic_2023/" title="Share on Facebook" rel="nofollow" target="_blank"><div data-icon='ei-sc-facebook' data-size='s'></div></a>
          <a href="https://plus.google.com/share?url=https://dong-jun-shin.github.io/2023/06/06/Beyond_AI_Basic_2023/" title="Share on Google+" rel="nofollow" target="_blank"><div data-icon='ei-sc-google-plus' data-size='s'></div></a>
        </div>
      </div>
      <div class="c-newsletter">
  <div class="c-newsletter__header">
    <h4 class="c-newsletter__title">Newsletter</h4>
    <div class="c-newsletter__subtitle">Subscribe to this blog and receive notifications of new posts by email.</div>
  </div>
  <form class="c-newsletter-form validate" action="#" method="POST" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" target="_blank" novalidate>
    <div class="c-newsletter-form__group">
      <label class="u-screen-reader-text" for="mce-EMAIL">Email address</label>
      <input class="c-newsletter__email required email" id="mce-EMAIL" type="email" name="EMAIL" placeholder="Your email address" autocomplete="on">
      <input class="c-newsletter__button" id="mc-embedded-subscribe" type="submit" name="subscribe" value="Subscribe">
    </div>
  </form>
</div> <!-- /.c-newsletter -->
      <div class="c-recent-post">
        <h4 class="c-recent__title">You might also enjoy</h4>
        <div class="c-recent__box">
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2025/01/10/permutation_and_combination_implements/" style="background-image: url( /images/CS_Algorithm/2025/01/permutation_and_combination_implements/thumbnail.png)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2025/01/10/permutation_and_combination_implements/">순열과 조합 - 2, 경우의 수 뿐만 아니라 경우를 직접 구해보자</a></h4>
              <div class="c-recent__date">
                <time datetime="2025-01-10T17:45:03+09:00">January 10, 2025</time>
              </div>
            </div>
          </div>
        
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2025/01/10/permutation_and_combination_theory/" style="background-image: url( /images/CS_Algorithm/2025/01/permutation_and_combination_theory/thumbnail.png)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2025/01/10/permutation_and_combination_theory/">순열과 조합 - 1, 어떤 원리로 경우의 수를 계산할 수 있는걸까?</a></h4>
              <div class="c-recent__date">
                <time datetime="2025-01-10T17:45:02+09:00">January 10, 2025</time>
              </div>
            </div>
          </div>
        
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2024/08/21/2_years_of_experience_from_2022/" style="background-image: url( /images/Life/2024/08/2_years_of_experience_from_2022/thumbnail_retrospective-journey.jpg)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2024/08/21/2_years_of_experience_from_2022/">퇴사 회고(라 쓰고, 2022년 ~ 2024년 정리)</a></h4>
              <div class="c-recent__date">
                <time datetime="2024-08-21T23:47:38+09:00">August 21, 2024</time>
              </div>
            </div>
          </div>
        
        
        
          <div class="c-recent__item">
            <a class="c-recent__image" href="/2024/05/21/Aws_Summit_Seoul_2024/" style="background-image: url( /images/IT_Tech/2024/05/Aws_Summit_Seoul_2024/thumbnail.png)"></a>
            <div class="c-recent__footer">
              <h4><a href="/2024/05/21/Aws_Summit_Seoul_2024/">AWS Summit Seoul 2024 방문기</a></h4>
              <div class="c-recent__date">
                <time datetime="2024-05-21T21:55:48+09:00">May 21, 2024</time>
              </div>
            </div>
          </div>
        
        
        </div>
      </div> <!-- /.c-recent-post -->
      
        <div class="c-comments">
  <div id="disqus_thread" class="article-comments"></div>
  <script>
    (function () {
      var d = document, s = d.createElement('script');
      s.src = '//dong-jun-shin-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
  </noscript>
</div> <!-- /.c-comments -->
      
    </div> <!-- /.c-wrap-content -->
  </div> <!-- /.c-article__content -->
</article> <!-- /.c-article-page -->

</main> <!-- /.c-content -->
  </div> <!-- /.o-wrapper -->
  <div class="c-top" data-icon='ei-chevron-up' data-size='s' title="Scroll To Top"></div> <!-- /.c-top -->
  <script src="/js/jquery-3.3.1.min.js"></script>
<script src="/js/evil-icons.min.js"></script>
<script src="/js/jquery.fitvids.js"></script>
<script src="/js/simple-jekyll-search.min.js"></script>
<script src="/js/main.js"></script>
<!-- /javascripts -->
</body>
</html>